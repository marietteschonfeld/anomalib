/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad bottle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/13 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   8%|â–Š         | 1/13 [00:00<00:09,  1.30it/s]Epoch 0:   8%|â–Š         | 1/13 [00:00<00:09,  1.30it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:01<00:06,  1.79it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:01<00:06,  1.79it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:05,  1.82it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:05,  1.82it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:04,  1.83it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:04,  1.83it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:04,  1.84it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:04,  1.84it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:03<00:03,  1.85it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:03<00:03,  1.85it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:03<00:03,  1.85it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:03<00:03,  1.85it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:04<00:02,  1.85it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:04<00:02,  1.85it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:04<00:02,  1.85it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:04<00:02,  1.85it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:05<00:01,  1.85it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:05<00:01,  1.85it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:05<00:01,  1.86it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:05<00:01,  1.86it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:06<00:00,  1.85it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:06<00:00,  1.85it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  1.96it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  1.96it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.95it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.99it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:02<00:01,  1.01it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:03<00:00,  1.02it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.02it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:50<00:00,  0.26it/s, pixel_AUPRO=0.940]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:50<00:00,  0.26it/s, pixel_AUPRO=0.940]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [01:15<00:00,  0.17it/s, pixel_AUPRO=0.940]INFO:anomalib.callbacks.timer:Training took 76.11 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:04<00:18,  0.21it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:08<00:13,  0.23it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:13<00:08,  0.22it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:17<00:04,  0.23it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:21<00:00,  0.23it/s]INFO:anomalib.callbacks.timer:Testing took 64.25471806526184 seconds
Throughput (batch_size=17) : 1.2917339379763377 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:03<00:00,  0.08it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚            1.0            â”‚
â”‚        pixel_AUPRO        â”‚    0.9411697387695312     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad cable with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/14 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/14 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/14 [00:01<00:13,  0.95it/s]Epoch 0:   7%|â–‹         | 1/14 [00:01<00:13,  0.95it/s]Epoch 0:  14%|â–ˆâ–        | 2/14 [00:01<00:10,  1.19it/s]Epoch 0:  14%|â–ˆâ–        | 2/14 [00:01<00:10,  1.19it/s]Epoch 0:  21%|â–ˆâ–ˆâ–       | 3/14 [00:02<00:09,  1.20it/s]Epoch 0:  21%|â–ˆâ–ˆâ–       | 3/14 [00:02<00:09,  1.20it/s]Epoch 0:  29%|â–ˆâ–ˆâ–Š       | 4/14 [00:03<00:08,  1.21it/s]Epoch 0:  29%|â–ˆâ–ˆâ–Š       | 4/14 [00:03<00:08,  1.21it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 5/14 [00:04<00:07,  1.21it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 5/14 [00:04<00:07,  1.21it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6/14 [00:04<00:06,  1.21it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6/14 [00:04<00:06,  1.21it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 7/14 [00:05<00:05,  1.21it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 7/14 [00:05<00:05,  1.21it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 8/14 [00:06<00:04,  1.21it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 8/14 [00:06<00:04,  1.21it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 9/14 [00:07<00:04,  1.21it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 9/14 [00:07<00:04,  1.21it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 10/14 [00:08<00:03,  1.22it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 10/14 [00:08<00:03,  1.22it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 11/14 [00:09<00:02,  1.22it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 11/14 [00:09<00:02,  1.22it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12/14 [00:09<00:01,  1.22it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12/14 [00:09<00:01,  1.22it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 13/14 [00:10<00:00,  1.22it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 13/14 [00:10<00:00,  1.22it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:10<00:00,  1.29it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:10<00:00,  1.29it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:01<00:08,  0.98it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:01<00:06,  1.01it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:02<00:05,  1.03it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:03<00:04,  1.04it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:04<00:03,  1.04it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:05<00:02,  1.05it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:06<00:01,  1.05it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:07<00:00,  1.05it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:08<00:00,  1.05it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [02:13<00:00,  0.11it/s, pixel_AUPRO=0.917]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [02:13<00:00,  0.11it/s, pixel_AUPRO=0.917]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [02:39<00:00,  0.09it/s, pixel_AUPRO=0.917]INFO:anomalib.callbacks.timer:Training took 160.72 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:04<00:37,  0.21it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:09<00:32,  0.22it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:13<00:27,  0.22it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:18<00:22,  0.22it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:22<00:18,  0.22it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:27<00:13,  0.22it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:31<00:09,  0.22it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:36<00:04,  0.22it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:40<00:00,  0.22it/s]INFO:anomalib.callbacks.timer:Testing took 213.71944308280945 seconds
Throughput (batch_size=17) : 0.7018547205453824 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [03:32<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9615816473960876     â”‚
â”‚        pixel_AUPRO        â”‚    0.9192482233047485     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad capsule with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/13 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   8%|â–Š         | 1/13 [00:01<00:12,  0.93it/s]Epoch 0:   8%|â–Š         | 1/13 [00:01<00:12,  0.93it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:01<00:09,  1.19it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:01<00:09,  1.19it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:02<00:08,  1.20it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:02<00:08,  1.20it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:03<00:07,  1.21it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:03<00:07,  1.21it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:04<00:06,  1.20it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:04<00:06,  1.20it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:04<00:05,  1.21it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:04<00:05,  1.21it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:05<00:04,  1.21it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:05<00:04,  1.21it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:06<00:04,  1.21it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:06<00:04,  1.21it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:07<00:03,  1.21it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:07<00:03,  1.21it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:08<00:02,  1.21it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:08<00:02,  1.21it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:09<00:01,  1.21it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:09<00:01,  1.21it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:09<00:00,  1.21it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:09<00:00,  1.21it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.22it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.22it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/8 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s][A
Validation DataLoader 0:  12%|â–ˆâ–Ž        | 1/8 [00:01<00:07,  0.96it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:01<00:05,  1.00it/s][A
Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:02<00:04,  1.02it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:03<00:03,  1.03it/s][A
Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:04<00:02,  1.04it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:05<00:01,  1.04it/s][A
Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:06<00:00,  1.04it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:07<00:00,  1.04it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [02:16<00:00,  0.10it/s, pixel_AUPRO=0.940]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [02:16<00:00,  0.10it/s, pixel_AUPRO=0.940]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [02:41<00:00,  0.08it/s, pixel_AUPRO=0.940]INFO:anomalib.callbacks.timer:Training took 162.37 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/8 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]Testing DataLoader 0:  12%|â–ˆâ–Ž        | 1/8 [00:04<00:33,  0.21it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:09<00:27,  0.22it/s]Testing DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:14<00:24,  0.21it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:19<00:19,  0.21it/s]Testing DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:24<00:14,  0.21it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:28<00:09,  0.21it/s]Testing DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:33<00:04,  0.21it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:37<00:00,  0.21it/s]INFO:anomalib.callbacks.timer:Testing took 162.11942148208618 seconds
Throughput (batch_size=17) : 0.8142146005288188 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:41<00:00,  0.05it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9764658808708191     â”‚
â”‚        pixel_AUPRO        â”‚     0.939543604850769     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad carpet with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/17 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/17 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.99it/s]Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.99it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:12,  1.24it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:12,  1.24it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:11,  1.25it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:11,  1.25it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:10,  1.26it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:10,  1.26it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:03<00:09,  1.27it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:03<00:09,  1.27it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:04<00:08,  1.27it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:04<00:08,  1.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:05<00:07,  1.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:05<00:07,  1.27it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:06<00:07,  1.27it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:06<00:07,  1.27it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:07<00:06,  1.28it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:07<00:06,  1.28it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:07<00:05,  1.28it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:07<00:05,  1.28it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:08<00:04,  1.28it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:08<00:04,  1.28it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:09<00:03,  1.28it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:09<00:03,  1.28it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:10<00:03,  1.28it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:10<00:03,  1.28it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:10<00:02,  1.28it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:10<00:02,  1.28it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:11<00:01,  1.28it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:11<00:01,  1.28it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:12<00:00,  1.28it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:12<00:00,  1.28it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:12<00:00,  1.32it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:12<00:00,  1.32it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.99it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:04,  1.03it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:02<00:03,  1.04it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:03<00:02,  1.03it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:04<00:01,  1.03it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:05<00:00,  1.04it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.04it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:26<00:00,  0.20it/s, pixel_AUPRO=0.948]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:26<00:00,  0.20it/s, pixel_AUPRO=0.948]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:59<00:00,  0.14it/s, pixel_AUPRO=0.948]INFO:anomalib.callbacks.timer:Training took 120.08 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:05<00:30,  0.20it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:09<00:23,  0.21it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:14<00:18,  0.21it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:18<00:14,  0.21it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:23<00:09,  0.22it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:27<00:04,  0.22it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:31<00:00,  0.22it/s]INFO:anomalib.callbacks.timer:Testing took 121.58028674125671 seconds
Throughput (batch_size=17) : 0.9623270608745615 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [02:00<00:00,  0.06it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9823434948921204     â”‚
â”‚        pixel_AUPRO        â”‚    0.9480385184288025     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad grid with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/16 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   6%|â–‹         | 1/16 [00:00<00:10,  1.38it/s]Epoch 0:   6%|â–‹         | 1/16 [00:00<00:10,  1.38it/s]Epoch 0:  12%|â–ˆâ–Ž        | 2/16 [00:01<00:07,  1.99it/s]Epoch 0:  12%|â–ˆâ–Ž        | 2/16 [00:01<00:07,  1.99it/s]Epoch 0:  19%|â–ˆâ–‰        | 3/16 [00:01<00:06,  2.03it/s]Epoch 0:  19%|â–ˆâ–‰        | 3/16 [00:01<00:06,  2.03it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:05,  2.06it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:05,  2.05it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:02<00:05,  2.08it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:02<00:05,  2.08it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:02<00:04,  2.08it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:02<00:04,  2.08it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:03<00:04,  2.09it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:03<00:04,  2.09it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:03<00:03,  2.09it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:03<00:03,  2.09it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:04<00:03,  2.10it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:04<00:03,  2.10it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:04<00:02,  2.10it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:04<00:02,  2.10it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:05<00:02,  2.10it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:05<00:02,  2.10it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:05<00:01,  2.11it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:05<00:01,  2.11it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:06<00:01,  2.11it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:06<00:01,  2.11it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:06<00:00,  2.11it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:06<00:00,  2.11it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:07<00:00,  2.11it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:07<00:00,  2.11it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:07<00:00,  2.17it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:07<00:00,  2.17it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.95it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  1.00it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:02<00:01,  1.03it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:03<00:00,  1.04it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.04it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:23<00:00,  0.19it/s, pixel_AUPRO=0.890]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:23<00:00,  0.19it/s, pixel_AUPRO=0.890]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:53<00:00,  0.14it/s, pixel_AUPRO=0.890]INFO:anomalib.callbacks.timer:Training took 114.66 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:04<00:17,  0.22it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:08<00:13,  0.23it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:13<00:09,  0.22it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:17<00:04,  0.22it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:20<00:00,  0.24it/s]INFO:anomalib.callbacks.timer:Testing took 120.949951171875 seconds
Throughput (batch_size=17) : 0.6448948448863671 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:00<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9782790541648865     â”‚
â”‚        pixel_AUPRO        â”‚    0.8899616003036499     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad hazelnut with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/23 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/23 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–         | 1/23 [00:01<00:22,  0.96it/s]Epoch 0:   4%|â–         | 1/23 [00:01<00:22,  0.96it/s]Epoch 0:   9%|â–Š         | 2/23 [00:01<00:17,  1.20it/s]Epoch 0:   9%|â–Š         | 2/23 [00:01<00:17,  1.20it/s]Epoch 0:  13%|â–ˆâ–Ž        | 3/23 [00:02<00:16,  1.22it/s]Epoch 0:  13%|â–ˆâ–Ž        | 3/23 [00:02<00:16,  1.22it/s]Epoch 0:  17%|â–ˆâ–‹        | 4/23 [00:03<00:15,  1.23it/s]Epoch 0:  17%|â–ˆâ–‹        | 4/23 [00:03<00:15,  1.23it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 5/23 [00:04<00:14,  1.24it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 5/23 [00:04<00:14,  1.24it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 6/23 [00:04<00:13,  1.24it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 6/23 [00:04<00:13,  1.24it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:05<00:12,  1.25it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:05<00:12,  1.25it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 8/23 [00:06<00:12,  1.25it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 8/23 [00:06<00:12,  1.25it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:07<00:11,  1.25it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:07<00:11,  1.25it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10/23 [00:07<00:10,  1.25it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10/23 [00:07<00:10,  1.25it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:08<00:09,  1.25it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:08<00:09,  1.25it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/23 [00:09<00:08,  1.25it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/23 [00:09<00:08,  1.25it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:10<00:07,  1.25it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:10<00:07,  1.25it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14/23 [00:11<00:07,  1.25it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14/23 [00:11<00:07,  1.25it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:11<00:06,  1.26it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:11<00:06,  1.26it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16/23 [00:12<00:05,  1.26it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16/23 [00:12<00:05,  1.25it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:13<00:04,  1.26it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:13<00:04,  1.26it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18/23 [00:14<00:03,  1.26it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18/23 [00:14<00:03,  1.26it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:15<00:03,  1.26it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:15<00:03,  1.26it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20/23 [00:15<00:02,  1.26it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20/23 [00:15<00:02,  1.26it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:16<00:01,  1.26it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:16<00:01,  1.26it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22/23 [00:17<00:00,  1.26it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22/23 [00:17<00:00,  1.26it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:18<00:00,  1.26it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:18<00:00,  1.26it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.92it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.96it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.97it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.98it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.99it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.99it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  0.99it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [02:00<00:00,  0.19it/s, pixel_AUPRO=0.944]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [02:00<00:00,  0.19it/s, pixel_AUPRO=0.944]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [02:42<00:00,  0.14it/s, pixel_AUPRO=0.944]INFO:anomalib.callbacks.timer:Training took 163.97 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:04<00:29,  0.20it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:09<00:23,  0.21it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:14<00:19,  0.21it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:19<00:14,  0.21it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:23<00:09,  0.21it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:28<00:04,  0.21it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:31<00:00,  0.22it/s]INFO:anomalib.callbacks.timer:Testing took 141.00774550437927 seconds
Throughput (batch_size=17) : 0.780098991062755 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [02:20<00:00,  0.05it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9953571557998657     â”‚
â”‚        pixel_AUPRO        â”‚    0.9483054876327515     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad leather with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:00<00:12,  1.09it/s]Epoch 0:   7%|â–‹         | 1/15 [00:00<00:12,  1.09it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:09,  1.37it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:09,  1.37it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:08,  1.40it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:08,  1.40it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:02<00:07,  1.41it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:02<00:07,  1.41it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.43it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.43it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.44it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.44it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:04<00:05,  1.44it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:04<00:05,  1.44it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:05<00:04,  1.44it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:05<00:04,  1.44it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.44it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.44it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:06<00:03,  1.44it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:06<00:03,  1.44it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:07<00:02,  1.44it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:07<00:02,  1.44it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:08<00:02,  1.44it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:08<00:02,  1.44it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:08<00:01,  1.44it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:08<00:01,  1.44it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:09<00:00,  1.44it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:09<00:00,  1.44it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:09<00:00,  1.50it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:09<00:00,  1.50it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/8 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s][A
Validation DataLoader 0:  12%|â–ˆâ–Ž        | 1/8 [00:01<00:07,  0.96it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:01<00:05,  1.00it/s][A
Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:02<00:04,  1.02it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:03<00:03,  1.03it/s][A
Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:04<00:02,  1.03it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:05<00:01,  1.03it/s][A
Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:06<00:00,  1.04it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:07<00:00,  1.04it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:32<00:00,  0.16it/s, pixel_AUPRO=0.969]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:32<00:00,  0.16it/s, pixel_AUPRO=0.969]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:01<00:00,  0.12it/s, pixel_AUPRO=0.969]INFO:anomalib.callbacks.timer:Training took 121.84 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/8 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]Testing DataLoader 0:  12%|â–ˆâ–Ž        | 1/8 [00:04<00:32,  0.21it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:09<00:27,  0.22it/s]Testing DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:14<00:24,  0.21it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:19<00:19,  0.21it/s]Testing DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:23<00:14,  0.21it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:28<00:09,  0.21it/s]Testing DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:33<00:04,  0.21it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:35<00:00,  0.23it/s]INFO:anomalib.callbacks.timer:Testing took 131.79719972610474 seconds
Throughput (batch_size=17) : 0.9408394128076428 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:10<00:00,  0.06it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9986412525177002     â”‚
â”‚        pixel_AUPRO        â”‚    0.9694840908050537     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad metal_nut with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/13 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   8%|â–Š         | 1/13 [00:00<00:07,  1.60it/s]Epoch 0:   8%|â–Š         | 1/13 [00:00<00:07,  1.60it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:00<00:04,  2.42it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:00<00:04,  2.42it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:04,  2.47it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:04,  2.47it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:01<00:03,  2.50it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:01<00:03,  2.50it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:01<00:03,  2.52it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:01<00:03,  2.52it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:02<00:02,  2.53it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:02<00:02,  2.53it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:02<00:02,  2.54it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:02<00:02,  2.54it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:03<00:01,  2.54it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:03<00:01,  2.54it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:03<00:01,  2.55it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:03<00:01,  2.55it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:03<00:01,  2.55it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:03<00:01,  2.55it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:04<00:00,  2.56it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:04<00:00,  2.56it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:04<00:00,  2.56it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:04<00:00,  2.56it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:05<00:00,  2.57it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:05<00:00,  2.57it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.96it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  1.00it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:02<00:03,  1.01it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:03<00:02,  1.01it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:04<00:01,  1.02it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:05<00:00,  1.03it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.03it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [01:41<00:00,  0.13it/s, pixel_AUPRO=0.940]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [01:41<00:00,  0.13it/s, pixel_AUPRO=0.940]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [02:06<00:00,  0.10it/s, pixel_AUPRO=0.940]INFO:anomalib.callbacks.timer:Training took 127.37 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:04<00:26,  0.23it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:08<00:20,  0.24it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:12<00:16,  0.24it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:16<00:12,  0.24it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:20<00:08,  0.24it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:24<00:04,  0.24it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:27<00:00,  0.25it/s]INFO:anomalib.callbacks.timer:Testing took 134.85383009910583 seconds
Throughput (batch_size=17) : 0.8527751856620238 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [02:14<00:00,  0.05it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9960898756980896     â”‚
â”‚        pixel_AUPRO        â”‚    0.9401252865791321     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad pill with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/16 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   6%|â–‹         | 1/16 [00:00<00:11,  1.30it/s]Epoch 0:   6%|â–‹         | 1/16 [00:00<00:11,  1.30it/s]Epoch 0:  12%|â–ˆâ–Ž        | 2/16 [00:01<00:07,  1.81it/s]Epoch 0:  12%|â–ˆâ–Ž        | 2/16 [00:01<00:07,  1.81it/s]Epoch 0:  19%|â–ˆâ–‰        | 3/16 [00:01<00:07,  1.82it/s]Epoch 0:  19%|â–ˆâ–‰        | 3/16 [00:01<00:07,  1.82it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:02<00:06,  1.82it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:02<00:06,  1.82it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:02<00:06,  1.82it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:02<00:06,  1.82it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:03<00:05,  1.83it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:03<00:05,  1.83it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:03<00:04,  1.83it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:03<00:04,  1.83it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:04<00:04,  1.84it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:04<00:04,  1.84it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:04<00:03,  1.85it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:04<00:03,  1.85it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:05<00:03,  1.86it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:05<00:03,  1.86it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:05<00:02,  1.86it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:05<00:02,  1.86it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:06<00:02,  1.86it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:06<00:02,  1.86it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:06<00:01,  1.87it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:06<00:01,  1.87it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:07<00:01,  1.87it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:07<00:01,  1.87it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:07<00:00,  1.88it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:07<00:00,  1.88it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:08<00:00,  1.91it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:08<00:00,  1.91it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|â–ˆ         | 1/10 [00:01<00:09,  0.96it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:01<00:07,  1.00it/s][A
Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:02<00:06,  1.02it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:03<00:05,  1.03it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:04<00:04,  1.03it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:05<00:03,  1.04it/s][A
Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:06<00:02,  1.04it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:07<00:01,  1.04it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:08<00:00,  1.04it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.04it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [04:36<00:00,  0.06it/s, pixel_AUPRO=0.953]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [04:36<00:00,  0.06it/s, pixel_AUPRO=0.953]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [05:07<00:00,  0.05it/s, pixel_AUPRO=0.953]INFO:anomalib.callbacks.timer:Training took 308.06 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|â–ˆ         | 1/10 [00:04<00:39,  0.23it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:08<00:33,  0.24it/s]Testing DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:12<00:29,  0.23it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:16<00:25,  0.24it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:20<00:20,  0.24it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:24<00:16,  0.24it/s]Testing DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:29<00:12,  0.24it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:33<00:08,  0.24it/s]Testing DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:37<00:04,  0.24it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:40<00:00,  0.24it/s]INFO:anomalib.callbacks.timer:Testing took 338.58979415893555 seconds
Throughput (batch_size=17) : 0.4932221906299085 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [05:37<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9247136116027832     â”‚
â”‚        pixel_AUPRO        â”‚    0.9528973698616028     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad screw with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/19 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/19 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   5%|â–Œ         | 1/19 [00:00<00:12,  1.41it/s]Epoch 0:   5%|â–Œ         | 1/19 [00:00<00:12,  1.41it/s]Epoch 0:  11%|â–ˆ         | 2/19 [00:00<00:08,  2.02it/s]Epoch 0:  11%|â–ˆ         | 2/19 [00:00<00:08,  2.02it/s]Epoch 0:  16%|â–ˆâ–Œ        | 3/19 [00:01<00:07,  2.05it/s]Epoch 0:  16%|â–ˆâ–Œ        | 3/19 [00:01<00:07,  2.05it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 4/19 [00:01<00:07,  2.08it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 4/19 [00:01<00:07,  2.08it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 5/19 [00:02<00:06,  2.09it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 5/19 [00:02<00:06,  2.09it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:02<00:06,  2.10it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:02<00:06,  2.10it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [00:03<00:05,  2.08it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [00:03<00:05,  2.08it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [00:03<00:05,  2.06it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [00:03<00:05,  2.06it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:04<00:04,  2.07it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:04<00:04,  2.07it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [00:04<00:04,  2.07it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [00:04<00:04,  2.07it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [00:05<00:03,  2.08it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [00:05<00:03,  2.08it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:05<00:03,  2.08it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:05<00:03,  2.08it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [00:06<00:02,  2.08it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [00:06<00:02,  2.08it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [00:06<00:02,  2.08it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [00:06<00:02,  2.08it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:07<00:01,  2.09it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:07<00:01,  2.09it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [00:07<00:01,  2.09it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [00:07<00:01,  2.09it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [00:08<00:00,  2.09it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [00:08<00:00,  2.09it/s]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:08<00:00,  2.10it/s]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:08<00:00,  2.10it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:08<00:00,  2.11it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:08<00:00,  2.11it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|â–ˆ         | 1/10 [00:01<00:09,  0.96it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:01<00:07,  1.00it/s][A
Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:02<00:06,  1.02it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:03<00:05,  1.02it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:04<00:04,  1.03it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:05<00:03,  1.03it/s][A
Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:06<00:02,  1.03it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:07<00:01,  1.03it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:08<00:00,  1.04it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.04it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [02:51<00:00,  0.11it/s, pixel_AUPRO=0.966]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [02:51<00:00,  0.11it/s, pixel_AUPRO=0.966]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [03:29<00:00,  0.09it/s, pixel_AUPRO=0.966]INFO:anomalib.callbacks.timer:Training took 210.59 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|â–ˆ         | 1/10 [00:04<00:39,  0.23it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:08<00:33,  0.24it/s]Testing DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:12<00:29,  0.24it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:16<00:24,  0.24it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:20<00:20,  0.24it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:24<00:16,  0.25it/s]Testing DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:28<00:12,  0.24it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:32<00:08,  0.25it/s]Testing DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:36<00:04,  0.25it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:38<00:00,  0.26it/s]INFO:anomalib.callbacks.timer:Testing took 211.00319719314575 seconds
Throughput (batch_size=17) : 0.75828234893304 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:30<00:00,  0.05it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9842180609703064     â”‚
â”‚        pixel_AUPRO        â”‚    0.9659935235977173     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad tile with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/14 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/14 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/14 [00:00<00:10,  1.22it/s]Epoch 0:   7%|â–‹         | 1/14 [00:00<00:10,  1.22it/s]Epoch 0:  14%|â–ˆâ–        | 2/14 [00:01<00:07,  1.63it/s]Epoch 0:  14%|â–ˆâ–        | 2/14 [00:01<00:07,  1.63it/s]Epoch 0:  21%|â–ˆâ–ˆâ–       | 3/14 [00:01<00:06,  1.66it/s]Epoch 0:  21%|â–ˆâ–ˆâ–       | 3/14 [00:01<00:06,  1.66it/s]Epoch 0:  29%|â–ˆâ–ˆâ–Š       | 4/14 [00:02<00:05,  1.68it/s]Epoch 0:  29%|â–ˆâ–ˆâ–Š       | 4/14 [00:02<00:05,  1.68it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 5/14 [00:02<00:05,  1.69it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 5/14 [00:02<00:05,  1.69it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6/14 [00:03<00:04,  1.69it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6/14 [00:03<00:04,  1.69it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 7/14 [00:04<00:04,  1.70it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 7/14 [00:04<00:04,  1.70it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 8/14 [00:04<00:03,  1.70it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 8/14 [00:04<00:03,  1.70it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 9/14 [00:05<00:02,  1.70it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 9/14 [00:05<00:02,  1.70it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 10/14 [00:05<00:02,  1.70it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 10/14 [00:05<00:02,  1.70it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 11/14 [00:06<00:01,  1.70it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 11/14 [00:06<00:01,  1.70it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12/14 [00:07<00:01,  1.71it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12/14 [00:07<00:01,  1.71it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 13/14 [00:07<00:00,  1.71it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 13/14 [00:07<00:00,  1.71it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.76it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.76it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.95it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.97it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.99it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  1.00it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:04<00:01,  1.00it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:05<00:00,  1.01it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.01it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:08<00:00,  0.21it/s, pixel_AUPRO=0.819]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:08<00:00,  0.21it/s, pixel_AUPRO=0.819]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:35<00:00,  0.15it/s, pixel_AUPRO=0.819]INFO:anomalib.callbacks.timer:Training took 96.66 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:04<00:28,  0.21it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:09<00:22,  0.22it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:14<00:18,  0.21it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:18<00:14,  0.21it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:23<00:09,  0.22it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:27<00:04,  0.22it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:32<00:00,  0.22it/s]INFO:anomalib.callbacks.timer:Testing took 111.57593536376953 seconds
Throughput (batch_size=17) : 1.0486132123252785 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:50<00:00,  0.06it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9949495792388916     â”‚
â”‚        pixel_AUPRO        â”‚    0.8204742074012756     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad toothbrush with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/4 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:03,  0.96it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:03,  0.96it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:01<00:01,  1.21it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:01<00:01,  1.21it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:02<00:00,  1.23it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:02<00:00,  1.23it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.39it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.39it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/3 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:02,  0.95it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:02<00:01,  0.99it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.01it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  0.15it/s, pixel_AUPRO=0.914]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:27<00:00,  0.15it/s, pixel_AUPRO=0.914]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:34<00:00,  0.12it/s, pixel_AUPRO=0.914]INFO:anomalib.callbacks.timer:Training took 34.59 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/3 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:09,  0.21it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:09<00:04,  0.22it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:11<00:00,  0.25it/s]INFO:anomalib.callbacks.timer:Testing took 31.073655605316162 seconds
Throughput (batch_size=17) : 1.3516272605149982 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:30<00:00,  0.10it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.947222113609314     â”‚
â”‚        pixel_AUPRO        â”‚    0.9149606823921204     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad transistor with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/13 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   8%|â–Š         | 1/13 [00:01<00:13,  0.92it/s]Epoch 0:   8%|â–Š         | 1/13 [00:01<00:13,  0.92it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:01<00:09,  1.15it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:01<00:09,  1.15it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:02<00:08,  1.16it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:02<00:08,  1.16it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:03<00:07,  1.16it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:03<00:07,  1.16it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:04<00:06,  1.16it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:04<00:06,  1.16it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:05<00:06,  1.16it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:05<00:06,  1.16it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:06<00:05,  1.16it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:06<00:05,  1.16it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:06<00:04,  1.16it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:06<00:04,  1.16it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:07<00:03,  1.16it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:07<00:03,  1.16it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:08<00:02,  1.16it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:08<00:02,  1.16it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:09<00:01,  1.16it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:09<00:01,  1.16it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:10<00:00,  1.16it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:10<00:00,  1.16it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.21it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.21it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/6 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 1/6 [00:01<00:05,  0.94it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:02<00:04,  0.98it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:03<00:03,  0.99it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:04<00:02,  1.00it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:04<00:00,  1.01it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:05<00:00,  1.01it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:45<00:00,  0.28it/s, pixel_AUPRO=0.951]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:45<00:00,  0.28it/s, pixel_AUPRO=0.951]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [01:04<00:00,  0.20it/s, pixel_AUPRO=0.951]INFO:anomalib.callbacks.timer:Training took 65.62 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/6 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 1/6 [00:04<00:24,  0.20it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:09<00:19,  0.21it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:14<00:14,  0.21it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:18<00:09,  0.21it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:23<00:04,  0.22it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:27<00:00,  0.22it/s]INFO:anomalib.callbacks.timer:Testing took 62.291815996170044 seconds
Throughput (batch_size=17) : 1.605347322128936 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:01<00:00,  0.10it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9925000667572021     â”‚
â”‚        pixel_AUPRO        â”‚    0.9506940841674805     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad wood with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.98it/s]Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.98it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.22it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.22it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.23it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.23it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:09,  1.21it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:09,  1.21it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:08,  1.20it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:08,  1.20it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.21it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.21it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.21it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.21it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.21it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.21it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.21it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.21it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:08<00:04,  1.21it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:08<00:04,  1.21it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:09<00:03,  1.22it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:09<00:03,  1.22it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.22it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.22it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.22it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.22it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.22it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.22it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.26it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.26it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.97it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:02,  1.01it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:02<00:01,  1.03it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:03<00:00,  1.04it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.04it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:27<00:00,  0.17it/s, pixel_AUPRO=0.876]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:27<00:00,  0.17it/s, pixel_AUPRO=0.876]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:55<00:00,  0.13it/s, pixel_AUPRO=0.876]INFO:anomalib.callbacks.timer:Training took 116.38 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:04<00:19,  0.20it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:09<00:13,  0.21it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:13<00:09,  0.21it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:18<00:04,  0.22it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:21<00:00,  0.23it/s]INFO:anomalib.callbacks.timer:Testing took 122.91199564933777 seconds
Throughput (batch_size=17) : 0.6427362893478953 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:01<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9868420958518982     â”‚
â”‚        pixel_AUPRO        â”‚    0.8780173659324646     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad zipper with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:00<00:09,  1.47it/s]Epoch 0:   7%|â–‹         | 1/15 [00:00<00:09,  1.47it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:00<00:06,  2.15it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:00<00:06,  2.15it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:01<00:05,  2.20it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:01<00:05,  2.20it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:01<00:04,  2.22it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:01<00:04,  2.22it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:02<00:04,  2.24it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:02<00:04,  2.24it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:02<00:04,  2.25it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:02<00:04,  2.25it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:03<00:03,  2.26it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:03<00:03,  2.26it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:03<00:03,  2.27it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:03<00:03,  2.27it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:03<00:02,  2.27it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:03<00:02,  2.27it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:04<00:02,  2.28it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:04<00:02,  2.28it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:04<00:01,  2.28it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:04<00:01,  2.28it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:05<00:01,  2.28it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:05<00:01,  2.28it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:05<00:00,  2.28it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:05<00:00,  2.28it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:06<00:00,  2.28it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:06<00:00,  2.28it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:06<00:00,  2.42it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:06<00:00,  2.42it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:01<00:08,  0.97it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:01<00:06,  1.01it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:02<00:05,  1.02it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:03<00:04,  1.03it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:04<00:03,  1.03it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:05<00:02,  1.04it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:06<00:01,  1.04it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:07<00:00,  1.04it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:08<00:00,  1.04it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:16<00:00,  0.08it/s, pixel_AUPRO=0.941]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:16<00:00,  0.08it/s, pixel_AUPRO=0.941]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:45<00:00,  0.07it/s, pixel_AUPRO=0.941]INFO:anomalib.callbacks.timer:Training took 225.74 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:04<00:34,  0.23it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:08<00:30,  0.23it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:13<00:26,  0.23it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:17<00:21,  0.23it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:21<00:17,  0.23it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:25<00:12,  0.23it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:30<00:08,  0.23it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:34<00:04,  0.23it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:38<00:00,  0.23it/s]INFO:anomalib.callbacks.timer:Testing took 244.46888256072998 seconds
Throughput (batch_size=17) : 0.6176655221651336 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:03<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.943802535533905     â”‚
â”‚        pixel_AUPRO        â”‚    0.9411241412162781     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa candle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.57it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.57it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.34it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.34it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.40it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.40it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.44it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.44it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.47it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.47it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:18,  2.48it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:18,  2.48it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.47it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.47it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.49it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.49it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.47it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.47it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.43it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.43it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.44it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.44it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.44it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.44it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.44it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.44it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.45it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.45it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.45it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.45it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.46it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.46it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.45it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.45it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.46it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.46it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.46it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.46it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.46it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.46it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.47it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.47it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.47it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.47it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.47it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.47it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.47it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.47it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.48it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.48it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.48it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.48it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.48it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.48it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.48it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.48it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.48it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.48it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.48it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.48it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.48it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.48it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.48it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.48it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.48it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.48it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.48it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.48it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.48it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.48it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.48it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.48it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.48it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.48it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.48it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.48it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.48it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.48it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.48it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.48it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.48it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.48it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.48it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.48it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.48it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.48it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.48it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.48it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.48it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.48it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.48it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.48it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.48it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.48it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.48it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.48it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.48it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.48it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.47it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.47it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.47it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.47it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.48it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.48it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.48it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.48it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:01<00:11,  0.97it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:01<00:09,  1.02it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:08,  1.04it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:03<00:07,  1.05it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:04<00:06,  1.06it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:05<00:05,  1.06it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:06<00:04,  1.07it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:07<00:03,  1.07it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:08<00:02,  1.07it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:09<00:01,  1.07it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:10<00:00,  1.08it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.08it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [07:26<00:00,  0.12it/s, pixel_AUPRO=0.952]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [07:26<00:00,  0.12it/s, pixel_AUPRO=0.952]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [08:04<00:00,  0.11it/s, pixel_AUPRO=0.952]INFO:anomalib.callbacks.timer:Training took 485.56 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:44,  0.25it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:07<00:39,  0.25it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:12<00:36,  0.25it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:15<00:31,  0.25it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:19<00:27,  0.26it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:23<00:23,  0.26it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:27<00:19,  0.25it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:31<00:15,  0.26it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:35<00:11,  0.26it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:39<00:07,  0.26it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:43<00:03,  0.25it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:46<00:00,  0.26it/s]INFO:anomalib.callbacks.timer:Testing took 502.84648418426514 seconds
Throughput (batch_size=17) : 0.39773570322251905 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [08:22<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9251999855041504     â”‚
â”‚        pixel_AUPRO        â”‚    0.9524563550949097     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa capsules with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/32 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/32 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:19,  1.56it/s]Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:19,  1.56it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.29it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.29it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.32it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.32it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:12,  2.32it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:12,  2.32it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:11,  2.30it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:11,  2.30it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:11,  2.27it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:11,  2.27it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:03<00:11,  2.25it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:03<00:11,  2.25it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:10,  2.23it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:10,  2.23it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:04<00:10,  2.24it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:04<00:10,  2.24it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.25it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.25it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:09,  2.26it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:09,  2.26it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:05<00:08,  2.27it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:05<00:08,  2.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:08,  2.28it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:08,  2.27it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:06<00:07,  2.28it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:06<00:07,  2.28it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:07,  2.28it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:07,  2.28it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:07<00:07,  2.28it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:07<00:07,  2.28it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:07<00:06,  2.29it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:07<00:06,  2.29it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:07<00:06,  2.30it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:07<00:06,  2.30it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:08<00:05,  2.30it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:08<00:05,  2.30it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:05,  2.30it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:05,  2.30it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:09<00:04,  2.31it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:09<00:04,  2.31it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.31it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.31it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:09<00:03,  2.31it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:09<00:03,  2.31it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:10<00:03,  2.31it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:10<00:03,  2.31it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:10<00:03,  2.32it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:10<00:03,  2.32it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:11<00:02,  2.32it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:11<00:02,  2.32it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.32it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.32it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:12<00:01,  2.33it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:12<00:01,  2.33it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:12<00:01,  2.33it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:12<00:01,  2.33it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:12<00:00,  2.33it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:12<00:00,  2.33it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:13<00:00,  2.34it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:13<00:00,  2.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.35it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.35it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|â–ˆ         | 1/10 [00:00<00:06,  1.36it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:01<00:05,  1.45it/s][A
Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:02<00:04,  1.42it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:02<00:04,  1.40it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:03<00:03,  1.39it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:04<00:02,  1.39it/s][A
Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:04<00:02,  1.41it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:05<00:01,  1.43it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:06<00:00,  1.44it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.45it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:27<00:00,  0.37it/s, pixel_AUPRO=0.676]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:27<00:00,  0.37it/s, pixel_AUPRO=0.676]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:55<00:00,  0.28it/s, pixel_AUPRO=0.676]INFO:anomalib.callbacks.timer:Training took 115.80 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|â–ˆ         | 1/10 [00:03<00:31,  0.28it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:06<00:26,  0.30it/s]Testing DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:10<00:23,  0.30it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:13<00:19,  0.30it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:16<00:16,  0.31it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:19<00:13,  0.30it/s]Testing DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:22<00:09,  0.31it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:26<00:06,  0.31it/s]Testing DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:29<00:03,  0.31it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:31<00:00,  0.32it/s]INFO:anomalib.callbacks.timer:Testing took 135.24798464775085 seconds
Throughput (batch_size=17) : 1.1830120827065556 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:14<00:00,  0.07it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.6798332929611206     â”‚
â”‚        pixel_AUPRO        â”‚    0.6832558512687683     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa cashew with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.51it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.51it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.19it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.19it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.23it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.23it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.25it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.25it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.27it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.27it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.28it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.28it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.29it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.29it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.30it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.30it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.30it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.30it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.30it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.31it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.31it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.31it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.31it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.31it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.31it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.32it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.32it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.32it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.32it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.31it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.31it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.31it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.31it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.31it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.31it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.31it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.31it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.31it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.31it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.32it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.32it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.32it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.32it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.32it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.32it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.32it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.32it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.32it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.32it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.32it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.32it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.36it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:01<00:08,  0.95it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:02<00:07,  1.00it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:02<00:05,  1.02it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:03<00:04,  1.04it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:04<00:03,  1.05it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:05<00:02,  1.05it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:06<00:01,  1.06it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:07<00:00,  1.06it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:08<00:00,  1.06it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:24<00:00,  0.19it/s, pixel_AUPRO=0.886]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:24<00:00,  0.19it/s, pixel_AUPRO=0.886]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:03<00:00,  0.15it/s, pixel_AUPRO=0.886]INFO:anomalib.callbacks.timer:Training took 184.11 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:04<00:32,  0.25it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:07<00:26,  0.26it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:11<00:23,  0.26it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:15<00:18,  0.26it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:18<00:15,  0.27it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:22<00:11,  0.27it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:26<00:07,  0.27it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:29<00:03,  0.27it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:33<00:00,  0.27it/s]INFO:anomalib.callbacks.timer:Testing took 193.59552121162415 seconds
Throughput (batch_size=17) : 0.7748113130986704 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [03:12<00:00,  0.05it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9638000726699829     â”‚
â”‚        pixel_AUPRO        â”‚    0.8855357766151428     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa chewinggum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.56it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.56it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.32it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.32it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.23it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.23it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.23it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.23it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.27it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.27it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.30it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.30it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.32it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.32it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.33it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.33it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.34it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.34it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.36it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.36it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.37it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.38it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.38it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.39it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.39it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.39it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.39it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.40it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.40it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.41it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.41it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.41it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.41it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.41it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.41it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.41it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.41it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.42it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.42it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.42it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.42it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.42it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.42it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.43it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.43it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.43it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.43it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.43it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.43it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.43it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.43it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.47it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.47it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:00<00:07,  1.07it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:01<00:06,  1.12it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:02<00:05,  1.14it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:03<00:04,  1.15it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:04<00:03,  1.16it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:05<00:02,  1.16it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:05<00:01,  1.17it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:06<00:00,  1.17it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:07<00:00,  1.17it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:09<00:00,  0.09it/s, pixel_AUPRO=0.780]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:09<00:00,  0.09it/s, pixel_AUPRO=0.780]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:44<00:00,  0.08it/s, pixel_AUPRO=0.780]INFO:anomalib.callbacks.timer:Training took 344.95 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:04<00:32,  0.25it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:07<00:26,  0.26it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:11<00:22,  0.27it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:14<00:18,  0.27it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:17<00:14,  0.28it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:21<00:10,  0.28it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:24<00:07,  0.28it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:28<00:03,  0.28it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:31<00:00,  0.29it/s]INFO:anomalib.callbacks.timer:Testing took 408.7973642349243 seconds
Throughput (batch_size=17) : 0.36692995876020185 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [06:47<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9945999383926392     â”‚
â”‚        pixel_AUPRO        â”‚    0.7803299427032471     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.52it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.52it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.27it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.27it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.32it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.32it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.34it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.34it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.35it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.35it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.32it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.32it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.29it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.29it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.26it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.26it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.27it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.27it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.28it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.28it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.29it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.29it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.30it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.30it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.30it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.30it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.31it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.31it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.31it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.31it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.31it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.31it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.32it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.31it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.32it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.32it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.33it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.33it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.33it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.33it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.34it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.34it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.34it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.34it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.35it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.35it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.35it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.35it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.35it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.35it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.35it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.35it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.40it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.40it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:00<00:05,  1.35it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:01<00:04,  1.44it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:02<00:04,  1.41it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:02<00:03,  1.40it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:03<00:02,  1.38it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:04<00:02,  1.40it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:04<00:01,  1.41it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:05<00:00,  1.43it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:06<00:00,  1.44it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:48<00:00,  0.12it/s, pixel_AUPRO=0.836]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:48<00:00,  0.12it/s, pixel_AUPRO=0.836]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:13<00:00,  0.11it/s, pixel_AUPRO=0.836]INFO:anomalib.callbacks.timer:Training took 254.20 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:03<00:28,  0.28it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:06<00:23,  0.29it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:10<00:20,  0.30it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:13<00:16,  0.30it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:16<00:13,  0.30it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:19<00:09,  0.30it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:22<00:06,  0.31it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:26<00:03,  0.31it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:28<00:00,  0.31it/s]INFO:anomalib.callbacks.timer:Testing took 272.9726767539978 seconds
Throughput (batch_size=17) : 0.5495055467957314 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:32<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.920799970626831     â”‚
â”‚        pixel_AUPRO        â”‚    0.8355889916419983     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:32,  1.59it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:32,  1.59it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.38it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.38it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.43it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.43it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:19,  2.48it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:19,  2.48it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.50it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.50it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:18,  2.52it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:18,  2.52it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.53it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.53it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:17,  2.54it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:17,  2.54it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.55it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.55it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:03<00:16,  2.55it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:03<00:16,  2.55it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.56it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.56it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:15,  2.57it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:15,  2.57it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:15,  2.57it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:15,  2.57it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.57it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.57it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:05<00:14,  2.58it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:05<00:14,  2.58it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.58it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.58it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:13,  2.57it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:13,  2.57it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:06<00:13,  2.57it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:06<00:13,  2.57it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.57it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.57it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:07<00:12,  2.57it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:07<00:12,  2.57it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.57it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.57it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.58it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.58it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:08<00:11,  2.58it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:08<00:11,  2.58it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.58it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.58it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:10,  2.58it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:10,  2.58it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.58it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.58it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.57it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.57it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:10<00:09,  2.57it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:10<00:09,  2.57it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.57it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.57it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:08,  2.57it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:08,  2.57it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.57it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.57it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.57it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.57it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:12<00:07,  2.57it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:12<00:07,  2.57it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.57it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.57it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.57it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.57it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.57it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.57it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.57it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.57it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:14<00:05,  2.57it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:14<00:05,  2.57it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.57it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.57it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.57it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.57it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:15<00:04,  2.57it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:15<00:04,  2.57it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.57it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.57it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:16<00:03,  2.57it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:16<00:03,  2.57it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.57it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.57it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.57it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.57it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:17<00:02,  2.57it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:17<00:02,  2.57it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.57it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.57it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:18<00:01,  2.57it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:18<00:01,  2.57it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.57it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.57it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.57it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.57it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:19<00:00,  2.57it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:19<00:00,  2.57it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.57it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.57it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:20<00:00,  2.57it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:20<00:00,  2.57it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:00<00:08,  1.35it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:01<00:07,  1.42it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:06,  1.44it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:02<00:05,  1.38it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:03<00:05,  1.38it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:04<00:04,  1.40it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:04<00:03,  1.42it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:05<00:02,  1.43it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:06<00:02,  1.45it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:06<00:01,  1.46it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:07<00:00,  1.46it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:08<00:00,  1.47it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:27<00:00,  0.16it/s, pixel_AUPRO=0.918]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:27<00:00,  0.16it/s, pixel_AUPRO=0.918]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:55<00:00,  0.15it/s, pixel_AUPRO=0.918]INFO:anomalib.callbacks.timer:Training took 355.92 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:38,  0.28it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:06<00:33,  0.30it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:09<00:29,  0.30it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:13<00:26,  0.31it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:16<00:22,  0.31it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:19<00:19,  0.31it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:22<00:16,  0.31it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:25<00:12,  0.31it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:28<00:09,  0.31it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:32<00:06,  0.31it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:36<00:03,  0.30it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:39<00:00,  0.31it/s]INFO:anomalib.callbacks.timer:Testing took 338.3304178714752 seconds
Throughput (batch_size=17) : 0.5911380988391528 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [05:37<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9079999327659607     â”‚
â”‚        pixel_AUPRO        â”‚    0.9179656505584717     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.55it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.55it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.32it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.32it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.38it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.38it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.40it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.40it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.42it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.42it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.44it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.44it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.45it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.45it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.45it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.45it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.46it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.46it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.46it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.46it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.47it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.47it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.47it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.47it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.47it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.47it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.47it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.47it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.47it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.47it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.47it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.47it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.48it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.48it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.47it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.47it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.47it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.47it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.47it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.47it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.47it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.47it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.48it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.48it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.48it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.48it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.48it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.48it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.48it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.48it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.48it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.48it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.48it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.48it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.49it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.49it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.49it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.49it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.49it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.49it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.49it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.49it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.49it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.49it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.49it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.49it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.49it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.49it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.49it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.49it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.49it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.49it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.49it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.49it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.49it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.49it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.49it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.49it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.49it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.49it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.49it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.49it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.49it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.49it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.49it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.49it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.50it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.50it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.50it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.50it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.50it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.50it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.50it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.50it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.50it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.50it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.50it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.50it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.50it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.50it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.50it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.50it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.50it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.50it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.50it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.50it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:00<00:08,  1.36it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:01<00:06,  1.44it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:06,  1.43it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:02<00:05,  1.41it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:03<00:05,  1.39it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:04<00:04,  1.40it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:04<00:03,  1.42it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:05<00:02,  1.44it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:06<00:02,  1.44it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:06<00:01,  1.45it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:07<00:00,  1.46it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:08<00:00,  1.47it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:30<00:00,  0.20it/s, pixel_AUPRO=0.790]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:30<00:00,  0.20it/s, pixel_AUPRO=0.790]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:58<00:00,  0.18it/s, pixel_AUPRO=0.790]INFO:anomalib.callbacks.timer:Training took 299.34 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:39,  0.28it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:06<00:33,  0.30it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:10<00:31,  0.28it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:14<00:28,  0.28it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:17<00:24,  0.28it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:21<00:21,  0.28it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:24<00:17,  0.28it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:28<00:14,  0.28it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:31<00:10,  0.28it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:34<00:06,  0.29it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:38<00:03,  0.29it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:41<00:00,  0.29it/s]INFO:anomalib.callbacks.timer:Testing took 201.3231770992279 seconds
Throughput (batch_size=17) : 0.9934275967710576 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [03:20<00:00,  0.06it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7024999856948853     â”‚
â”‚        pixel_AUPRO        â”‚    0.7903936505317688     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:35,  1.51it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:35,  1.51it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.20it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.20it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.24it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.24it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:22,  2.27it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:22,  2.27it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:21,  2.29it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:21,  2.29it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.30it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.30it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:20,  2.31it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:20,  2.30it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.31it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.31it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:19,  2.31it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:19,  2.31it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:19,  2.32it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:19,  2.31it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.32it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.32it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.33it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.33it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.33it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.33it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:17,  2.34it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:17,  2.34it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.34it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.34it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.33it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.33it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.34it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.34it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.34it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.34it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:14,  2.34it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:14,  2.34it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.34it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.34it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:14,  2.34it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:14,  2.34it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.35it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.35it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.35it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.35it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.35it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.35it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.35it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.35it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:11<00:11,  2.35it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:11<00:11,  2.35it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.36it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.36it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:11,  2.36it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:11,  2.36it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.35it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.35it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.35it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.35it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.35it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.35it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.36it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.36it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.36it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.36it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.36it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.36it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:08,  2.36it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:08,  2.36it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.36it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.36it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.36it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.36it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:16<00:06,  2.36it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:16<00:06,  2.36it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.36it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.36it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.36it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.36it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.37it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.37it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:05,  2.36it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:05,  2.36it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:18<00:04,  2.37it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:18<00:04,  2.37it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.37it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.37it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:19<00:03,  2.36it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:19<00:03,  2.36it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.36it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.36it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.37it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.37it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:20<00:02,  2.37it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:20<00:02,  2.37it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.37it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.37it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:21<00:01,  2.37it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:21<00:01,  2.37it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.37it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.37it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.37it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.37it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:22<00:00,  2.37it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:22<00:00,  2.37it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.41it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.41it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:00<00:09,  1.16it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:01<00:08,  1.24it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:07,  1.27it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:03<00:06,  1.29it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:03<00:05,  1.29it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:04<00:04,  1.30it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:05<00:03,  1.31it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:06<00:03,  1.31it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:06<00:02,  1.31it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:07<00:01,  1.32it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:08<00:00,  1.32it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:09<00:00,  1.32it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:28<00:00,  0.20it/s, pixel_AUPRO=0.881]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:28<00:00,  0.20it/s, pixel_AUPRO=0.881]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [05:00<00:00,  0.18it/s, pixel_AUPRO=0.881]INFO:anomalib.callbacks.timer:Training took 301.27 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:42,  0.26it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:07<00:35,  0.28it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:10<00:32,  0.28it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:14<00:28,  0.28it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:17<00:25,  0.28it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:21<00:21,  0.28it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:25<00:18,  0.27it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:29<00:14,  0.27it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:33<00:11,  0.27it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:37<00:07,  0.27it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:41<00:03,  0.27it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:44<00:00,  0.27it/s]INFO:anomalib.callbacks.timer:Testing took 352.30428552627563 seconds
Throughput (batch_size=17) : 0.5676910790376507 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [05:51<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9182000160217285     â”‚
â”‚        pixel_AUPRO        â”‚    0.8809725642204285     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.49it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.49it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:23,  2.20it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:23,  2.20it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:22,  2.27it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:22,  2.27it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:21,  2.31it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:21,  2.30it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.33it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.33it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:20,  2.35it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:20,  2.35it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.36it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.36it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.37it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.37it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.34it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.34it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.35it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.35it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.35it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.35it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.35it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.35it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.36it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.36it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.36it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.36it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.36it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.36it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.36it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.36it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.36it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.36it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.36it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.36it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.36it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.36it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.37it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.37it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.37it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.37it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.37it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.37it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.37it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.37it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.37it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.37it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.37it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.37it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.38it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.38it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.38it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.38it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.38it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.38it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:10,  2.38it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:10,  2.38it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.38it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.38it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.39it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.39it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.39it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.39it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.39it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.39it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.39it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.39it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.39it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.39it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:15<00:07,  2.39it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:15<00:07,  2.39it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.40it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.40it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.40it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.40it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.40it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.40it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.40it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.40it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:17<00:05,  2.40it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:17<00:05,  2.40it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.40it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.40it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.40it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.40it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.40it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.40it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.40it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.40it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.41it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.41it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.41it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.41it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.41it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.41it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.41it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.41it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.41it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.41it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.41it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.41it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.41it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.41it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:22<00:00,  2.41it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:22<00:00,  2.41it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:00<00:09,  1.19it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:01<00:08,  1.25it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:07,  1.27it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:03<00:06,  1.29it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:03<00:05,  1.30it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:04<00:04,  1.31it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:05<00:03,  1.31it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:06<00:03,  1.32it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:06<00:02,  1.32it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:07<00:01,  1.32it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:08<00:00,  1.32it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:09<00:00,  1.32it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:56<00:00,  0.18it/s, pixel_AUPRO=0.886]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:56<00:00,  0.18it/s, pixel_AUPRO=0.886]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:28<00:00,  0.16it/s, pixel_AUPRO=0.886]INFO:anomalib.callbacks.timer:Training took 329.11 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:42,  0.26it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:07<00:35,  0.28it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:10<00:32,  0.28it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:14<00:28,  0.28it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:17<00:24,  0.29it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:21<00:21,  0.28it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:25<00:18,  0.28it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:28<00:14,  0.28it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:32<00:10,  0.28it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:36<00:07,  0.28it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:40<00:03,  0.27it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:43<00:00,  0.28it/s]INFO:anomalib.callbacks.timer:Testing took 386.37844610214233 seconds
Throughput (batch_size=17) : 0.517627217609153 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [06:25<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9546999931335449     â”‚
â”‚        pixel_AUPRO        â”‚    0.8861170411109924     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb3 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:33,  1.57it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:33,  1.57it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:21,  2.40it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:21,  2.40it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:20,  2.48it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:20,  2.48it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:19,  2.52it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:19,  2.52it/s]Epoch 0:   9%|â–‰         | 5/54 [00:01<00:19,  2.55it/s]Epoch 0:   9%|â–‰         | 5/54 [00:01<00:19,  2.55it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:19,  2.51it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:19,  2.51it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.44it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.44it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.40it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.40it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.42it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.42it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.43it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.43it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:17,  2.45it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:17,  2.45it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:17,  2.46it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:17,  2.46it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:16,  2.46it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:16,  2.46it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.47it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.47it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:15,  2.48it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:15,  2.48it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.48it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.48it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:14,  2.49it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:14,  2.49it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:14,  2.49it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:14,  2.49it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.50it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.50it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:07<00:13,  2.50it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:07<00:13,  2.50it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.51it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.51it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:08<00:12,  2.51it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:08<00:12,  2.51it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.51it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.51it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:11,  2.52it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:11,  2.52it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:09<00:11,  2.52it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:09<00:11,  2.52it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.52it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.52it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:10<00:10,  2.52it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:10<00:10,  2.52it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.52it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.52it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:09,  2.53it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:09,  2.53it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:11<00:09,  2.53it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:11<00:09,  2.53it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.53it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.53it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:12<00:08,  2.53it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:12<00:08,  2.53it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.53it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.53it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:07,  2.53it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:07,  2.53it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:13<00:07,  2.53it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:13<00:07,  2.53it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.54it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.54it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:14<00:06,  2.54it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:14<00:06,  2.54it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:14<00:06,  2.54it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:14<00:06,  2.54it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:15<00:05,  2.54it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:15<00:05,  2.54it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:15<00:05,  2.54it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:15<00:05,  2.54it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.54it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.54it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:16<00:04,  2.54it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:16<00:04,  2.54it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:16<00:04,  2.54it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:16<00:04,  2.54it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:17<00:03,  2.55it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:17<00:03,  2.55it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:17<00:03,  2.55it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:17<00:03,  2.55it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.55it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.55it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:18<00:02,  2.55it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:18<00:02,  2.55it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:18<00:02,  2.55it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:18<00:02,  2.55it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:19<00:01,  2.55it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:19<00:01,  2.55it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:19<00:01,  2.55it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:19<00:01,  2.55it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:19<00:01,  2.55it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:19<00:01,  2.55it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:20<00:00,  2.55it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:20<00:00,  2.55it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:20<00:00,  2.55it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:20<00:00,  2.55it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:20<00:00,  2.59it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:20<00:00,  2.59it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:00<00:07,  1.45it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:01<00:06,  1.51it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:06,  1.48it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:02<00:05,  1.47it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:03<00:04,  1.46it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:04<00:04,  1.47it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:04<00:03,  1.49it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:05<00:02,  1.51it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:05<00:01,  1.52it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:06<00:01,  1.53it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:07<00:00,  1.55it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:07<00:00,  1.55it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:06<00:00,  0.22it/s, pixel_AUPRO=0.868]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:06<00:00,  0.22it/s, pixel_AUPRO=0.868]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:33<00:00,  0.20it/s, pixel_AUPRO=0.868]INFO:anomalib.callbacks.timer:Training took 274.51 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:38,  0.29it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:06<00:33,  0.30it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:09<00:29,  0.30it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:13<00:26,  0.31it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:16<00:22,  0.31it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:19<00:19,  0.31it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:22<00:16,  0.31it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:25<00:12,  0.31it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:28<00:09,  0.31it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:31<00:06,  0.32it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:34<00:03,  0.31it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:37<00:00,  0.32it/s]INFO:anomalib.callbacks.timer:Testing took 296.8702247142792 seconds
Throughput (batch_size=17) : 0.6770635222628041 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [04:56<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9522771835327148     â”‚
â”‚        pixel_AUPRO        â”‚    0.8678251504898071     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb4 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:33,  1.56it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:33,  1.56it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.30it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.30it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.36it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.36it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:20,  2.39it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:20,  2.39it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.42it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.42it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:19,  2.44it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:19,  2.44it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.45it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.45it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:18,  2.45it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:18,  2.45it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.46it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.46it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:17,  2.47it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:17,  2.47it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:17,  2.47it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:17,  2.47it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:16,  2.47it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:16,  2.47it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:16,  2.48it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:16,  2.48it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.48it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.48it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:15,  2.48it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:15,  2.48it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.48it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.48it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:14,  2.48it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:14,  2.48it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:14,  2.48it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:14,  2.48it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.48it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.48it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:13,  2.49it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:13,  2.49it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.49it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.49it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:08<00:12,  2.49it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:08<00:12,  2.49it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.49it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.49it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:12,  2.49it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:12,  2.49it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:11,  2.49it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:11,  2.49it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.49it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.49it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:10<00:10,  2.49it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:10<00:10,  2.49it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.49it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.49it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:10,  2.50it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:10,  2.50it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.49it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.49it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.49it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.49it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:12<00:08,  2.50it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:12<00:08,  2.50it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.50it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.50it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:08,  2.49it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:08,  2.49it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.50it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.50it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.50it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.50it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:14<00:06,  2.50it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:14<00:06,  2.50it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.50it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.50it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:15<00:05,  2.50it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:15<00:05,  2.50it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:15<00:05,  2.50it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:15<00:05,  2.50it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.50it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.50it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:16<00:04,  2.50it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:16<00:04,  2.50it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.51it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.51it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:17<00:03,  2.51it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:17<00:03,  2.51it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:17<00:03,  2.51it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:17<00:03,  2.51it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.50it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.50it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:18<00:02,  2.50it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:18<00:02,  2.50it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.51it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.50it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:19<00:01,  2.51it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:19<00:01,  2.51it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:19<00:01,  2.51it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:19<00:01,  2.51it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.51it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.51it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:20<00:00,  2.51it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:20<00:00,  2.51it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.51it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.51it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.55it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.55it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:00<00:10,  1.10it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:01<00:08,  1.15it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:07,  1.18it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:03<00:06,  1.19it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:04<00:05,  1.19it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:05<00:05,  1.20it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:05<00:04,  1.20it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:06<00:03,  1.20it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:07<00:02,  1.21it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:08<00:01,  1.21it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:09<00:00,  1.21it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:09<00:00,  1.21it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [07:26<00:00,  0.12it/s, pixel_AUPRO=0.868]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [07:26<00:00,  0.12it/s, pixel_AUPRO=0.868]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [08:00<00:00,  0.11it/s, pixel_AUPRO=0.868]INFO:anomalib.callbacks.timer:Training took 480.92 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:43,  0.25it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:07<00:38,  0.26it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:11<00:34,  0.26it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:15<00:30,  0.26it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:19<00:26,  0.26it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:23<00:23,  0.26it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:27<00:19,  0.26it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:31<00:15,  0.26it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:34<00:11,  0.26it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:38<00:07,  0.26it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:43<00:03,  0.25it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:46<00:00,  0.26it/s]INFO:anomalib.callbacks.timer:Testing took 588.4799914360046 seconds
Throughput (batch_size=17) : 0.3415579168792489 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [09:47<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9985148310661316     â”‚
â”‚        pixel_AUPRO        â”‚    0.8675538301467896     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pipe_fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.52it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.52it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.33it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.33it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.39it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.39it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.43it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.43it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:08,  2.46it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:08,  2.46it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.47it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.47it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.48it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.48it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.50it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.50it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.50it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.50it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:03<00:06,  2.51it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:03<00:06,  2.51it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.51it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.51it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.52it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.52it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.52it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.52it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.52it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.52it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.52it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.52it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.52it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.52it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.52it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.52it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.53it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.53it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.53it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.53it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.53it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.53it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.53it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.53it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.54it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.53it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.54it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.54it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.54it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.54it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.54it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.54it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.54it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.54it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.59it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.59it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:00<00:07,  1.03it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:01<00:06,  1.08it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:02<00:05,  1.10it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:03<00:04,  1.11it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:04<00:03,  1.11it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:05<00:02,  1.11it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:06<00:01,  1.12it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:07<00:00,  1.12it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:08<00:00,  1.12it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [06:06<00:00,  0.07it/s, pixel_AUPRO=0.936]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [06:06<00:00,  0.07it/s, pixel_AUPRO=0.936]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [06:43<00:00,  0.07it/s, pixel_AUPRO=0.936]INFO:anomalib.callbacks.timer:Training took 403.93 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:03<00:31,  0.26it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:07<00:26,  0.26it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:11<00:22,  0.26it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:15<00:18,  0.27it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:18<00:14,  0.27it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:22<00:11,  0.27it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:26<00:07,  0.27it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:29<00:03,  0.27it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:32<00:00,  0.27it/s]INFO:anomalib.callbacks.timer:Testing took 421.6517515182495 seconds
Throughput (batch_size=17) : 0.3557438086285475 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [07:00<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.989799976348877     â”‚
â”‚        pixel_AUPRO        â”‚    0.9358774423599243     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad bottle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/13 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   8%|â–Š         | 1/13 [00:00<00:09,  1.28it/s]Epoch 0:   8%|â–Š         | 1/13 [00:00<00:09,  1.28it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:01<00:06,  1.76it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:01<00:06,  1.76it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:05,  1.81it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:05,  1.81it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:04,  1.83it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:02<00:04,  1.83it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:04,  1.85it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:02<00:04,  1.85it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:03<00:03,  1.86it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:03<00:03,  1.86it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:03<00:03,  1.86it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:03<00:03,  1.86it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:04<00:02,  1.87it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:04<00:02,  1.87it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:04<00:02,  1.88it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:04<00:02,  1.88it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:05<00:01,  1.88it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:05<00:01,  1.88it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:05<00:01,  1.88it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:05<00:01,  1.88it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:06<00:00,  1.88it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:06<00:00,  1.88it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  1.99it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  1.99it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:05,  0.67it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:04,  0.69it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:04<00:02,  0.70it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:05<00:01,  0.70it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:07<00:00,  0.70it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:53<00:00,  0.24it/s, pixel_AUPRO=0.940]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:53<00:00,  0.24it/s, pixel_AUPRO=0.940]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [01:17<00:00,  0.17it/s, pixel_AUPRO=0.940]INFO:anomalib.callbacks.timer:Training took 77.91 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:05<00:20,  0.20it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:09<00:14,  0.21it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:14<00:09,  0.21it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:19<00:04,  0.21it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:23<00:00,  0.21it/s]INFO:anomalib.callbacks.timer:Testing took 66.31018328666687 seconds
Throughput (batch_size=17) : 1.2516931168955008 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:05<00:00,  0.08it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚            1.0            â”‚
â”‚        pixel_AUPRO        â”‚    0.9411426782608032     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad cable with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/14 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/14 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/14 [00:01<00:13,  0.94it/s]Epoch 0:   7%|â–‹         | 1/14 [00:01<00:13,  0.94it/s]Epoch 0:  14%|â–ˆâ–        | 2/14 [00:01<00:10,  1.19it/s]Epoch 0:  14%|â–ˆâ–        | 2/14 [00:01<00:10,  1.19it/s]Epoch 0:  21%|â–ˆâ–ˆâ–       | 3/14 [00:02<00:09,  1.20it/s]Epoch 0:  21%|â–ˆâ–ˆâ–       | 3/14 [00:02<00:09,  1.20it/s]Epoch 0:  29%|â–ˆâ–ˆâ–Š       | 4/14 [00:03<00:08,  1.21it/s]Epoch 0:  29%|â–ˆâ–ˆâ–Š       | 4/14 [00:03<00:08,  1.21it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 5/14 [00:04<00:07,  1.19it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 5/14 [00:04<00:07,  1.19it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6/14 [00:05<00:06,  1.19it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6/14 [00:05<00:06,  1.19it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 7/14 [00:05<00:05,  1.19it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 7/14 [00:05<00:05,  1.19it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 8/14 [00:06<00:05,  1.19it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 8/14 [00:06<00:05,  1.19it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 9/14 [00:07<00:04,  1.19it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 9/14 [00:07<00:04,  1.19it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 10/14 [00:08<00:03,  1.19it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 10/14 [00:08<00:03,  1.19it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 11/14 [00:09<00:02,  1.19it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 11/14 [00:09<00:02,  1.19it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12/14 [00:10<00:01,  1.19it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12/14 [00:10<00:01,  1.19it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 13/14 [00:10<00:00,  1.19it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 13/14 [00:10<00:00,  1.19it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:11<00:00,  1.27it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:11<00:00,  1.27it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:01<00:12,  0.64it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:03<00:10,  0.65it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:04<00:09,  0.66it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:06<00:07,  0.66it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:07<00:06,  0.66it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:09<00:04,  0.67it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:10<00:03,  0.67it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:11<00:01,  0.67it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:13<00:00,  0.67it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [02:24<00:00,  0.10it/s, pixel_AUPRO=0.923]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [02:24<00:00,  0.10it/s, pixel_AUPRO=0.923]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [02:51<00:00,  0.08it/s, pixel_AUPRO=0.923]INFO:anomalib.callbacks.timer:Training took 171.80 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:05<00:44,  0.18it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:10<00:37,  0.19it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:16<00:33,  0.18it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:21<00:27,  0.18it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:27<00:21,  0.18it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:32<00:16,  0.18it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:38<00:10,  0.18it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:43<00:05,  0.18it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:48<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 222.49390125274658 seconds
Throughput (batch_size=17) : 0.6741757825964154 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [03:41<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9642053842544556     â”‚
â”‚        pixel_AUPRO        â”‚    0.9236264228820801     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad capsule with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/13 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   8%|â–Š         | 1/13 [00:01<00:12,  0.95it/s]Epoch 0:   8%|â–Š         | 1/13 [00:01<00:12,  0.95it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:01<00:09,  1.18it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:01<00:09,  1.18it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:02<00:08,  1.19it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:02<00:08,  1.19it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:03<00:07,  1.20it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:03<00:07,  1.20it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:04<00:06,  1.20it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:04<00:06,  1.20it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:04<00:05,  1.20it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:04<00:05,  1.20it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:05<00:04,  1.21it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:05<00:04,  1.21it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:06<00:04,  1.21it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:06<00:04,  1.21it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:07<00:03,  1.21it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:07<00:03,  1.21it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:08<00:02,  1.21it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:08<00:02,  1.21it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:09<00:01,  1.21it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:09<00:01,  1.21it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:09<00:00,  1.21it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:09<00:00,  1.21it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.22it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.22it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/8 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s][A
Validation DataLoader 0:  12%|â–ˆâ–Ž        | 1/8 [00:01<00:10,  0.65it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:03<00:09,  0.66it/s][A
Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:04<00:07,  0.67it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:05<00:05,  0.67it/s][A
Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:07<00:04,  0.67it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:08<00:02,  0.67it/s][A
Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:10<00:01,  0.68it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:11<00:00,  0.68it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [02:19<00:00,  0.09it/s, pixel_AUPRO=0.941]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [02:19<00:00,  0.09it/s, pixel_AUPRO=0.941]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [02:45<00:00,  0.08it/s, pixel_AUPRO=0.941]INFO:anomalib.callbacks.timer:Training took 166.32 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/8 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]Testing DataLoader 0:  12%|â–ˆâ–Ž        | 1/8 [00:05<00:37,  0.19it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:10<00:31,  0.19it/s]Testing DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:15<00:26,  0.19it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:21<00:21,  0.19it/s]Testing DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:26<00:15,  0.19it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:31<00:10,  0.19it/s]Testing DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:36<00:05,  0.19it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:41<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 165.5352749824524 seconds
Throughput (batch_size=17) : 0.797413119433261 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:44<00:00,  0.05it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9908256530761719     â”‚
â”‚        pixel_AUPRO        â”‚    0.9406978487968445     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad carpet with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/17 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/17 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.99it/s]Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.99it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:12,  1.24it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:12,  1.24it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:11,  1.25it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:11,  1.25it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:10,  1.25it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:10,  1.25it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:03<00:09,  1.26it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:03<00:09,  1.26it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:04<00:08,  1.26it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:04<00:08,  1.26it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:05<00:07,  1.26it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:05<00:07,  1.26it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:06<00:07,  1.27it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:06<00:07,  1.26it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:07<00:06,  1.26it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:07<00:06,  1.26it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:07<00:05,  1.26it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:07<00:05,  1.26it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:08<00:04,  1.27it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:08<00:04,  1.27it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:09<00:03,  1.27it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:09<00:03,  1.27it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:10<00:03,  1.27it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:10<00:03,  1.27it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:11<00:02,  1.27it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:11<00:02,  1.27it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:11<00:01,  1.27it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:11<00:01,  1.27it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:12<00:00,  1.27it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:12<00:00,  1.27it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:12<00:00,  1.31it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:12<00:00,  1.31it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:11,  0.53it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:03<00:09,  0.55it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:05<00:07,  0.55it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:07<00:05,  0.55it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:09<00:03,  0.55it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:10<00:01,  0.56it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:12<00:00,  0.56it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:31<00:00,  0.19it/s, pixel_AUPRO=0.945]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:31<00:00,  0.19it/s, pixel_AUPRO=0.945]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [02:03<00:00,  0.14it/s, pixel_AUPRO=0.945]INFO:anomalib.callbacks.timer:Training took 124.51 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:05<00:34,  0.18it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:27,  0.18it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:22,  0.18it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:22<00:17,  0.17it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:28<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:34<00:05,  0.18it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 127.00908637046814 seconds
Throughput (batch_size=17) : 0.9211939345719486 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [02:06<00:00,  0.06it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9803370833396912     â”‚
â”‚        pixel_AUPRO        â”‚    0.9451927542686462     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad grid with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/16 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   6%|â–‹         | 1/16 [00:00<00:10,  1.41it/s]Epoch 0:   6%|â–‹         | 1/16 [00:00<00:10,  1.41it/s]Epoch 0:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:06,  2.02it/s]Epoch 0:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:06,  2.02it/s]Epoch 0:  19%|â–ˆâ–‰        | 3/16 [00:01<00:06,  2.06it/s]Epoch 0:  19%|â–ˆâ–‰        | 3/16 [00:01<00:06,  2.06it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:05,  2.06it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:05,  2.06it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:02<00:05,  2.08it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:02<00:05,  2.08it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:02<00:04,  2.09it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:02<00:04,  2.09it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:03<00:04,  2.10it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:03<00:04,  2.10it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:03<00:03,  2.10it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:03<00:03,  2.10it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:04<00:03,  2.10it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:04<00:03,  2.10it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:04<00:02,  2.11it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:04<00:02,  2.11it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:05<00:02,  2.11it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:05<00:02,  2.11it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:05<00:01,  2.11it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:05<00:01,  2.11it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:06<00:01,  2.12it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:06<00:01,  2.12it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:06<00:00,  2.12it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:06<00:00,  2.12it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:07<00:00,  2.12it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:07<00:00,  2.12it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:07<00:00,  2.18it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:07<00:00,  2.18it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:07,  0.56it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:03<00:05,  0.57it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:05<00:03,  0.58it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:06<00:01,  0.58it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:08<00:00,  0.58it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:31<00:00,  0.18it/s, pixel_AUPRO=0.910]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:31<00:00,  0.18it/s, pixel_AUPRO=0.910]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [02:01<00:00,  0.13it/s, pixel_AUPRO=0.910]INFO:anomalib.callbacks.timer:Training took 122.53 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:05<00:20,  0.19it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:10<00:15,  0.20it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:15<00:10,  0.20it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:20<00:05,  0.20it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:23<00:00,  0.21it/s]INFO:anomalib.callbacks.timer:Testing took 125.67368507385254 seconds
Throughput (batch_size=17) : 0.6206549919671971 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:05<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9832915663719177     â”‚
â”‚        pixel_AUPRO        â”‚    0.9100852608680725     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad hazelnut with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/23 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/23 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–         | 1/23 [00:01<00:23,  0.95it/s]Epoch 0:   4%|â–         | 1/23 [00:01<00:23,  0.95it/s]Epoch 0:   9%|â–Š         | 2/23 [00:01<00:17,  1.20it/s]Epoch 0:   9%|â–Š         | 2/23 [00:01<00:17,  1.20it/s]Epoch 0:  13%|â–ˆâ–Ž        | 3/23 [00:02<00:16,  1.22it/s]Epoch 0:  13%|â–ˆâ–Ž        | 3/23 [00:02<00:16,  1.22it/s]Epoch 0:  17%|â–ˆâ–‹        | 4/23 [00:03<00:15,  1.23it/s]Epoch 0:  17%|â–ˆâ–‹        | 4/23 [00:03<00:15,  1.23it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 5/23 [00:04<00:14,  1.23it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 5/23 [00:04<00:14,  1.23it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 6/23 [00:04<00:13,  1.24it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 6/23 [00:04<00:13,  1.24it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:05<00:12,  1.23it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:05<00:12,  1.23it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 8/23 [00:06<00:12,  1.24it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 8/23 [00:06<00:12,  1.24it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:07<00:11,  1.24it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:07<00:11,  1.24it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10/23 [00:08<00:10,  1.24it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10/23 [00:08<00:10,  1.24it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:08<00:09,  1.24it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:08<00:09,  1.24it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/23 [00:09<00:08,  1.24it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/23 [00:09<00:08,  1.24it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:10<00:08,  1.24it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:10<00:08,  1.24it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14/23 [00:11<00:07,  1.24it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14/23 [00:11<00:07,  1.24it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:12<00:06,  1.24it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:12<00:06,  1.24it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16/23 [00:12<00:05,  1.24it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16/23 [00:12<00:05,  1.24it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:13<00:04,  1.25it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:13<00:04,  1.25it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18/23 [00:14<00:04,  1.25it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18/23 [00:14<00:04,  1.25it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:15<00:03,  1.25it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:15<00:03,  1.25it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20/23 [00:16<00:02,  1.25it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20/23 [00:16<00:02,  1.25it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:16<00:01,  1.25it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:16<00:01,  1.25it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22/23 [00:17<00:00,  1.25it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22/23 [00:17<00:00,  1.25it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:18<00:00,  1.25it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:18<00:00,  1.25it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:02<00:13,  0.43it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:04<00:11,  0.44it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:06<00:09,  0.44it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:09<00:06,  0.44it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:11<00:04,  0.44it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:13<00:02,  0.44it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:15<00:00,  0.44it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [02:08<00:00,  0.18it/s, pixel_AUPRO=0.948]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [02:08<00:00,  0.18it/s, pixel_AUPRO=0.948]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [02:50<00:00,  0.14it/s, pixel_AUPRO=0.948]INFO:anomalib.callbacks.timer:Training took 171.25 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:37,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:30,  0.16it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:18<00:24,  0.16it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:24<00:18,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:30<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:36<00:06,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:40<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 150.45907282829285 seconds
Throughput (batch_size=17) : 0.7310958251453163 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [02:29<00:00,  0.05it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9978570938110352     â”‚
â”‚        pixel_AUPRO        â”‚    0.9506927728652954     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad leather with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.06it/s]Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.06it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:09,  1.39it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:09,  1.39it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:08,  1.40it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:08,  1.40it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:02<00:07,  1.42it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:02<00:07,  1.42it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.42it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.42it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.42it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.42it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:04<00:05,  1.43it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:04<00:05,  1.43it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:05<00:04,  1.43it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:05<00:04,  1.43it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.43it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.43it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:06<00:03,  1.43it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:06<00:03,  1.43it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:07<00:02,  1.44it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:07<00:02,  1.44it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:08<00:02,  1.44it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:08<00:02,  1.44it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.43it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.43it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:09<00:00,  1.44it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:09<00:00,  1.44it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.49it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.49it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/8 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s][A
Validation DataLoader 0:  12%|â–ˆâ–Ž        | 1/8 [00:01<00:11,  0.59it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:03<00:09,  0.60it/s][A
Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:04<00:08,  0.61it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:06<00:06,  0.61it/s][A
Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:08<00:04,  0.61it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:09<00:03,  0.61it/s][A
Validation DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:11<00:01,  0.61it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  0.62it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:40<00:00,  0.15it/s, pixel_AUPRO=0.969]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:40<00:00,  0.15it/s, pixel_AUPRO=0.969]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:08<00:00,  0.12it/s, pixel_AUPRO=0.969]INFO:anomalib.callbacks.timer:Training took 129.25 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/8 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]Testing DataLoader 0:  12%|â–ˆâ–Ž        | 1/8 [00:05<00:39,  0.18it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:11<00:33,  0.18it/s]Testing DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:16<00:27,  0.18it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:22<00:22,  0.18it/s]Testing DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:27<00:16,  0.18it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:33<00:11,  0.18it/s]Testing DataLoader 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:38<00:05,  0.18it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:41<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 138.15080285072327 seconds
Throughput (batch_size=17) : 0.8975698833541076 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:17<00:00,  0.06it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9989809393882751     â”‚
â”‚        pixel_AUPRO        â”‚    0.9692407250404358     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad metal_nut with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/13 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   8%|â–Š         | 1/13 [00:00<00:07,  1.61it/s]Epoch 0:   8%|â–Š         | 1/13 [00:00<00:07,  1.61it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:00<00:04,  2.43it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:00<00:04,  2.43it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:04,  2.48it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:01<00:04,  2.48it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:01<00:03,  2.50it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:01<00:03,  2.50it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:01<00:03,  2.52it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:01<00:03,  2.52it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:02<00:02,  2.53it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:02<00:02,  2.53it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:02<00:02,  2.54it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:02<00:02,  2.54it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:03<00:01,  2.55it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:03<00:01,  2.55it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:03<00:01,  2.56it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:03<00:01,  2.56it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:03<00:01,  2.56it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:03<00:01,  2.56it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:04<00:00,  2.56it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:04<00:00,  2.56it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:04<00:00,  2.57it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:04<00:00,  2.57it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:05<00:00,  2.57it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:05<00:00,  2.57it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:09,  0.64it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:03<00:07,  0.66it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:04<00:05,  0.67it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:05<00:04,  0.67it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:07<00:02,  0.67it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:08<00:01,  0.67it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:10<00:00,  0.68it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [01:43<00:00,  0.13it/s, pixel_AUPRO=0.942]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [01:43<00:00,  0.13it/s, pixel_AUPRO=0.942]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [02:10<00:00,  0.10it/s, pixel_AUPRO=0.942]INFO:anomalib.callbacks.timer:Training took 131.20 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:04<00:29,  0.20it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:09<00:23,  0.21it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:15<00:20,  0.20it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:19<00:14,  0.20it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:24<00:09,  0.20it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:29<00:04,  0.20it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:34<00:00,  0.20it/s]INFO:anomalib.callbacks.timer:Testing took 141.24584770202637 seconds
Throughput (batch_size=17) : 0.8141832264167166 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [02:20<00:00,  0.05it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9985337257385254     â”‚
â”‚        pixel_AUPRO        â”‚    0.9428617358207703     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad pill with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/16 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   6%|â–‹         | 1/16 [00:00<00:11,  1.31it/s]Epoch 0:   6%|â–‹         | 1/16 [00:00<00:11,  1.31it/s]Epoch 0:  12%|â–ˆâ–Ž        | 2/16 [00:01<00:07,  1.83it/s]Epoch 0:  12%|â–ˆâ–Ž        | 2/16 [00:01<00:07,  1.83it/s]Epoch 0:  19%|â–ˆâ–‰        | 3/16 [00:01<00:06,  1.86it/s]Epoch 0:  19%|â–ˆâ–‰        | 3/16 [00:01<00:06,  1.86it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:02<00:06,  1.88it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:02<00:06,  1.88it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:02<00:05,  1.89it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:02<00:05,  1.89it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:03<00:05,  1.90it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:03<00:05,  1.90it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:03<00:04,  1.90it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:03<00:04,  1.90it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:04<00:04,  1.91it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:04<00:04,  1.91it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:04<00:03,  1.91it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:04<00:03,  1.91it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:05<00:03,  1.91it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:05<00:03,  1.91it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:05<00:02,  1.92it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:05<00:02,  1.92it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:06<00:02,  1.92it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:06<00:02,  1.92it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:06<00:01,  1.92it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:06<00:01,  1.92it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:07<00:01,  1.92it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:07<00:01,  1.92it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:07<00:00,  1.92it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:07<00:00,  1.92it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:08<00:00,  1.95it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:08<00:00,  1.95it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|â–ˆ         | 1/10 [00:01<00:16,  0.55it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:03<00:14,  0.56it/s][A
Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:05<00:12,  0.57it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:06<00:10,  0.57it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:08<00:08,  0.57it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:10<00:06,  0.57it/s][A
Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:12<00:05,  0.58it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:13<00:03,  0.58it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:15<00:01,  0.58it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:17<00:00,  0.58it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [04:44<00:00,  0.06it/s, pixel_AUPRO=0.953]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [04:44<00:00,  0.06it/s, pixel_AUPRO=0.953]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [05:15<00:00,  0.05it/s, pixel_AUPRO=0.953]INFO:anomalib.callbacks.timer:Training took 316.33 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|â–ˆ         | 1/10 [00:05<00:48,  0.18it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:10<00:42,  0.19it/s]Testing DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:16<00:37,  0.18it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:21<00:32,  0.19it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:26<00:26,  0.19it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:31<00:21,  0.19it/s]Testing DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:37<00:15,  0.19it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:42<00:10,  0.19it/s]Testing DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:47<00:05,  0.19it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 348.0843963623047 seconds
Throughput (batch_size=17) : 0.47976870478898903 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [05:47<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9318057298660278     â”‚
â”‚        pixel_AUPRO        â”‚     0.953453004360199     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad screw with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/19 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/19 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   5%|â–Œ         | 1/19 [00:00<00:12,  1.42it/s]Epoch 0:   5%|â–Œ         | 1/19 [00:00<00:12,  1.42it/s]Epoch 0:  11%|â–ˆ         | 2/19 [00:00<00:08,  2.05it/s]Epoch 0:  11%|â–ˆ         | 2/19 [00:00<00:08,  2.05it/s]Epoch 0:  16%|â–ˆâ–Œ        | 3/19 [00:01<00:07,  2.10it/s]Epoch 0:  16%|â–ˆâ–Œ        | 3/19 [00:01<00:07,  2.10it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 4/19 [00:01<00:07,  2.12it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 4/19 [00:01<00:07,  2.12it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 5/19 [00:02<00:06,  2.14it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 5/19 [00:02<00:06,  2.14it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:02<00:06,  2.16it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:02<00:06,  2.16it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [00:03<00:05,  2.16it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [00:03<00:05,  2.16it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [00:03<00:05,  2.16it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [00:03<00:05,  2.16it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:04<00:04,  2.16it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:04<00:04,  2.16it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [00:04<00:04,  2.17it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [00:04<00:04,  2.17it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [00:05<00:03,  2.17it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [00:05<00:03,  2.17it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:05<00:03,  2.17it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:05<00:03,  2.17it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [00:05<00:02,  2.17it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [00:05<00:02,  2.17it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [00:06<00:02,  2.17it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [00:06<00:02,  2.17it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:06<00:01,  2.17it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:06<00:01,  2.17it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [00:07<00:01,  2.17it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [00:07<00:01,  2.17it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [00:07<00:00,  2.17it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [00:07<00:00,  2.17it/s]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:08<00:00,  2.17it/s]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:08<00:00,  2.17it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:08<00:00,  2.19it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:08<00:00,  2.19it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|â–ˆ         | 1/10 [00:02<00:18,  0.48it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:04<00:16,  0.49it/s][A
Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:14,  0.49it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:08<00:12,  0.49it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:10<00:10,  0.49it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:12<00:08,  0.49it/s][A
Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:14<00:06,  0.49it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:16<00:04,  0.49it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:18<00:02,  0.49it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:20<00:00,  0.49it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [03:00<00:00,  0.10it/s, pixel_AUPRO=0.965]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [03:00<00:00,  0.10it/s, pixel_AUPRO=0.965]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [03:38<00:00,  0.09it/s, pixel_AUPRO=0.965]INFO:anomalib.callbacks.timer:Training took 219.38 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|â–ˆ         | 1/10 [00:05<00:48,  0.18it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:10<00:41,  0.19it/s]Testing DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:15<00:36,  0.19it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:20<00:31,  0.19it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:25<00:25,  0.19it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:30<00:20,  0.19it/s]Testing DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:36<00:15,  0.19it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:42<00:10,  0.19it/s]Testing DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:47<00:05,  0.19it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:51<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 224.44984650611877 seconds
Throughput (batch_size=17) : 0.7128541297337809 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:43<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9852428436279297     â”‚
â”‚        pixel_AUPRO        â”‚    0.9651972055435181     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad tile with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/14 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/14 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/14 [00:00<00:10,  1.21it/s]Epoch 0:   7%|â–‹         | 1/14 [00:00<00:10,  1.21it/s]Epoch 0:  14%|â–ˆâ–        | 2/14 [00:01<00:07,  1.64it/s]Epoch 0:  14%|â–ˆâ–        | 2/14 [00:01<00:07,  1.64it/s]Epoch 0:  21%|â–ˆâ–ˆâ–       | 3/14 [00:01<00:06,  1.67it/s]Epoch 0:  21%|â–ˆâ–ˆâ–       | 3/14 [00:01<00:06,  1.67it/s]Epoch 0:  29%|â–ˆâ–ˆâ–Š       | 4/14 [00:02<00:05,  1.68it/s]Epoch 0:  29%|â–ˆâ–ˆâ–Š       | 4/14 [00:02<00:05,  1.68it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 5/14 [00:02<00:05,  1.69it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 5/14 [00:02<00:05,  1.69it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6/14 [00:03<00:04,  1.69it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6/14 [00:03<00:04,  1.69it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 7/14 [00:04<00:04,  1.70it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 7/14 [00:04<00:04,  1.70it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 8/14 [00:04<00:03,  1.70it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 8/14 [00:04<00:03,  1.70it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 9/14 [00:05<00:02,  1.71it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 9/14 [00:05<00:02,  1.71it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 10/14 [00:05<00:02,  1.70it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 10/14 [00:05<00:02,  1.70it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 11/14 [00:06<00:01,  1.70it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 11/14 [00:06<00:01,  1.70it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12/14 [00:07<00:01,  1.71it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12/14 [00:07<00:01,  1.71it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 13/14 [00:07<00:00,  1.71it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 13/14 [00:07<00:00,  1.71it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.76it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:07<00:00,  1.76it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:09,  0.62it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:03<00:07,  0.64it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:04<00:06,  0.64it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:06<00:04,  0.64it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:07<00:03,  0.65it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:09<00:01,  0.65it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:10<00:00,  0.65it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:11<00:00,  0.20it/s, pixel_AUPRO=0.819]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:11<00:00,  0.20it/s, pixel_AUPRO=0.819]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [01:38<00:00,  0.14it/s, pixel_AUPRO=0.819]INFO:anomalib.callbacks.timer:Training took 98.78 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:05<00:33,  0.18it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:10<00:26,  0.19it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:16<00:21,  0.18it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:21<00:15,  0.19it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:26<00:10,  0.19it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:31<00:05,  0.19it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:36<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 117.77914071083069 seconds
Throughput (batch_size=17) : 0.9933847308943812 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [01:57<00:00,  0.06it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.994949460029602     â”‚
â”‚        pixel_AUPRO        â”‚    0.8204277753829956     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad toothbrush with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/4 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:03,  0.97it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:03,  0.97it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:01<00:01,  1.21it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:01<00:01,  1.21it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:02<00:00,  1.23it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:02<00:00,  1.23it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.40it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.40it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/3 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:02,  0.96it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:02<00:01,  1.00it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.02it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  0.15it/s, pixel_AUPRO=0.914]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:26<00:00,  0.15it/s, pixel_AUPRO=0.914]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:34<00:00,  0.12it/s, pixel_AUPRO=0.914]INFO:anomalib.callbacks.timer:Training took 34.73 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/3 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:09,  0.21it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:09<00:04,  0.22it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:11<00:00,  0.26it/s]INFO:anomalib.callbacks.timer:Testing took 28.568954467773438 seconds
Throughput (batch_size=17) : 1.4701273036567422 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:27<00:00,  0.11it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9416666030883789     â”‚
â”‚        pixel_AUPRO        â”‚    0.9149801731109619     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad transistor with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/13 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/13 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   8%|â–Š         | 1/13 [00:01<00:12,  0.93it/s]Epoch 0:   8%|â–Š         | 1/13 [00:01<00:12,  0.93it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:01<00:09,  1.15it/s]Epoch 0:  15%|â–ˆâ–Œ        | 2/13 [00:01<00:09,  1.15it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:02<00:08,  1.16it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:02<00:08,  1.16it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:03<00:07,  1.15it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:03<00:07,  1.15it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:04<00:06,  1.14it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:04<00:06,  1.14it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:05<00:06,  1.15it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:05<00:06,  1.15it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:06<00:05,  1.16it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:06<00:05,  1.16it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:06<00:04,  1.16it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:06<00:04,  1.16it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:07<00:03,  1.16it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:07<00:03,  1.16it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:08<00:02,  1.16it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:08<00:02,  1.16it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:09<00:01,  1.16it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:09<00:01,  1.16it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:10<00:00,  1.17it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:10<00:00,  1.17it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.21it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.21it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/6 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 1/6 [00:01<00:07,  0.66it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:02<00:05,  0.68it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:04<00:04,  0.68it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:05<00:02,  0.69it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:07<00:01,  0.69it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  0.69it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:48<00:00,  0.27it/s, pixel_AUPRO=0.950]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:48<00:00,  0.27it/s, pixel_AUPRO=0.950]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [01:13<00:00,  0.18it/s, pixel_AUPRO=0.950]INFO:anomalib.callbacks.timer:Training took 74.61 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/6 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 1/6 [00:05<00:27,  0.18it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:10<00:21,  0.19it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:15<00:15,  0.19it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:20<00:10,  0.19it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:25<00:05,  0.19it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  0.20it/s]INFO:anomalib.callbacks.timer:Testing took 65.1630551815033 seconds
Throughput (batch_size=17) : 1.5346119011986605 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:04<00:00,  0.09it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9941667318344116     â”‚
â”‚        pixel_AUPRO        â”‚    0.9500223398208618     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad wood with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.94it/s]Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.94it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.19it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.19it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:10,  1.20it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:10,  1.20it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:09,  1.19it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:09,  1.19it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:08,  1.19it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:08,  1.19it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:05<00:07,  1.19it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:05<00:07,  1.19it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.19it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.19it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.19it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.19it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:05,  1.20it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:05,  1.20it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:08<00:04,  1.20it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:08<00:04,  1.20it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:09<00:03,  1.20it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:09<00:03,  1.20it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:10<00:02,  1.20it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:10<00:02,  1.20it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.20it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.20it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.20it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.20it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:12<00:00,  1.24it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:12<00:00,  1.24it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:06,  0.59it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:03<00:05,  0.60it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:04<00:03,  0.60it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:06<00:01,  0.61it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:08<00:00,  0.61it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:31<00:00,  0.16it/s, pixel_AUPRO=0.876]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:31<00:00,  0.16it/s, pixel_AUPRO=0.876]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:52<00:00,  0.13it/s, pixel_AUPRO=0.876]INFO:anomalib.callbacks.timer:Training took 113.41 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:05<00:21,  0.18it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:10<00:15,  0.19it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:15<00:10,  0.19it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:21<00:05,  0.19it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:25<00:00,  0.20it/s]INFO:anomalib.callbacks.timer:Testing took 126.48213052749634 seconds
Throughput (batch_size=17) : 0.6245941594320784 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:05<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9868420362472534     â”‚
â”‚        pixel_AUPRO        â”‚     0.878081202507019     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.mvtec:Found the dataset.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset mvtec_ad zipper with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:00<00:09,  1.45it/s]Epoch 0:   7%|â–‹         | 1/15 [00:00<00:09,  1.45it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:00<00:06,  2.11it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:00<00:06,  2.11it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:01<00:05,  2.16it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:01<00:05,  2.16it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:01<00:05,  2.18it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:01<00:05,  2.18it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:02<00:04,  2.19it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:02<00:04,  2.19it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:02<00:04,  2.20it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:02<00:04,  2.20it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:03<00:03,  2.20it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:03<00:03,  2.20it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:03<00:03,  2.22it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:03<00:03,  2.22it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:04<00:02,  2.22it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:04<00:02,  2.22it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:04<00:02,  2.22it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:04<00:02,  2.22it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:04<00:01,  2.22it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:04<00:01,  2.22it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:05<00:01,  2.23it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:05<00:01,  2.23it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:05<00:00,  2.23it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:05<00:00,  2.23it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:06<00:00,  2.23it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:06<00:00,  2.23it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:06<00:00,  2.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:06<00:00,  2.36it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:01<00:13,  0.60it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:03<00:11,  0.61it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:04<00:09,  0.62it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:06<00:08,  0.62it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:08<00:06,  0.62it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:09<00:04,  0.62it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:11<00:03,  0.63it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:12<00:01,  0.63it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:14<00:00,  0.63it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:22<00:00,  0.07it/s, pixel_AUPRO=0.942]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:22<00:00,  0.07it/s, pixel_AUPRO=0.942]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:51<00:00,  0.06it/s, pixel_AUPRO=0.942]INFO:anomalib.callbacks.timer:Training took 232.27 seconds
INFO:src.anomalib.data.image.mvtec:Found the dataset.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:05<00:40,  0.20it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:10<00:35,  0.20it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:15<00:31,  0.19it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:20<00:25,  0.20it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:25<00:20,  0.20it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:30<00:15,  0.20it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:35<00:10,  0.20it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:40<00:05,  0.20it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:45<00:00,  0.20it/s]INFO:anomalib.callbacks.timer:Testing took 251.347918510437 seconds
Throughput (batch_size=17) : 0.6007608930874431 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:10<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9522058963775635     â”‚
â”‚        pixel_AUPRO        â”‚    0.9418679475784302     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa candle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.55it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.55it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.35it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.35it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.40it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.40it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.43it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.43it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.45it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.45it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.46it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.46it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.47it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.47it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.47it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.47it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.48it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.48it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.49it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.49it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.49it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.49it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.50it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.50it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:15,  2.50it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:15,  2.50it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.50it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.50it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:05<00:15,  2.50it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:05<00:15,  2.50it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.50it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.50it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.50it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.50it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.50it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.50it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.50it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.50it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.50it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.50it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.50it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.50it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.50it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.50it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.50it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.50it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.50it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.50it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:11,  2.50it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:11,  2.50it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.50it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.50it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.51it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.51it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.51it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.51it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.51it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.51it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.50it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.50it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.51it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.51it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.51it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.51it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.51it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.51it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.51it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.51it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.51it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.51it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.51it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.51it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.51it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.51it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:05,  2.52it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:05,  2.52it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.52it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.52it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.52it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.52it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.52it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.52it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.52it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.52it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.52it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.52it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.52it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.52it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.52it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.52it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.52it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.52it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.52it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.52it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.52it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.52it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.52it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.52it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.52it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.52it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.52it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.52it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.52it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.52it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.52it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.52it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:02<00:24,  0.45it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:04<00:21,  0.46it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:06<00:19,  0.46it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:08<00:17,  0.47it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:10<00:14,  0.47it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:12<00:12,  0.47it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:14<00:10,  0.47it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:17<00:08,  0.47it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:19<00:06,  0.47it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:21<00:04,  0.47it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:23<00:02,  0.47it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:25<00:00,  0.47it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [07:27<00:00,  0.12it/s, pixel_AUPRO=0.952]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [07:27<00:00,  0.12it/s, pixel_AUPRO=0.952]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [08:05<00:00,  0.11it/s, pixel_AUPRO=0.952]INFO:anomalib.callbacks.timer:Training took 486.87 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:05<00:59,  0.19it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:10<00:51,  0.19it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:15<00:46,  0.19it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:20<00:40,  0.20it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:25<00:35,  0.20it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:30<00:30,  0.20it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:36<00:25,  0.19it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:40<00:20,  0.20it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:46<00:15,  0.20it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:50<00:10,  0.20it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:56<00:05,  0.20it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:00<00:00,  0.20it/s]INFO:anomalib.callbacks.timer:Testing took 502.365971326828 seconds
Throughput (batch_size=17) : 0.3981161372689483 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [08:21<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9417000412940979     â”‚
â”‚        pixel_AUPRO        â”‚    0.9520546197891235     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa capsules with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/32 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/32 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:20,  1.50it/s]Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:20,  1.50it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.22it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.22it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.27it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.27it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:12,  2.30it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:12,  2.30it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:11,  2.32it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:11,  2.32it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:11,  2.33it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:11,  2.33it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:02<00:10,  2.33it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:02<00:10,  2.33it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:10,  2.34it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:10,  2.34it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:03<00:09,  2.35it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:03<00:09,  2.35it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.35it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.35it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:08,  2.36it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:08,  2.36it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:05<00:08,  2.36it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:05<00:08,  2.36it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:08,  2.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:08,  2.37it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:05<00:07,  2.37it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:05<00:07,  2.37it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:07,  2.38it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:07,  2.37it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:06<00:06,  2.38it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:06<00:06,  2.38it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:07<00:06,  2.37it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:07<00:06,  2.37it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:07<00:05,  2.37it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:07<00:05,  2.37it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:07<00:05,  2.38it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:07<00:05,  2.38it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:05,  2.38it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:05,  2.38it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:08<00:04,  2.38it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:08<00:04,  2.38it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.39it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.39it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:09<00:03,  2.39it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:09<00:03,  2.39it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:10<00:03,  2.39it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:10<00:03,  2.39it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:10<00:02,  2.39it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:10<00:02,  2.39it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:10<00:02,  2.39it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:10<00:02,  2.39it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.39it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.39it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:11<00:01,  2.39it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:11<00:01,  2.39it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:12<00:01,  2.39it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:12<00:01,  2.39it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:12<00:00,  2.39it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:12<00:00,  2.39it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:12<00:00,  2.39it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:12<00:00,  2.39it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.40it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.40it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|â–ˆ         | 1/10 [00:01<00:14,  0.62it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:03<00:12,  0.64it/s][A
Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:04<00:10,  0.64it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:06<00:09,  0.65it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:07<00:07,  0.65it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:09<00:06,  0.65it/s][A
Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:10<00:04,  0.65it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:12<00:03,  0.65it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:13<00:01,  0.65it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  0.65it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:39<00:00,  0.32it/s, pixel_AUPRO=0.732]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:39<00:00,  0.32it/s, pixel_AUPRO=0.732]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [02:07<00:00,  0.25it/s, pixel_AUPRO=0.732]INFO:anomalib.callbacks.timer:Training took 128.41 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|â–ˆ         | 1/10 [00:04<00:40,  0.22it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:08<00:35,  0.23it/s]Testing DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:13<00:32,  0.22it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:17<00:26,  0.22it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:22<00:22,  0.22it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:26<00:17,  0.22it/s]Testing DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:30<00:13,  0.23it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:35<00:08,  0.23it/s]Testing DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:39<00:04,  0.23it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:42<00:00,  0.23it/s]INFO:anomalib.callbacks.timer:Testing took 146.71587300300598 seconds
Throughput (batch_size=17) : 1.0905432161162403 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:25<00:00,  0.07it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.6949999928474426     â”‚
â”‚        pixel_AUPRO        â”‚    0.7342936992645264     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa cashew with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.49it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.49it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.16it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.16it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.21it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.21it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.25it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.25it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.26it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.26it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.27it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.27it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.29it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.29it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.29it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.29it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.29it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.29it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.30it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.30it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.30it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.30it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.30it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.30it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.30it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.30it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.30it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.30it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.30it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.30it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.30it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.30it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.30it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.30it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.31it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.31it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.31it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.31it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.31it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.31it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.31it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.31it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.31it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.31it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.32it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.32it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.32it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.32it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.32it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.32it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.36it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:02<00:17,  0.45it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:04<00:15,  0.45it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:06<00:13,  0.46it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:08<00:10,  0.46it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:10<00:08,  0.46it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:13<00:06,  0.46it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:15<00:04,  0.46it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:17<00:02,  0.46it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:19<00:00,  0.46it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:39<00:00,  0.17it/s, pixel_AUPRO=0.890]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:39<00:00,  0.17it/s, pixel_AUPRO=0.890]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:18<00:00,  0.14it/s, pixel_AUPRO=0.890]INFO:anomalib.callbacks.timer:Training took 199.19 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:05<00:42,  0.19it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:10<00:36,  0.19it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:16<00:32,  0.19it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:21<00:26,  0.19it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:26<00:21,  0.19it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:31<00:15,  0.19it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:36<00:10,  0.19it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:42<00:05,  0.19it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:47<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 209.1178023815155 seconds
Throughput (batch_size=17) : 0.7172990452832863 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [03:28<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9763999581336975     â”‚
â”‚        pixel_AUPRO        â”‚    0.8899759650230408     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa chewinggum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.57it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.57it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.33it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.33it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.37it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.37it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.39it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.39it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.42it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.42it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.44it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.44it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.45it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.45it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.45it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.45it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.46it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.46it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:06,  2.47it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:06,  2.46it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.47it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.47it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:06,  2.48it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:06,  2.48it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.48it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.48it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.48it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.48it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:04,  2.48it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:04,  2.48it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.48it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.48it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:04,  2.48it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:04,  2.48it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.48it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.48it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.48it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.48it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.49it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.49it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.49it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.49it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:02,  2.49it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:02,  2.49it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.49it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.49it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.49it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.49it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.49it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.49it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.49it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.49it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.53it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.53it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:02<00:16,  0.49it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:03<00:13,  0.50it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:05<00:11,  0.50it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:07<00:09,  0.50it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:09<00:07,  0.51it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:11<00:05,  0.51it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:13<00:03,  0.51it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:15<00:01,  0.51it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:17<00:00,  0.51it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:23<00:00,  0.08it/s, pixel_AUPRO=0.788]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:23<00:00,  0.08it/s, pixel_AUPRO=0.788]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:58<00:00,  0.08it/s, pixel_AUPRO=0.788]INFO:anomalib.callbacks.timer:Training took 359.69 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:05<00:40,  0.20it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:09<00:33,  0.21it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:14<00:29,  0.21it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:19<00:23,  0.21it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:23<00:18,  0.21it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:28<00:14,  0.21it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:33<00:09,  0.21it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:37<00:04,  0.21it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:41<00:00,  0.22it/s]INFO:anomalib.callbacks.timer:Testing took 428.4150040149689 seconds
Throughput (batch_size=17) : 0.35012779336448957 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [07:07<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9953999519348145     â”‚
â”‚        pixel_AUPRO        â”‚    0.7876123189926147     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.51it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.51it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:12,  2.07it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:12,  2.07it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:11,  2.07it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:11,  2.07it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:11,  2.08it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:11,  2.08it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:10,  2.07it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:10,  2.07it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:10,  2.09it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:10,  2.09it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:09,  2.09it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:09,  2.09it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:09,  2.10it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:09,  2.10it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:04<00:08,  2.13it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:04<00:08,  2.13it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.14it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.14it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:05<00:07,  2.16it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:05<00:07,  2.16it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.18it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.18it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.19it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.19it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.20it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.20it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.21it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.21it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:04,  2.21it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:04,  2.21it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.22it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.22it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:08<00:04,  2.24it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:08<00:04,  2.24it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.25it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.24it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.25it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.25it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.26it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.26it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.27it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.27it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.28it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.28it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.29it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.29it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.29it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.29it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.30it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.30it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.34it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:01<00:12,  0.62it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:03<00:10,  0.64it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:04<00:09,  0.65it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:06<00:07,  0.65it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:07<00:06,  0.65it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:09<00:04,  0.65it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:10<00:03,  0.65it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:12<00:01,  0.65it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:13<00:00,  0.65it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:58<00:00,  0.11it/s, pixel_AUPRO=0.846]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:58<00:00,  0.11it/s, pixel_AUPRO=0.846]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:26<00:00,  0.10it/s, pixel_AUPRO=0.846]INFO:anomalib.callbacks.timer:Training took 266.99 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:04<00:36,  0.22it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:08<00:31,  0.22it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:13<00:27,  0.22it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:17<00:22,  0.22it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:22<00:17,  0.22it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:26<00:13,  0.22it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:31<00:08,  0.22it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:35<00:04,  0.22it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:39<00:00,  0.23it/s]INFO:anomalib.callbacks.timer:Testing took 279.29353070259094 seconds
Throughput (batch_size=17) : 0.5370693679250641 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:38<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9244000315666199     â”‚
â”‚        pixel_AUPRO        â”‚    0.8459334373474121     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:32,  1.59it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:32,  1.59it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.38it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.38it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.45it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.45it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:19,  2.48it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:19,  2.48it/s]Epoch 0:   9%|â–‰         | 5/53 [00:01<00:19,  2.51it/s]Epoch 0:   9%|â–‰         | 5/53 [00:01<00:19,  2.51it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:18,  2.52it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:18,  2.52it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.53it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.53it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:17,  2.50it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:17,  2.50it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.45it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.45it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.46it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.46it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.47it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.47it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.48it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.48it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.48it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.48it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.49it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.49it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.49it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.49it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.49it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.49it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.50it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.50it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.50it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.50it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.50it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.50it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:07<00:13,  2.50it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:07<00:13,  2.50it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.51it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.51it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.51it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.51it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.51it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.51it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.52it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.52it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:11,  2.52it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:11,  2.52it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.52it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.52it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.52it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.52it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.52it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.52it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.53it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.53it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.53it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.53it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.53it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.53it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.53it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.53it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.53it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.53it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.53it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.53it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.54it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.54it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.54it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.54it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.54it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.54it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:14<00:05,  2.54it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:14<00:05,  2.54it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.55it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.55it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.55it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.55it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.55it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.55it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.55it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.55it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:16<00:03,  2.55it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:16<00:03,  2.55it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.55it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.55it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.55it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.55it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.55it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.55it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.55it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.55it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:18<00:01,  2.55it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:18<00:01,  2.55it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.55it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.55it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.56it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.56it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:19<00:00,  2.56it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:19<00:00,  2.56it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.56it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.56it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:20<00:00,  2.56it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:20<00:00,  2.56it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:01<00:17,  0.62it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:03<00:15,  0.64it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:04<00:13,  0.65it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:06<00:12,  0.65it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:07<00:10,  0.65it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:09<00:09,  0.65it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:10<00:07,  0.65it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:12<00:06,  0.65it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:13<00:04,  0.65it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:15<00:03,  0.65it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:16<00:01,  0.65it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:18<00:00,  0.65it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:45<00:00,  0.15it/s, pixel_AUPRO=0.924]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:45<00:00,  0.15it/s, pixel_AUPRO=0.924]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [06:13<00:00,  0.14it/s, pixel_AUPRO=0.924]INFO:anomalib.callbacks.timer:Training took 373.89 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:49,  0.22it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:08<00:44,  0.23it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:13<00:39,  0.23it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:17<00:34,  0.23it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:21<00:29,  0.23it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:25<00:25,  0.24it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:29<00:21,  0.23it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:34<00:17,  0.23it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:38<00:12,  0.23it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:42<00:08,  0.23it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:47<00:04,  0.23it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:50<00:00,  0.24it/s]INFO:anomalib.callbacks.timer:Testing took 354.3655831813812 seconds
Throughput (batch_size=17) : 0.5643888952320476 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [05:53<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9177000522613525     â”‚
â”‚        pixel_AUPRO        â”‚    0.9244478940963745     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.52it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.51it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.31it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.31it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.38it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.38it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.42it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.42it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.45it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.44it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.46it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.46it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.47it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.47it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.48it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.48it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.49it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.49it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.49it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.49it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.50it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.50it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.50it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.50it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:15,  2.50it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:15,  2.50it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.51it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.51it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:05<00:15,  2.51it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:05<00:15,  2.51it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.51it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.51it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.51it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.51it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:13,  2.51it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:13,  2.51it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.51it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.51it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:07<00:13,  2.52it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:07<00:13,  2.52it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.52it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.52it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.53it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.53it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.53it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.53it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.53it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.53it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:11,  2.54it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:11,  2.54it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.54it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.54it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.54it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.54it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.54it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.54it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.55it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.55it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.55it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.55it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.55it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.55it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.55it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.55it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:12<00:07,  2.55it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:12<00:07,  2.55it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.55it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.55it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.55it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.55it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.55it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.55it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.55it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.55it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:14<00:05,  2.55it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:14<00:05,  2.55it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.55it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.55it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.56it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.56it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.56it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.56it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.56it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.56it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:16<00:03,  2.56it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:16<00:03,  2.56it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.56it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.56it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.56it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.56it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:17<00:02,  2.56it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:17<00:02,  2.56it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.56it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.56it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:18<00:01,  2.56it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:18<00:01,  2.56it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.56it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.56it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.56it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.56it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:19<00:00,  2.56it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:19<00:00,  2.56it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.56it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.56it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:20<00:00,  2.57it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:20<00:00,  2.57it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:01<00:17,  0.62it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:03<00:15,  0.64it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:04<00:13,  0.64it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:06<00:12,  0.65it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:07<00:10,  0.65it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:09<00:09,  0.65it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:10<00:07,  0.65it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:12<00:06,  0.65it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:13<00:04,  0.65it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:15<00:03,  0.65it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:16<00:01,  0.65it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:18<00:00,  0.65it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:47<00:00,  0.18it/s, pixel_AUPRO=0.846]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:47<00:00,  0.18it/s, pixel_AUPRO=0.846]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:16<00:00,  0.17it/s, pixel_AUPRO=0.846]INFO:anomalib.callbacks.timer:Training took 317.22 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:50,  0.22it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:08<00:44,  0.23it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:13<00:40,  0.22it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:17<00:35,  0.23it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:21<00:30,  0.23it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:25<00:25,  0.23it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:30<00:21,  0.23it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:34<00:17,  0.23it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:38<00:12,  0.23it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:43<00:08,  0.23it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:47<00:04,  0.23it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:51<00:00,  0.23it/s]INFO:anomalib.callbacks.timer:Testing took 219.54666209220886 seconds
Throughput (batch_size=17) : 0.9109680743677199 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [03:38<00:00,  0.05it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7656000256538391     â”‚
â”‚        pixel_AUPRO        â”‚    0.8460745811462402     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:35,  1.51it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:35,  1.51it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.21it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.21it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.29it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.29it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.34it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.34it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.36it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.36it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.31it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.31it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:20,  2.25it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:20,  2.25it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:20,  2.21it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:20,  2.21it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:04<00:20,  2.23it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:04<00:20,  2.23it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:19,  2.26it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:19,  2.25it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.27it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.27it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.29it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.29it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.30it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.30it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:06<00:17,  2.31it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:06<00:17,  2.31it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.31it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.31it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.32it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.32it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.33it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.33it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.33it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.33it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:14,  2.34it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:14,  2.34it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.34it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.34it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:14,  2.35it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:14,  2.35it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.36it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.36it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.36it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.36it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.36it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.36it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.37it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.37it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.37it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.37it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.38it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.38it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.38it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.38it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.38it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.38it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.38it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.38it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.39it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.39it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.39it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.39it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.39it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.39it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.40it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.40it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.40it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.40it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.40it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.40it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.40it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.40it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.40it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.40it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.41it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.41it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.41it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.41it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.41it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.41it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.41it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.41it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.41it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.41it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.42it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.42it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.42it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.42it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.42it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.42it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.42it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.42it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.42it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.42it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.42it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.42it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.42it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.42it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.42it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.42it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.43it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.42it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.43it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.43it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.46it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.46it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:01<00:20,  0.54it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:03<00:18,  0.55it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:05<00:16,  0.56it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:07<00:14,  0.56it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:08<00:12,  0.56it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:10<00:10,  0.56it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:12<00:08,  0.56it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:14<00:07,  0.56it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:16<00:05,  0.56it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:17<00:03,  0.56it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:19<00:01,  0.56it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:21<00:00,  0.56it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:42<00:00,  0.19it/s, pixel_AUPRO=0.886]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:42<00:00,  0.19it/s, pixel_AUPRO=0.886]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [05:16<00:00,  0.17it/s, pixel_AUPRO=0.886]INFO:anomalib.callbacks.timer:Training took 317.16 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:53,  0.21it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:09<00:47,  0.21it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:14<00:42,  0.21it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:18<00:37,  0.21it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:23<00:32,  0.22it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:27<00:27,  0.22it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:32<00:23,  0.21it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:37<00:18,  0.22it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:41<00:13,  0.22it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:46<00:09,  0.22it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:50<00:04,  0.22it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:54<00:00,  0.22it/s]INFO:anomalib.callbacks.timer:Testing took 362.596421957016 seconds
Throughput (batch_size=17) : 0.5515774229667082 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [06:01<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9307000041007996     â”‚
â”‚        pixel_AUPRO        â”‚    0.8857173323631287     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.52it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.52it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.26it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.26it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.32it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.32it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.36it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.35it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.38it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.38it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.39it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.39it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.40it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.40it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:19,  2.37it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:19,  2.37it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.35it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.35it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.35it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.35it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.36it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.36it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.36it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.36it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.37it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.37it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.37it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.37it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.37it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.37it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.38it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.38it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.38it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.38it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.38it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.38it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:14,  2.38it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:14,  2.38it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.39it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.39it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.39it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.39it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:12,  2.39it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:12,  2.39it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.40it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.40it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.40it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.40it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.40it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.40it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.40it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.40it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.40it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.40it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.41it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.41it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:09,  2.41it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:09,  2.41it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.41it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.41it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.41it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.41it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.41it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.41it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.41it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.41it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.41it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.41it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.42it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.42it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:07,  2.42it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:07,  2.42it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.42it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.42it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.42it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.42it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.42it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.42it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.43it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.43it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.43it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.43it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.43it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.43it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.43it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.43it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.43it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.43it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.43it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.43it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.43it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.43it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.43it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.43it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.43it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.43it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.43it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.43it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.43it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.43it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.43it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.43it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.43it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.43it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.43it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.43it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:01<00:20,  0.54it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:03<00:18,  0.55it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:05<00:16,  0.56it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:07<00:14,  0.56it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:08<00:12,  0.56it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:10<00:10,  0.56it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:12<00:08,  0.56it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:14<00:07,  0.56it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:16<00:05,  0.56it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:17<00:03,  0.56it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:19<00:01,  0.56it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:21<00:00,  0.56it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:17<00:00,  0.17it/s, pixel_AUPRO=0.894]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:17<00:00,  0.17it/s, pixel_AUPRO=0.894]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:50<00:00,  0.15it/s, pixel_AUPRO=0.894]INFO:anomalib.callbacks.timer:Training took 351.29 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:52,  0.21it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:09<00:46,  0.22it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:14<00:44,  0.20it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:19<00:38,  0.21it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:23<00:33,  0.21it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:28<00:28,  0.21it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:32<00:23,  0.21it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:37<00:18,  0.21it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:41<00:13,  0.22it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:46<00:09,  0.22it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:50<00:04,  0.22it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:54<00:00,  0.22it/s]INFO:anomalib.callbacks.timer:Testing took 401.333242893219 seconds
Throughput (batch_size=17) : 0.49833898273214594 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [06:40<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9567000269889832     â”‚
â”‚        pixel_AUPRO        â”‚    0.8938495516777039     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb3 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:33,  1.58it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:33,  1.58it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:21,  2.43it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:21,  2.43it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:20,  2.48it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:20,  2.48it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:19,  2.52it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:19,  2.52it/s]Epoch 0:   9%|â–‰         | 5/54 [00:01<00:19,  2.54it/s]Epoch 0:   9%|â–‰         | 5/54 [00:01<00:19,  2.54it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:18,  2.55it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:18,  2.55it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:18,  2.56it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:18,  2.56it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:17,  2.57it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:17,  2.57it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:17,  2.57it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:17,  2.57it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:03<00:17,  2.57it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:03<00:17,  2.57it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:16,  2.57it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:16,  2.57it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:16,  2.57it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:16,  2.57it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:15,  2.57it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:15,  2.57it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:15,  2.57it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:15,  2.57it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:05<00:15,  2.58it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:05<00:15,  2.58it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:14,  2.58it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:14,  2.58it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:14,  2.58it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:14,  2.58it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:06<00:13,  2.58it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:06<00:13,  2.58it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:13,  2.58it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:13,  2.58it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:07<00:13,  2.58it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:07<00:13,  2.58it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:12,  2.58it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:12,  2.58it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:08<00:12,  2.59it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:08<00:12,  2.59it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:08<00:11,  2.59it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:08<00:11,  2.59it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:11,  2.59it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:11,  2.59it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:09<00:11,  2.59it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:09<00:11,  2.59it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:10,  2.59it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:10,  2.59it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:10<00:10,  2.59it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:10<00:10,  2.59it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:10<00:10,  2.59it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:10<00:10,  2.59it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:09,  2.59it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:09,  2.59it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:11<00:09,  2.59it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:11<00:09,  2.59it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:11<00:08,  2.59it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:11<00:08,  2.59it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:12<00:08,  2.59it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:12<00:08,  2.59it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:12<00:08,  2.59it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:12<00:08,  2.59it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:07,  2.59it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:07,  2.59it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:13<00:07,  2.60it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:13<00:07,  2.60it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:13<00:06,  2.60it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:13<00:06,  2.60it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:14<00:06,  2.60it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:14<00:06,  2.60it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:14<00:06,  2.60it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:14<00:06,  2.60it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:14<00:05,  2.60it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:14<00:05,  2.60it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:15<00:05,  2.60it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:15<00:05,  2.60it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:15<00:04,  2.60it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:15<00:04,  2.60it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:16<00:04,  2.60it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:16<00:04,  2.60it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:16<00:04,  2.60it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:16<00:04,  2.60it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:16<00:03,  2.60it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:16<00:03,  2.60it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:17<00:03,  2.60it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:17<00:03,  2.60it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:17<00:03,  2.60it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:17<00:03,  2.60it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:18<00:02,  2.60it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:18<00:02,  2.60it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:18<00:02,  2.60it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:18<00:02,  2.60it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:18<00:01,  2.60it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:18<00:01,  2.60it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:19<00:01,  2.60it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:19<00:01,  2.60it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:19<00:01,  2.61it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:19<00:01,  2.61it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:19<00:00,  2.61it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:19<00:00,  2.60it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:20<00:00,  2.61it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:20<00:00,  2.61it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:20<00:00,  2.64it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:20<00:00,  2.64it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:01<00:16,  0.65it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:02<00:14,  0.67it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:04<00:13,  0.68it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:05<00:11,  0.68it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:07<00:10,  0.68it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:08<00:08,  0.68it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:10<00:07,  0.69it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:11<00:05,  0.69it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:13<00:04,  0.69it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:14<00:02,  0.69it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:15<00:01,  0.69it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:17<00:00,  0.69it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:23<00:00,  0.21it/s, pixel_AUPRO=0.878]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:23<00:00,  0.21it/s, pixel_AUPRO=0.878]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:48<00:00,  0.19it/s, pixel_AUPRO=0.878]INFO:anomalib.callbacks.timer:Training took 289.73 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:50,  0.22it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:08<00:43,  0.23it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:13<00:40,  0.22it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:17<00:34,  0.23it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:21<00:29,  0.24it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:25<00:25,  0.24it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:29<00:20,  0.24it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:33<00:16,  0.24it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:37<00:12,  0.24it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:41<00:08,  0.24it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:45<00:04,  0.24it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:48<00:00,  0.25it/s]INFO:anomalib.callbacks.timer:Testing took 314.7199373245239 seconds
Throughput (batch_size=17) : 0.6386630656726986 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [05:13<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.961881160736084     â”‚
â”‚        pixel_AUPRO        â”‚    0.8779980540275574     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb4 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:36,  1.47it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:36,  1.47it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.20it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.20it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.26it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.26it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.30it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.30it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:21,  2.32it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:21,  2.32it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.33it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.33it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:20,  2.34it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:20,  2.34it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:20,  2.30it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:20,  2.30it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:19,  2.30it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:19,  2.30it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:19,  2.31it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:19,  2.31it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.32it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.32it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.33it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.33it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.34it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.34it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:17,  2.34it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:17,  2.34it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.35it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.35it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.35it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.35it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.35it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.35it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.36it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.36it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:14,  2.36it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:14,  2.36it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.36it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.36it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.37it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.37it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.37it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.37it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.38it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.38it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.38it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.38it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.38it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.38it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.38it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.38it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.39it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.39it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.39it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.39it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.39it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.39it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.39it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.39it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.39it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.39it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.39it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.39it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.40it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.40it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.40it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.40it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.40it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.40it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.40it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.40it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.40it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.40it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.40it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.40it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.40it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.40it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.40it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.40it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:05,  2.40it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:05,  2.40it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.40it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.40it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.40it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.40it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.40it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.40it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.40it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.40it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.40it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.40it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.40it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.40it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.41it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.41it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.41it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.41it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.41it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.41it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.41it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.41it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.41it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.41it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.45it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.45it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:01<00:21,  0.50it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:03<00:19,  0.51it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:05<00:17,  0.52it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:07<00:15,  0.52it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:09<00:13,  0.52it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:11<00:11,  0.52it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:13<00:09,  0.52it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:15<00:07,  0.52it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:17<00:05,  0.52it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:19<00:03,  0.52it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:20<00:01,  0.52it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:22<00:00,  0.52it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [08:01<00:00,  0.11it/s, pixel_AUPRO=0.869]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [08:01<00:00,  0.11it/s, pixel_AUPRO=0.869]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [08:36<00:00,  0.10it/s, pixel_AUPRO=0.869]INFO:anomalib.callbacks.timer:Training took 517.33 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:05<00:55,  0.20it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:09<00:48,  0.20it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:15<00:45,  0.20it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:20<00:40,  0.20it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:24<00:34,  0.20it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:29<00:29,  0.20it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:34<00:24,  0.20it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:39<00:19,  0.20it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:44<00:14,  0.20it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:49<00:09,  0.20it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:54<00:04,  0.20it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:58<00:00,  0.21it/s][rank: 0] Received SIGTERM: 15
