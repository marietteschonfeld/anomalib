/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa candle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:35,  1.48it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:35,  1.48it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.28it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.28it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.36it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.36it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.39it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.39it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.42it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.42it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.42it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.42it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.43it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.43it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.43it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.43it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.41it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.41it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.41it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.41it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.41it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.41it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:17,  2.41it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:17,  2.41it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.41it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.41it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.41it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.41it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.41it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.41it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.41it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.41it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:14,  2.41it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:14,  2.41it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.40it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.40it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:14,  2.41it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:14,  2.41it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.41it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.41it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.40it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.40it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:12,  2.41it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:12,  2.41it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.41it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.41it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:12,  2.42it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:12,  2.42it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.42it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.42it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.42it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.42it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.42it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.42it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.42it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.42it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.42it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.42it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.42it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.42it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.43it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.43it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.43it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.43it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.43it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.43it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.43it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.43it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.43it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.43it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:07,  2.43it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:07,  2.43it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.43it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.43it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.43it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.43it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.43it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.43it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.43it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.43it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.43it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.43it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.43it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.43it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.43it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.43it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.43it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.43it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.43it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.43it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.43it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.43it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.44it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.44it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.44it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.44it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.44it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.44it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.44it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.44it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.44it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.44it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.44it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.44it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.44it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.44it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:01<00:11,  0.98it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:01<00:09,  1.02it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:08,  1.04it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:03<00:07,  1.05it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:04<00:06,  1.05it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:05<00:05,  1.06it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:06<00:04,  1.06it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:07<00:03,  1.06it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:08<00:02,  1.07it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:09<00:01,  1.07it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:10<00:00,  1.07it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.07it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [06:25<00:00,  0.14it/s, pixel_AUPRO=0.951]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [06:25<00:00,  0.14it/s, pixel_AUPRO=0.951]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [07:04<00:00,  0.12it/s, pixel_AUPRO=0.951]INFO:anomalib.callbacks.timer:Training took 425.36 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:46,  0.24it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:08<00:40,  0.25it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:11<00:35,  0.25it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:15<00:30,  0.26it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:19<00:26,  0.26it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:22<00:22,  0.26it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:26<00:18,  0.26it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:31<00:15,  0.26it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:35<00:11,  0.26it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:38<00:07,  0.26it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:43<00:03,  0.25it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:46<00:00,  0.26it/s]INFO:anomalib.callbacks.timer:Testing took 493.33464884757996 seconds
Throughput (batch_size=17) : 0.40540432436115337 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [08:12<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8289999961853027     â”‚
â”‚        pixel_AUPRO        â”‚     0.951507031917572     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa capsules with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/32 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/32 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:20,  1.51it/s]Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:20,  1.51it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.29it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.29it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.34it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.33it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:11,  2.35it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:11,  2.35it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:11,  2.37it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:11,  2.37it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:10,  2.39it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:10,  2.39it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:02<00:10,  2.39it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:02<00:10,  2.39it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:09,  2.40it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:09,  2.40it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:03<00:09,  2.41it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:03<00:09,  2.41it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.41it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.41it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:08,  2.41it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:08,  2.41it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:04<00:08,  2.42it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:04<00:08,  2.42it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:07,  2.42it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:07,  2.42it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:05<00:07,  2.42it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:05<00:07,  2.42it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:07,  2.42it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:07,  2.42it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:06<00:06,  2.43it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:06<00:06,  2.43it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:07<00:06,  2.43it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:07<00:06,  2.43it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:07<00:05,  2.42it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:07<00:05,  2.42it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:07<00:05,  2.43it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:07<00:05,  2.43it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:04,  2.43it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:04,  2.43it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:08<00:04,  2.43it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:08<00:04,  2.43it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.43it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.43it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:09<00:03,  2.43it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:09<00:03,  2.43it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:09<00:03,  2.43it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:09<00:03,  2.43it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:10<00:02,  2.43it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:10<00:02,  2.43it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:10<00:02,  2.43it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:10<00:02,  2.43it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.44it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.44it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:11<00:01,  2.44it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:11<00:01,  2.44it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:11<00:01,  2.44it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:11<00:01,  2.44it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:12<00:00,  2.44it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:12<00:00,  2.44it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:12<00:00,  2.44it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:12<00:00,  2.44it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.45it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.45it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|â–ˆ         | 1/10 [00:00<00:06,  1.36it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:01<00:05,  1.45it/s][A
Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:02<00:04,  1.41it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:02<00:04,  1.38it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:03<00:03,  1.36it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:04<00:02,  1.37it/s][A
Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:05<00:02,  1.39it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:05<00:01,  1.41it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:06<00:00,  1.42it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.44it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:03<00:00,  0.50it/s, pixel_AUPRO=nan.0]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:03<00:00,  0.50it/s, pixel_AUPRO=nan.0]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:31<00:00,  0.35it/s, pixel_AUPRO=nan.0]INFO:anomalib.callbacks.timer:Training took 92.57 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|â–ˆ         | 1/10 [00:03<00:32,  0.28it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:07<00:28,  0.28it/s]Testing DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:10<00:25,  0.27it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:14<00:21,  0.28it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:17<00:17,  0.28it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:21<00:14,  0.27it/s]Testing DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:25<00:10,  0.28it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:28<00:07,  0.28it/s]Testing DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:32<00:03,  0.28it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:34<00:00,  0.29it/s]INFO:anomalib.callbacks.timer:Testing took 145.360857963562 seconds
Throughput (batch_size=17) : 1.1007089682981068 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:24<00:00,  0.07it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.6704167127609253     â”‚
â”‚        pixel_AUPRO        â”‚    0.7838695645332336     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa cashew with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:18,  1.41it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:18,  1.41it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:12,  2.04it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:12,  2.04it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:11,  2.10it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:11,  2.10it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.13it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.13it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:10,  2.15it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:10,  2.15it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.16it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.16it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:09,  2.17it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:09,  2.17it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.18it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.18it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:04<00:08,  2.18it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:04<00:08,  2.18it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.19it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.19it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:07,  2.20it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:07,  2.20it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.21it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.21it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.21it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.21it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.22it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.22it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.22it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.22it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:04,  2.22it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:04,  2.22it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.23it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.23it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:08<00:04,  2.23it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:08<00:04,  2.23it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.23it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.23it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.23it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.23it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.24it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.24it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.24it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.24it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.24it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.24it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.24it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.24it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:11<00:00,  2.25it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:11<00:00,  2.25it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.25it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.25it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.29it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.29it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:01<00:08,  0.97it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:01<00:06,  1.01it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:02<00:05,  1.03it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:03<00:04,  1.04it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:04<00:03,  1.05it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:05<00:02,  1.05it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:06<00:01,  1.06it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:07<00:00,  1.06it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:08<00:00,  1.06it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [01:47<00:00,  0.25it/s, pixel_AUPRO=0.827]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [01:47<00:00,  0.25it/s, pixel_AUPRO=0.827]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:27<00:00,  0.18it/s, pixel_AUPRO=0.827]INFO:anomalib.callbacks.timer:Training took 148.51 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:03<00:31,  0.25it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:07<00:26,  0.26it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:11<00:23,  0.25it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:15<00:19,  0.26it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:19<00:15,  0.26it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:23<00:11,  0.26it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:27<00:07,  0.26it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:30<00:03,  0.26it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:33<00:00,  0.27it/s]INFO:anomalib.callbacks.timer:Testing took 196.81220126152039 seconds
Throughput (batch_size=17) : 0.7621478700941046 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [03:15<00:00,  0.05it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8989999890327454     â”‚
â”‚        pixel_AUPRO        â”‚    0.8337320685386658     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa chewinggum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.51it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.51it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.25it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.25it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.33it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.33it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.37it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.37it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.40it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.40it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.38it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.38it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.39it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.39it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.41it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.41it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.42it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.42it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:06,  2.43it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:06,  2.43it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.43it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.43it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:06,  2.44it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:06,  2.44it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.45it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.45it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.45it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.45it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:04,  2.45it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:04,  2.45it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.45it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.45it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:04,  2.45it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:04,  2.45it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.46it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.46it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.46it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.46it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.45it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.45it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.45it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.45it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:02,  2.45it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:02,  2.45it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.46it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.46it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.46it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.46it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.46it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.46it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.46it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.46it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.50it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.50it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:00<00:07,  1.06it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:01<00:06,  1.12it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:02<00:05,  1.14it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:03<00:04,  1.15it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:04<00:03,  1.16it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:05<00:02,  1.16it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:06<00:01,  1.17it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:06<00:00,  1.17it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:07<00:00,  1.17it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:34<00:00,  0.13it/s, pixel_AUPRO=0.666]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:34<00:00,  0.13it/s, pixel_AUPRO=0.666]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:09<00:00,  0.11it/s, pixel_AUPRO=0.666]INFO:anomalib.callbacks.timer:Training took 250.48 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:03<00:31,  0.26it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:07<00:25,  0.27it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:11<00:23,  0.26it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:15<00:19,  0.26it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:19<00:15,  0.26it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:22<00:11,  0.26it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:26<00:07,  0.26it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:29<00:03,  0.27it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:32<00:00,  0.27it/s]INFO:anomalib.callbacks.timer:Testing took 413.143492937088 seconds
Throughput (batch_size=17) : 0.36306998068305885 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [06:52<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9782000184059143     â”‚
â”‚        pixel_AUPRO        â”‚      0.6811483502388      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.52it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.52it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.25it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.25it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.29it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.29it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.31it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.31it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.33it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.33it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.35it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.34it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.36it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.36it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.34it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.34it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.32it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.32it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.31it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.31it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.30it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.28it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.28it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.28it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.28it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.27it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.27it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.27it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.27it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:04,  2.27it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:04,  2.27it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.26it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.26it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.26it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.26it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.27it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.27it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.28it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.28it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.29it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.29it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.29it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.29it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.30it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.30it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.30it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.30it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.30it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.30it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.30it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.30it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.34it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:00<00:06,  1.33it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:01<00:04,  1.42it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:02<00:04,  1.41it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:02<00:03,  1.40it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:03<00:02,  1.38it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:04<00:02,  1.39it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:04<00:01,  1.40it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:05<00:00,  1.42it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:06<00:00,  1.43it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:14<00:00,  0.14it/s, pixel_AUPRO=0.844]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:14<00:00,  0.14it/s, pixel_AUPRO=0.844]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:42<00:00,  0.12it/s, pixel_AUPRO=0.844]INFO:anomalib.callbacks.timer:Training took 222.93 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:03<00:28,  0.28it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:06<00:23,  0.30it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:10<00:20,  0.30it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:13<00:16,  0.30it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:16<00:13,  0.31it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:19<00:09,  0.31it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:22<00:06,  0.31it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:25<00:03,  0.31it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:28<00:00,  0.31it/s]INFO:anomalib.callbacks.timer:Testing took 270.10176515579224 seconds
Throughput (batch_size=17) : 0.555346241123161 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:29<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9322000741958618     â”‚
â”‚        pixel_AUPRO        â”‚    0.8521323204040527     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.49it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.49it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.28it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.28it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.35it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.35it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.39it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.39it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.39it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.39it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.40it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.40it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.40it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.40it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.41it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.41it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.42it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.42it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.42it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.42it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.43it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.43it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.44it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.44it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.44it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.44it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.44it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.44it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.44it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.44it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.44it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.44it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.44it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.44it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.44it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.44it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.45it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.45it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.45it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.45it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.45it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.45it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.45it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.45it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.45it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.45it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.46it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.46it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.46it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.46it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.46it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.46it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.46it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.46it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.46it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.46it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.47it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.47it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.47it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.47it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.47it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.47it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.47it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.47it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.48it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.48it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.48it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.48it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.48it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.48it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.48it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.48it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.49it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.49it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.49it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.49it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.49it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.49it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.49it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.49it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.49it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.49it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.49it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.49it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.50it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.50it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.50it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.50it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.50it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.50it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.50it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.50it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.50it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.50it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.50it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.50it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.50it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.50it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.50it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.50it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.50it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.50it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.51it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.51it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.51it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.51it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:00<00:08,  1.32it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:01<00:07,  1.42it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:06,  1.44it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:02<00:05,  1.43it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:03<00:04,  1.42it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:04<00:04,  1.43it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:04<00:03,  1.44it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:05<00:02,  1.45it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:06<00:02,  1.46it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:06<00:01,  1.47it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:07<00:00,  1.47it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:08<00:00,  1.47it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:40<00:00,  0.19it/s, pixel_AUPRO=0.927]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:40<00:00,  0.19it/s, pixel_AUPRO=0.927]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:08<00:00,  0.17it/s, pixel_AUPRO=0.927]INFO:anomalib.callbacks.timer:Training took 309.27 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:39,  0.28it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:06<00:33,  0.30it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:10<00:30,  0.30it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:13<00:26,  0.31it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:16<00:22,  0.31it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:19<00:19,  0.31it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:22<00:16,  0.31it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:25<00:12,  0.31it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:28<00:09,  0.31it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:31<00:06,  0.31it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:35<00:03,  0.31it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:37<00:00,  0.32it/s]INFO:anomalib.callbacks.timer:Testing took 269.63286662101746 seconds
Throughput (batch_size=17) : 0.7417493368162348 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [04:28<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9271999597549438     â”‚
â”‚        pixel_AUPRO        â”‚     0.92757648229599      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.52it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.52it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.30it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.29it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.35it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.35it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:21,  2.32it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:21,  2.32it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.31it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.31it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:20,  2.32it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:20,  2.32it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:03<00:19,  2.31it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:03<00:19,  2.31it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:19,  2.31it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:19,  2.31it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:19,  2.30it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:19,  2.30it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.31it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.31it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:18,  2.32it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:18,  2.32it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.33it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.33it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:17,  2.34it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:17,  2.34it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.35it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.35it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.35it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.35it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.35it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.35it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.35it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.35it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.36it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.36it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.36it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.36it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.36it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.36it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.36it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.36it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.37it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.37it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.37it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.37it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.38it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.38it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.38it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.38it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.39it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.39it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.39it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.39it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.39it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.39it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:10,  2.39it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:10,  2.39it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.40it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.40it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.40it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.40it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.40it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.40it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.40it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.40it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.40it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.40it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.40it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.40it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:07,  2.41it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:07,  2.41it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.41it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.41it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.41it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.41it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.41it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.41it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.41it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.41it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.41it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.41it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.41it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.41it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.41it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.41it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.41it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.41it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.42it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.42it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.42it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.42it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.42it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.42it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.42it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.42it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.42it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.42it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.42it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.42it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.42it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.42it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.43it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.43it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.43it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.43it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:00<00:08,  1.35it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:01<00:07,  1.41it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:06,  1.38it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:02<00:05,  1.36it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:03<00:05,  1.36it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:04<00:04,  1.36it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:05<00:03,  1.38it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:05<00:02,  1.40it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:06<00:02,  1.41it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:07<00:01,  1.43it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:07<00:00,  1.44it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:08<00:00,  1.44it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [03:36<00:00,  0.24it/s, pixel_AUPRO=0.763]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [03:36<00:00,  0.24it/s, pixel_AUPRO=0.763]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:04<00:00,  0.22it/s, pixel_AUPRO=0.763]INFO:anomalib.callbacks.timer:Training took 245.56 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:38,  0.29it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:06<00:34,  0.29it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:10<00:31,  0.28it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:13<00:27,  0.29it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:17<00:24,  0.29it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:20<00:20,  0.29it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:24<00:17,  0.28it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:28<00:14,  0.28it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:31<00:10,  0.29it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:34<00:06,  0.29it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:38<00:03,  0.29it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:41<00:00,  0.29it/s]INFO:anomalib.callbacks.timer:Testing took 178.43681263923645 seconds
Throughput (batch_size=17) : 1.1208449480901679 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [02:57<00:00,  0.07it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7878000736236572     â”‚
â”‚        pixel_AUPRO        â”‚    0.8204387426376343     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.53it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.53it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.29it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.29it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.35it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.35it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.36it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.36it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:21,  2.28it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:21,  2.28it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:21,  2.24it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:21,  2.24it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:21,  2.20it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:21,  2.20it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:21,  2.17it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:21,  2.17it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:04<00:20,  2.17it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:04<00:20,  2.17it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:20,  2.19it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:20,  2.19it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:19,  2.21it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:19,  2.21it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.23it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.23it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:18,  2.24it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:18,  2.24it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:06<00:17,  2.25it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:06<00:17,  2.25it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:17,  2.26it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:17,  2.26it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:07<00:16,  2.27it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:07<00:16,  2.27it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:16,  2.28it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:16,  2.28it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.28it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.28it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:15,  2.29it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:15,  2.29it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.30it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.30it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:09<00:14,  2.31it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:09<00:14,  2.31it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.31it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.31it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.32it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.32it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.32it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.32it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.32it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.32it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:11<00:12,  2.33it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:11<00:12,  2.33it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.33it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.33it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:11,  2.34it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:11,  2.34it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.34it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.34it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.34it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.34it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.35it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.35it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.35it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.35it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:14<00:08,  2.35it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:14<00:08,  2.35it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.35it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.35it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:08,  2.36it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:08,  2.36it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.36it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.36it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.36it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.36it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:16<00:06,  2.37it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:16<00:06,  2.37it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.37it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.37it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.37it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.37it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.37it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.37it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:05,  2.38it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:05,  2.38it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:18<00:04,  2.38it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:18<00:04,  2.38it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.38it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.38it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.38it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.38it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.38it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.38it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.39it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.39it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:20<00:02,  2.39it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:20<00:02,  2.39it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.39it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.39it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.39it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.39it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.39it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.39it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.40it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.40it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:22<00:00,  2.40it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:22<00:00,  2.40it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.43it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.43it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:00<00:09,  1.19it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:01<00:07,  1.26it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:07,  1.29it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:03<00:06,  1.30it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:03<00:05,  1.31it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:04<00:04,  1.30it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:05<00:03,  1.31it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:06<00:03,  1.31it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:06<00:02,  1.32it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:07<00:01,  1.32it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:08<00:00,  1.32it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:09<00:00,  1.32it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:16<00:00,  0.27it/s, pixel_AUPRO=0.835]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:16<00:00,  0.27it/s, pixel_AUPRO=0.835]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:48<00:00,  0.24it/s, pixel_AUPRO=0.835]INFO:anomalib.callbacks.timer:Training took 229.79 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:41,  0.27it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:07<00:35,  0.29it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:10<00:31,  0.28it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:13<00:27,  0.29it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:17<00:24,  0.29it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:20<00:20,  0.29it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:24<00:17,  0.29it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:27<00:13,  0.29it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:30<00:10,  0.29it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:33<00:06,  0.29it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:37<00:03,  0.29it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:40<00:00,  0.30it/s]INFO:anomalib.callbacks.timer:Testing took 348.8971371650696 seconds
Throughput (batch_size=17) : 0.5732348554794141 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [05:48<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9323999881744385     â”‚
â”‚        pixel_AUPRO        â”‚    0.8948943614959717     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.50it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.50it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:23,  2.18it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:23,  2.18it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:22,  2.24it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:22,  2.24it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:21,  2.27it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:21,  2.27it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.29it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.29it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:20,  2.29it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:20,  2.29it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:03<00:20,  2.24it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:03<00:20,  2.24it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:20,  2.21it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:20,  2.21it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:04<00:19,  2.23it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:04<00:19,  2.23it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:19,  2.24it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:19,  2.24it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:18,  2.25it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:18,  2.25it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:18,  2.26it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:18,  2.26it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:17,  2.27it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:17,  2.27it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:06<00:17,  2.28it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:06<00:17,  2.28it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.28it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.28it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:07<00:16,  2.28it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:07<00:16,  2.28it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.29it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.29it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:15,  2.29it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:15,  2.29it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.30it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.30it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:14,  2.30it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:14,  2.30it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:09<00:13,  2.31it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:09<00:13,  2.31it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.31it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.31it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.32it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.32it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.32it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.32it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:12,  2.32it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:12,  2.32it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:11<00:11,  2.32it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:11<00:11,  2.32it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:11,  2.33it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:11,  2.33it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:12<00:10,  2.33it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:12<00:10,  2.33it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:10,  2.33it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:10,  2.33it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.33it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.33it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:13<00:09,  2.34it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:13<00:09,  2.34it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.34it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.34it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:14<00:08,  2.34it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:14<00:08,  2.34it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:08,  2.34it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:08,  2.34it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.34it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.34it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:15<00:07,  2.34it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:15<00:07,  2.34it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.35it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.35it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:16<00:06,  2.35it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:16<00:06,  2.35it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.35it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.35it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:17<00:05,  2.35it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:17<00:05,  2.35it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:17<00:05,  2.35it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:17<00:05,  2.35it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.35it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.35it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:18<00:04,  2.36it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:18<00:04,  2.36it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.36it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.36it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:19<00:03,  2.36it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:19<00:03,  2.36it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.36it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.36it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.36it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.36it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:20<00:02,  2.36it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:20<00:02,  2.36it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.36it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.36it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:21<00:01,  2.36it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:21<00:01,  2.36it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.37it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.37it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.37it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.37it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:22<00:00,  2.37it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:22<00:00,  2.37it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:00<00:09,  1.18it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:01<00:08,  1.25it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:07,  1.28it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:03<00:06,  1.29it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:03<00:05,  1.30it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:04<00:04,  1.30it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:05<00:03,  1.30it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:06<00:03,  1.30it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:06<00:02,  1.30it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:07<00:01,  1.31it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:08<00:00,  1.31it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:09<00:00,  1.31it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [03:43<00:00,  0.24it/s, pixel_AUPRO=0.887]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [03:43<00:00,  0.24it/s, pixel_AUPRO=0.887]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:15<00:00,  0.21it/s, pixel_AUPRO=0.887]INFO:anomalib.callbacks.timer:Training took 256.11 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:40,  0.27it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:07<00:35,  0.28it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:11<00:33,  0.27it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:14<00:29,  0.27it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:18<00:25,  0.28it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:21<00:21,  0.28it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:25<00:17,  0.28it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:28<00:14,  0.28it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:31<00:10,  0.28it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:35<00:07,  0.28it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:38<00:03,  0.28it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:41<00:00,  0.29it/s]INFO:anomalib.callbacks.timer:Testing took 390.1740679740906 seconds
Throughput (batch_size=17) : 0.5125917287083286 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [06:29<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.914400041103363     â”‚
â”‚        pixel_AUPRO        â”‚    0.9040173888206482     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb3 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.56it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.56it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.35it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.35it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.39it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.39it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:20,  2.41it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:20,  2.41it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.42it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.42it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.37it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.37it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.37it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.37it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.37it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.37it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:19,  2.37it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:19,  2.37it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.37it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.37it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.37it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.36it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:17,  2.36it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:17,  2.36it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.36it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.36it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.36it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.36it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.36it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.36it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.37it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.37it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.37it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.37it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.36it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.36it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:14,  2.36it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:14,  2.36it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.36it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.36it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.37it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.38it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.38it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.38it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.38it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.38it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.38it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.39it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.39it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.39it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.39it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.39it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.39it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.40it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.40it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.40it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.40it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.40it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.40it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.41it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.41it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.41it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.41it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.41it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.41it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.42it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.42it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.42it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.42it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.42it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.42it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.42it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.42it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.43it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.43it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.43it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.43it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.43it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.43it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.43it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.43it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.43it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.43it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.43it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.43it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.44it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.44it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.44it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.44it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.44it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.44it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.44it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.44it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.44it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.44it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.44it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.44it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.44it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.44it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.45it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.45it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.45it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.45it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.45it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.45it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.48it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.48it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:00<00:07,  1.39it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:01<00:06,  1.50it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:06,  1.46it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:02<00:05,  1.45it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:03<00:04,  1.43it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:04<00:04,  1.45it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:04<00:03,  1.47it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:05<00:02,  1.49it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:05<00:01,  1.51it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:06<00:01,  1.52it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:07<00:00,  1.53it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:07<00:00,  1.54it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:32<00:00,  0.25it/s, pixel_AUPRO=0.858]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:32<00:00,  0.25it/s, pixel_AUPRO=0.858]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:59<00:00,  0.23it/s, pixel_AUPRO=0.858]INFO:anomalib.callbacks.timer:Training took 239.96 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:39,  0.28it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:06<00:33,  0.30it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:10<00:30,  0.30it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:13<00:26,  0.30it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:16<00:23,  0.30it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:20<00:20,  0.30it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:24<00:17,  0.29it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:27<00:13,  0.29it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:30<00:10,  0.29it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:33<00:06,  0.30it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:37<00:03,  0.29it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:40<00:00,  0.30it/s]INFO:anomalib.callbacks.timer:Testing took 307.2765648365021 seconds
Throughput (batch_size=17) : 0.6541338422829268 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [05:06<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9335643649101257     â”‚
â”‚        pixel_AUPRO        â”‚    0.8738002777099609     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb4 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:35,  1.51it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:35,  1.51it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.22it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.22it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.30it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.30it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.32it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.32it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.34it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.34it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.36it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.36it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.36it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.36it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.37it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.37it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.38it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.38it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.38it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.38it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.38it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.38it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:17,  2.39it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:17,  2.39it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.39it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.39it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.40it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.40it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.40it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.40it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.40it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.40it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.40it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.40it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:14,  2.40it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:14,  2.40it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.40it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.40it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.40it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.40it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.41it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.41it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.41it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.41it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.41it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.41it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:12,  2.41it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:12,  2.41it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:11,  2.42it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:11,  2.42it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.42it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.42it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.42it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.42it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.42it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.42it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:10,  2.43it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:10,  2.43it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.43it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.43it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.43it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.43it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.43it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.43it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.43it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.43it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:08,  2.43it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:08,  2.43it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.43it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.43it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.43it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.43it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:06,  2.43it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:06,  2.43it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.43it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.43it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.43it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.43it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.43it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.43it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.43it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.43it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.43it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.43it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.43it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.43it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.43it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.43it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.43it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.43it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.43it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.43it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.43it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.43it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.43it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.43it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.43it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.43it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.44it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.44it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.44it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.44it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.44it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.44it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.44it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.44it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.48it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.48it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:00<00:10,  1.10it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:01<00:08,  1.15it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:02<00:07,  1.17it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:03<00:06,  1.18it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:04<00:05,  1.19it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:05<00:05,  1.19it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:05<00:04,  1.19it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:06<00:03,  1.20it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:07<00:02,  1.20it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:08<00:01,  1.20it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:09<00:00,  1.20it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:09<00:00,  1.20it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [05:54<00:00,  0.15it/s, pixel_AUPRO=0.815]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [05:54<00:00,  0.15it/s, pixel_AUPRO=0.815]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [06:29<00:00,  0.14it/s, pixel_AUPRO=0.815]INFO:anomalib.callbacks.timer:Training took 390.14 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:44,  0.25it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:07<00:38,  0.26it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:11<00:35,  0.25it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:15<00:30,  0.26it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:19<00:26,  0.26it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:22<00:22,  0.26it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:26<00:19,  0.26it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:30<00:15,  0.26it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:34<00:11,  0.26it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:37<00:07,  0.26it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:41<00:03,  0.26it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:44<00:00,  0.27it/s]INFO:anomalib.callbacks.timer:Testing took 597.3043344020844 seconds
Throughput (batch_size=17) : 0.33651187246314845 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [09:56<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9960397481918335     â”‚
â”‚        pixel_AUPRO        â”‚    0.8460772037506104     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pipe_fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.48it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.48it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.23it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.23it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.30it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.30it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.33it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.33it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.36it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.36it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.36it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.36it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.38it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.38it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.39it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.39it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.40it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.40it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.41it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.41it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.41it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.41it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:06,  2.41it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:06,  2.41it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.42it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.42it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.42it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.42it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:04,  2.42it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:04,  2.42it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.43it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.43it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:04,  2.43it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:04,  2.43it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.43it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.43it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.43it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.43it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.43it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.43it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.44it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.44it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.44it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.44it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.44it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.44it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.45it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.45it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.45it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.45it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.45it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.45it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.50it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.50it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:00<00:07,  1.02it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:01<00:06,  1.07it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:02<00:05,  1.09it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:03<00:04,  1.10it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:04<00:03,  1.11it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:05<00:02,  1.11it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:06<00:01,  1.11it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:07<00:00,  1.12it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:08<00:00,  1.12it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:13<00:00,  0.09it/s, pixel_AUPRO=0.938]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:13<00:00,  0.09it/s, pixel_AUPRO=0.938]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:40<00:00,  0.08it/s, pixel_AUPRO=0.938]INFO:anomalib.callbacks.timer:Training took 341.48 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:03<00:31,  0.25it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:07<00:26,  0.26it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:11<00:22,  0.26it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:14<00:18,  0.27it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:18<00:14,  0.27it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:22<00:11,  0.27it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:26<00:07,  0.27it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:29<00:03,  0.27it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:32<00:00,  0.27it/s]INFO:anomalib.callbacks.timer:Testing took 423.02565693855286 seconds
Throughput (batch_size=17) : 0.3545884216232975 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [07:02<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9854000210762024     â”‚
â”‚        pixel_AUPRO        â”‚    0.9396547675132751     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa candle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:32,  1.58it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.58it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.30it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.30it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.37it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.37it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.40it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.40it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.40it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.40it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.41it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.41it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.41it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.41it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.42it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.42it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.42it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.42it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.43it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.43it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.43it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.43it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.43it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.43it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.44it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.44it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.45it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.45it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.45it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.45it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.45it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.45it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.45it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.45it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.45it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.45it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.45it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.45it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.46it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.46it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.46it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.46it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.47it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.47it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.47it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.47it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.47it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.47it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.47it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.47it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.48it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.47it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.48it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.48it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.48it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.48it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.48it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.48it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.49it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.49it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.49it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.49it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.49it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.49it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.49it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.49it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.49it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.49it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.49it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.49it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.49it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.49it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.49it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.49it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.49it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.49it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.49it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.49it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.49it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.49it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.49it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.49it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.49it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.49it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.49it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.49it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.49it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.49it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.49it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.49it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.49it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.49it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.49it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.49it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.49it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.49it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.49it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.49it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.49it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.49it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.49it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.49it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.50it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.50it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.50it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.50it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:02<00:24,  0.45it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:04<00:21,  0.46it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:06<00:19,  0.46it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:08<00:17,  0.46it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:10<00:15,  0.47it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:12<00:12,  0.47it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:14<00:10,  0.47it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:17<00:08,  0.47it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:19<00:06,  0.47it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:21<00:04,  0.47it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:23<00:02,  0.47it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:25<00:00,  0.47it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [06:44<00:00,  0.13it/s, pixel_AUPRO=0.948]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [06:44<00:00,  0.13it/s, pixel_AUPRO=0.948]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [07:22<00:00,  0.12it/s, pixel_AUPRO=0.948]INFO:anomalib.callbacks.timer:Training took 443.77 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:05<00:57,  0.19it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:09<00:49,  0.20it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:14<00:44,  0.20it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:19<00:39,  0.20it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:24<00:34,  0.20it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:29<00:29,  0.21it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:34<00:24,  0.21it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:38<00:19,  0.21it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:43<00:14,  0.21it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:48<00:09,  0.21it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:53<00:04,  0.21it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:57<00:00,  0.21it/s]INFO:anomalib.callbacks.timer:Testing took 508.2164661884308 seconds
Throughput (batch_size=17) : 0.3935330972252407 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [08:27<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8842999339103699     â”‚
â”‚        pixel_AUPRO        â”‚    0.9486863613128662     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa capsules with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/32 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/32 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:20,  1.49it/s]Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:20,  1.49it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.26it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.26it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.32it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.32it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:11,  2.34it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:11,  2.34it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:11,  2.36it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:11,  2.36it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:10,  2.37it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:10,  2.37it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:02<00:10,  2.39it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:02<00:10,  2.39it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:10,  2.39it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:10,  2.39it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:03<00:09,  2.40it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:03<00:09,  2.40it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.40it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.40it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:08,  2.39it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:08,  2.39it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:05<00:08,  2.39it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:05<00:08,  2.39it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:07,  2.39it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:07,  2.39it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:05<00:07,  2.39it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:05<00:07,  2.39it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:07,  2.38it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:07,  2.38it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:06<00:06,  2.38it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:06<00:06,  2.38it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:07<00:06,  2.38it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:07<00:06,  2.38it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:07<00:05,  2.39it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:07<00:05,  2.39it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:07<00:05,  2.39it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:07<00:05,  2.39it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:05,  2.39it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:05,  2.39it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:08<00:04,  2.39it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:08<00:04,  2.39it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.39it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.39it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:09<00:03,  2.39it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:09<00:03,  2.39it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:10<00:03,  2.39it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:10<00:03,  2.39it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:10<00:02,  2.39it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:10<00:02,  2.39it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:10<00:02,  2.39it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:10<00:02,  2.39it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.39it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.39it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:11<00:01,  2.39it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:11<00:01,  2.39it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:12<00:01,  2.38it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:12<00:01,  2.38it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:12<00:00,  2.38it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:12<00:00,  2.38it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:13<00:00,  2.38it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:13<00:00,  2.38it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.39it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.39it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|â–ˆ         | 1/10 [00:01<00:14,  0.62it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:03<00:12,  0.64it/s][A
Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:04<00:10,  0.64it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:06<00:09,  0.65it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:07<00:07,  0.65it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:09<00:06,  0.65it/s][A
Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:10<00:04,  0.65it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:12<00:03,  0.65it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:13<00:01,  0.65it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  0.65it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:14<00:00,  0.43it/s, pixel_AUPRO=nan.0]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:14<00:00,  0.43it/s, pixel_AUPRO=nan.0]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:42<00:00,  0.31it/s, pixel_AUPRO=nan.0]INFO:anomalib.callbacks.timer:Training took 102.92 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|â–ˆ         | 1/10 [00:04<00:39,  0.23it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:08<00:33,  0.24it/s]Testing DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:12<00:29,  0.24it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:16<00:25,  0.24it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:20<00:20,  0.24it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:25<00:16,  0.24it/s]Testing DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:29<00:12,  0.24it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:33<00:08,  0.24it/s]Testing DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:37<00:04,  0.24it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:40<00:00,  0.25it/s]INFO:anomalib.callbacks.timer:Testing took 152.25324773788452 seconds
Throughput (batch_size=17) : 1.050880702889518 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:31<00:00,  0.07it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7308333516120911     â”‚
â”‚        pixel_AUPRO        â”‚    0.8273953199386597     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa cashew with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.45it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.45it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.18it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.18it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.21it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.21it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.23it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.23it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.24it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.24it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.24it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.24it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.25it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.25it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.26it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.26it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.27it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.27it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.27it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:07,  2.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:07,  2.27it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.27it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.27it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.28it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.28it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.27it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.27it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.27it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.27it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:04,  2.27it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:04,  2.27it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.28it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.28it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.28it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.28it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.28it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.28it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.28it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.28it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.28it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.28it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.28it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.28it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.28it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.28it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.28it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.28it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.28it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.28it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.28it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.28it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.33it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:02<00:17,  0.45it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:04<00:15,  0.45it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:06<00:13,  0.46it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:08<00:10,  0.46it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:10<00:08,  0.46it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:13<00:06,  0.46it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:15<00:04,  0.46it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:17<00:02,  0.46it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:19<00:00,  0.46it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:01<00:00,  0.22it/s, pixel_AUPRO=0.833]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:01<00:00,  0.22it/s, pixel_AUPRO=0.833]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:40<00:00,  0.17it/s, pixel_AUPRO=0.833]INFO:anomalib.callbacks.timer:Training took 161.38 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:05<00:42,  0.19it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:10<00:36,  0.19it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:15<00:31,  0.19it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:20<00:25,  0.19it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:25<00:20,  0.20it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:30<00:15,  0.19it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:36<00:10,  0.19it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:41<00:05,  0.20it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:45<00:00,  0.20it/s]INFO:anomalib.callbacks.timer:Testing took 208.8997859954834 seconds
Throughput (batch_size=17) : 0.718047647991574 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [03:28<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9132000207901001     â”‚
â”‚        pixel_AUPRO        â”‚    0.8366385102272034     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa chewinggum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.53it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.53it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.33it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.33it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.40it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.40it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.43it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.43it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:08,  2.46it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:08,  2.46it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.49it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.48it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.50it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.50it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.51it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.51it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.52it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.52it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:03<00:06,  2.52it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:03<00:06,  2.52it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.53it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.53it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.53it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.53it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.53it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.53it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.54it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.54it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.54it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.54it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.55it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.55it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.55it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.55it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.55it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.55it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.55it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.55it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.55it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.55it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.55it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.55it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.55it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.55it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.55it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.55it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.55it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.55it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.56it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.56it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.56it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.56it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.59it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.59it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:02<00:16,  0.49it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:04<00:14,  0.50it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:05<00:11,  0.50it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:07<00:09,  0.50it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:09<00:07,  0.51it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:11<00:05,  0.51it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:13<00:03,  0.51it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:15<00:01,  0.51it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:17<00:00,  0.51it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:56<00:00,  0.11it/s, pixel_AUPRO=0.677]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:56<00:00,  0.11it/s, pixel_AUPRO=0.677]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:31<00:00,  0.10it/s, pixel_AUPRO=0.677]INFO:anomalib.callbacks.timer:Training took 272.22 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:05<00:41,  0.19it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:09<00:33,  0.21it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:14<00:28,  0.21it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:18<00:23,  0.21it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:23<00:18,  0.21it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:27<00:13,  0.21it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:32<00:09,  0.21it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:37<00:04,  0.21it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:41<00:00,  0.22it/s]INFO:anomalib.callbacks.timer:Testing took 434.4820544719696 seconds
Throughput (batch_size=17) : 0.345238654752488 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [07:13<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9734000563621521     â”‚
â”‚        pixel_AUPRO        â”‚    0.6862027049064636     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.57it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.57it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.31it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.31it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.37it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.37it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.39it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.39it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.41it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.41it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.35it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.35it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.31it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.31it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.27it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.27it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:04<00:08,  2.24it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:04<00:08,  2.24it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.22it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.22it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:05<00:07,  2.20it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:05<00:07,  2.20it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.22it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.22it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.24it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.24it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.26it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.26it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.27it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.27it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.29it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.29it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.30it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.30it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.31it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.31it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.32it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.32it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.33it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.33it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.34it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.34it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.34it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.34it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.35it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.35it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.36it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.36it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.36it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.36it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.36it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.42it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.42it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:01<00:12,  0.63it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:03<00:10,  0.64it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:04<00:09,  0.65it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:06<00:07,  0.65it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:07<00:06,  0.65it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:09<00:04,  0.65it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:10<00:03,  0.65it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:12<00:01,  0.65it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:13<00:00,  0.65it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:18<00:00,  0.14it/s, pixel_AUPRO=0.858]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:18<00:00,  0.14it/s, pixel_AUPRO=0.858]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:47<00:00,  0.12it/s, pixel_AUPRO=0.858]INFO:anomalib.callbacks.timer:Training took 228.77 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:04<00:34,  0.23it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:08<00:29,  0.24it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:12<00:25,  0.24it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:16<00:20,  0.24it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:20<00:16,  0.24it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:24<00:12,  0.24it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:28<00:08,  0.24it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:32<00:04,  0.24it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:36<00:00,  0.25it/s]INFO:anomalib.callbacks.timer:Testing took 274.7899181842804 seconds
Throughput (batch_size=17) : 0.5458715552271702 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:33<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.941800057888031     â”‚
â”‚        pixel_AUPRO        â”‚    0.8628589510917664     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.56it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.56it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.34it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.34it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.40it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.40it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.42it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.42it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.43it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.43it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.44it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.44it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.42it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.42it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.39it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.39it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.36it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.36it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.36it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.36it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.37it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.37it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.37it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.37it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.38it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.38it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.39it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.39it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.39it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.39it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.40it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.40it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:14,  2.40it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:14,  2.40it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.40it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.40it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:14,  2.40it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:14,  2.40it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.40it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.40it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.41it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.41it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:12,  2.41it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:12,  2.41it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.42it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.42it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.42it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.42it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.42it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.42it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.42it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.42it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.43it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.43it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.43it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.43it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.43it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.43it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.43it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.43it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.43it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.43it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.44it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.44it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.44it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.44it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.44it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.44it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.43it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.43it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:07,  2.42it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:07,  2.42it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.42it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.42it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.42it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.42it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.42it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.42it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.42it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.42it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.42it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.42it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.42it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.42it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.42it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.42it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.43it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.43it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.43it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.43it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.43it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.43it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.43it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.43it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.43it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.43it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.43it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.43it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.43it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.43it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.43it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.43it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.43it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.43it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.43it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.43it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:01<00:17,  0.62it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:03<00:15,  0.64it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:04<00:13,  0.64it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:06<00:12,  0.65it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:07<00:10,  0.65it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:09<00:09,  0.65it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:10<00:07,  0.65it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:12<00:06,  0.65it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:13<00:04,  0.65it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:15<00:03,  0.65it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:16<00:01,  0.65it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:18<00:00,  0.65it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:58<00:00,  0.18it/s, pixel_AUPRO=0.936]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:58<00:00,  0.18it/s, pixel_AUPRO=0.936]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:27<00:00,  0.16it/s, pixel_AUPRO=0.936]INFO:anomalib.callbacks.timer:Training took 328.44 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:47,  0.23it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:08<00:41,  0.24it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:12<00:37,  0.24it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:16<00:33,  0.24it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:20<00:29,  0.24it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:25<00:25,  0.23it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:30<00:21,  0.23it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:34<00:17,  0.23it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:38<00:12,  0.23it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:42<00:08,  0.23it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:46<00:04,  0.23it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:50<00:00,  0.24it/s]INFO:anomalib.callbacks.timer:Testing took 390.77142572402954 seconds
Throughput (batch_size=17) : 0.5118081487903978 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [06:29<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9406000375747681     â”‚
â”‚        pixel_AUPRO        â”‚    0.9359332323074341     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.55it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.55it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.36it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.36it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.40it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.40it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.43it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.43it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.45it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.45it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.47it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.47it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.48it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.48it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.48it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.48it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.48it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.48it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.49it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.49it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.49it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.49it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.49it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.49it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.50it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.50it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.50it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.50it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.50it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.50it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.49it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.49it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.49it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.49it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.49it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.49it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.50it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.50it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.50it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.50it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.50it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.50it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.50it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.50it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.51it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.51it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.51it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.51it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:11,  2.51it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:11,  2.51it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.51it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.51it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.51it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.51it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.52it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.52it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.52it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.52it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.52it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.52it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.52it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.52it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.52it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.52it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.52it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.52it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.52it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.52it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.52it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.52it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.52it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.52it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.52it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.52it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:05,  2.52it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:05,  2.52it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.52it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.52it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.52it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.52it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.52it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.52it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.52it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.52it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.52it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.52it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.52it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.52it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.52it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.52it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.52it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.52it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.52it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.52it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.52it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.52it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.52it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.52it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.52it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.52it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.52it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.52it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.52it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.52it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:20<00:00,  2.53it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:20<00:00,  2.53it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:01<00:17,  0.62it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:03<00:15,  0.64it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:04<00:14,  0.64it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:06<00:12,  0.65it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:07<00:10,  0.65it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:09<00:09,  0.65it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:10<00:07,  0.65it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:12<00:06,  0.65it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:13<00:04,  0.65it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:15<00:03,  0.65it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:16<00:01,  0.65it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:18<00:00,  0.65it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [03:48<00:00,  0.23it/s, pixel_AUPRO=0.834]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [03:48<00:00,  0.23it/s, pixel_AUPRO=0.834]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:10<00:00,  0.21it/s, pixel_AUPRO=0.834]INFO:anomalib.callbacks.timer:Training took 251.49 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:48,  0.23it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:08<00:41,  0.24it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:12<00:37,  0.24it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:16<00:33,  0.24it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:20<00:28,  0.24it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:24<00:24,  0.25it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:28<00:20,  0.24it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:32<00:16,  0.25it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:36<00:12,  0.25it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:40<00:08,  0.25it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:44<00:04,  0.25it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:48<00:00,  0.25it/s]INFO:anomalib.callbacks.timer:Testing took 208.78907799720764 seconds
Throughput (batch_size=17) : 0.9579045126233797 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [03:27<00:00,  0.06it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8344999551773071     â”‚
â”‚        pixel_AUPRO        â”‚    0.8689491152763367     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.52it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.52it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.29it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.28it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.33it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.33it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.36it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.36it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.39it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.39it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.40it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.40it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.41it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.41it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.42it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.42it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.43it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.43it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.43it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.43it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:17,  2.44it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:17,  2.44it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:17,  2.43it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:17,  2.43it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:16,  2.44it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:16,  2.44it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.44it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.44it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.44it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.44it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.44it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.44it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:15,  2.43it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:15,  2.43it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:14,  2.43it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:14,  2.43it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.43it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.43it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:13,  2.44it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:13,  2.44it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.44it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.44it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.44it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.44it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.44it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.44it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:12,  2.44it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:12,  2.44it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:11,  2.44it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:11,  2.44it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.44it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.44it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.44it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.44it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.44it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.44it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:10,  2.44it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:10,  2.44it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.44it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.44it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.44it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.44it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.44it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.44it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.44it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.44it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:08,  2.44it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:08,  2.44it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.45it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.45it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.45it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.45it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:06,  2.45it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:06,  2.45it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.45it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.45it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:15<00:06,  2.45it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:15<00:06,  2.45it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.45it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.45it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.45it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.45it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.45it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.45it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.44it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.44it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.44it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.44it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.44it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.44it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.44it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.44it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.44it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.44it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.44it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.44it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.44it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.44it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.44it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.44it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.44it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.44it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.45it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.45it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.45it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.45it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.48it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.48it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:01<00:20,  0.54it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:03<00:18,  0.55it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:05<00:16,  0.56it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:07<00:14,  0.56it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:08<00:12,  0.56it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:10<00:10,  0.56it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:12<00:08,  0.56it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:14<00:07,  0.56it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:15<00:05,  0.56it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:17<00:03,  0.56it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:19<00:01,  0.56it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:21<00:00,  0.56it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:34<00:00,  0.25it/s, pixel_AUPRO=0.855]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:34<00:00,  0.25it/s, pixel_AUPRO=0.855]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:07<00:00,  0.22it/s, pixel_AUPRO=0.855]INFO:anomalib.callbacks.timer:Training took 248.20 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:52,  0.21it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:09<00:46,  0.22it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:14<00:42,  0.21it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:18<00:37,  0.22it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:23<00:32,  0.22it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:27<00:27,  0.22it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:32<00:22,  0.22it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:36<00:18,  0.22it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:40<00:13,  0.22it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:45<00:09,  0.22it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:50<00:04,  0.22it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:53<00:00,  0.22it/s]INFO:anomalib.callbacks.timer:Testing took 365.35419821739197 seconds
Throughput (batch_size=17) : 0.547413991616422 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [06:04<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9495999813079834     â”‚
â”‚        pixel_AUPRO        â”‚    0.9026592969894409     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.51it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.51it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.22it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.22it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:22,  2.25it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:22,  2.25it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:21,  2.27it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:21,  2.26it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:21,  2.28it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:21,  2.28it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:20,  2.29it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:20,  2.29it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:03<00:20,  2.29it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:03<00:20,  2.29it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:19,  2.29it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:19,  2.29it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:19,  2.30it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:19,  2.30it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.30it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.30it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:18,  2.31it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:18,  2.31it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.31it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.31it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:17,  2.32it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:17,  2.32it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:06<00:16,  2.32it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:06<00:16,  2.32it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.32it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.32it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.31it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.31it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.32it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.32it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:15,  2.32it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:15,  2.32it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.33it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.33it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:14,  2.33it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:14,  2.33it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.33it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.33it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.34it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.34it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.34it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.34it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.34it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.34it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.34it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.34it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:11<00:11,  2.34it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:11<00:11,  2.34it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:11,  2.34it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:11,  2.34it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.34it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.34it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:10,  2.34it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:10,  2.34it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.33it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.33it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:13<00:09,  2.34it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:13<00:09,  2.34it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.34it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.34it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:14<00:08,  2.34it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:14<00:08,  2.34it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:08,  2.34it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:08,  2.34it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.34it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.34it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:15<00:07,  2.34it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:15<00:07,  2.34it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.34it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.34it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:16<00:06,  2.34it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:16<00:06,  2.34it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.34it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.34it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:17<00:05,  2.34it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:17<00:05,  2.34it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:17<00:05,  2.34it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:17<00:05,  2.34it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.35it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.35it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:18<00:04,  2.34it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:18<00:04,  2.34it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.34it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.34it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:19<00:03,  2.35it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:19<00:03,  2.35it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.35it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.35it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:20<00:02,  2.35it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:20<00:02,  2.35it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:20<00:02,  2.35it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:20<00:02,  2.35it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.35it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.35it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:21<00:01,  2.35it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:21<00:01,  2.35it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.35it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.35it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:22<00:00,  2.35it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:22<00:00,  2.35it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:22<00:00,  2.35it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:22<00:00,  2.35it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:01<00:20,  0.54it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:03<00:18,  0.55it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:05<00:16,  0.56it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:07<00:14,  0.56it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:08<00:12,  0.56it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:10<00:10,  0.56it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:12<00:08,  0.56it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:14<00:07,  0.56it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:15<00:05,  0.56it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:17<00:03,  0.56it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:19<00:01,  0.56it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:21<00:00,  0.56it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:05<00:00,  0.22it/s, pixel_AUPRO=0.900]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:05<00:00,  0.22it/s, pixel_AUPRO=0.900]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:37<00:00,  0.19it/s, pixel_AUPRO=0.900]INFO:anomalib.callbacks.timer:Training took 278.93 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:54,  0.20it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:09<00:47,  0.21it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:14<00:43,  0.21it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:19<00:38,  0.21it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:23<00:33,  0.21it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:28<00:28,  0.21it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:33<00:23,  0.21it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:37<00:18,  0.21it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:42<00:14,  0.21it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:46<00:09,  0.21it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:51<00:04,  0.21it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:55<00:00,  0.22it/s]INFO:anomalib.callbacks.timer:Testing took 407.4684844017029 seconds
Throughput (batch_size=17) : 0.490835506686279 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [06:46<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9302999973297119     â”‚
â”‚        pixel_AUPRO        â”‚    0.9092594981193542     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb3 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:33,  1.60it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:33,  1.60it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:21,  2.41it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:21,  2.41it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:20,  2.47it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:20,  2.47it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:19,  2.51it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:19,  2.51it/s]Epoch 0:   9%|â–‰         | 5/54 [00:01<00:19,  2.53it/s]Epoch 0:   9%|â–‰         | 5/54 [00:01<00:19,  2.53it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:18,  2.55it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:18,  2.55it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:18,  2.56it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:18,  2.56it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:17,  2.57it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:17,  2.57it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:17,  2.57it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:17,  2.57it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:03<00:17,  2.58it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:03<00:17,  2.58it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:16,  2.58it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:16,  2.58it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:16,  2.58it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:16,  2.58it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:15,  2.58it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:15,  2.58it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:15,  2.59it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:15,  2.59it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:05<00:15,  2.59it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:05<00:15,  2.59it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:14,  2.59it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:14,  2.59it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:14,  2.58it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:14,  2.58it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:06<00:13,  2.58it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:06<00:13,  2.58it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:13,  2.58it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:13,  2.58it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:07<00:13,  2.58it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:07<00:13,  2.58it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:12,  2.58it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:12,  2.58it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:08<00:12,  2.59it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:08<00:12,  2.59it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:08<00:11,  2.59it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:08<00:11,  2.59it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:11,  2.59it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:11,  2.59it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:09<00:11,  2.59it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:09<00:11,  2.59it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:10,  2.59it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:10,  2.59it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:10<00:10,  2.59it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:10<00:10,  2.59it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:10<00:10,  2.59it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:10<00:10,  2.59it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:09,  2.59it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:09,  2.59it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:11<00:09,  2.60it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:11<00:09,  2.60it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:11<00:08,  2.60it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:11<00:08,  2.60it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:12<00:08,  2.60it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:12<00:08,  2.60it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:12<00:08,  2.60it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:12<00:08,  2.60it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:07,  2.60it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:07,  2.60it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:13<00:07,  2.59it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:13<00:07,  2.59it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:13<00:06,  2.59it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:13<00:06,  2.59it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:14<00:06,  2.59it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:14<00:06,  2.59it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:14<00:06,  2.59it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:14<00:06,  2.59it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:15<00:05,  2.59it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:15<00:05,  2.59it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:15<00:05,  2.59it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:15<00:05,  2.59it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:15<00:05,  2.59it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:15<00:05,  2.59it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:16<00:04,  2.59it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:16<00:04,  2.59it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:16<00:04,  2.59it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:16<00:04,  2.59it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:16<00:03,  2.59it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:16<00:03,  2.59it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:17<00:03,  2.59it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:17<00:03,  2.59it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:17<00:03,  2.59it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:17<00:03,  2.59it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:18<00:02,  2.59it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:18<00:02,  2.59it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:18<00:02,  2.59it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:18<00:02,  2.59it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:18<00:01,  2.59it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:18<00:01,  2.59it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:19<00:01,  2.59it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:19<00:01,  2.59it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:19<00:01,  2.59it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:19<00:01,  2.59it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:20<00:00,  2.59it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:20<00:00,  2.59it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:20<00:00,  2.60it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:20<00:00,  2.60it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:20<00:00,  2.63it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:20<00:00,  2.63it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:01<00:16,  0.65it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:02<00:14,  0.67it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:04<00:13,  0.68it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:05<00:11,  0.68it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:07<00:10,  0.68it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:08<00:08,  0.69it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:10<00:07,  0.69it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:11<00:05,  0.69it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:13<00:04,  0.69it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:14<00:02,  0.69it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:15<00:01,  0.69it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:17<00:00,  0.69it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:34<00:00,  0.25it/s, pixel_AUPRO=0.872]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:34<00:00,  0.25it/s, pixel_AUPRO=0.872]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:01<00:00,  0.22it/s, pixel_AUPRO=0.872]INFO:anomalib.callbacks.timer:Training took 242.04 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:50,  0.22it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:08<00:43,  0.23it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:13<00:39,  0.23it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:17<00:34,  0.23it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:21<00:30,  0.23it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:25<00:25,  0.23it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:30<00:21,  0.23it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:34<00:17,  0.23it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:38<00:12,  0.23it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:42<00:08,  0.24it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:47<00:04,  0.23it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:50<00:00,  0.24it/s]INFO:anomalib.callbacks.timer:Testing took 312.4932041168213 seconds
Throughput (batch_size=17) : 0.6432139878627854 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [05:11<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9574257731437683     â”‚
â”‚        pixel_AUPRO        â”‚    0.8851308226585388     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb4 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.55it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.55it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.32it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.32it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.36it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.36it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.38it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.38it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.39it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.39it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.40it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.40it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.40it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.40it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.41it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.41it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.41it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.41it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.41it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.41it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:17,  2.42it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:17,  2.42it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:17,  2.42it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:17,  2.42it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:16,  2.42it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:16,  2.42it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.42it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.42it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.42it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.42it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.42it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.42it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.42it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.42it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:14,  2.42it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:14,  2.42it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.42it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.42it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.42it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.42it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.42it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.42it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.43it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.43it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.43it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.43it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:12,  2.43it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:12,  2.43it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:11,  2.43it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:11,  2.43it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.43it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.43it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.44it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.44it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.43it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.43it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:10,  2.43it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:10,  2.43it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.44it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.44it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.44it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.44it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.44it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.44it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.44it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.44it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:08,  2.44it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:08,  2.44it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.44it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.44it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.44it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.44it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:06,  2.44it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:06,  2.44it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.44it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.44it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.44it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.44it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.44it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.44it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.43it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.43it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.43it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.43it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.43it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.43it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.43it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.43it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.43it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.43it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.43it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.43it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.43it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.43it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.43it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.43it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.43it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.43it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.44it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.44it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.43it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.43it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.44it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.44it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.44it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.44it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.47it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.47it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:01<00:21,  0.51it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:03<00:19,  0.52it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:05<00:17,  0.52it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:07<00:15,  0.52it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:09<00:13,  0.52it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:11<00:11,  0.53it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:13<00:09,  0.53it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:15<00:07,  0.53it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:17<00:05,  0.53it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:18<00:03,  0.53it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:20<00:01,  0.53it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:22<00:00,  0.53it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [06:06<00:00,  0.15it/s, pixel_AUPRO=0.839]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [06:06<00:00,  0.15it/s, pixel_AUPRO=0.839]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [06:40<00:00,  0.13it/s, pixel_AUPRO=0.839]INFO:anomalib.callbacks.timer:Training took 401.84 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:54,  0.20it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:09<00:47,  0.21it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:14<00:43,  0.21it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:19<00:38,  0.21it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:24<00:33,  0.21it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:28<00:28,  0.21it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:33<00:24,  0.21it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:38<00:19,  0.21it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:43<00:14,  0.21it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:47<00:09,  0.21it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:53<00:04,  0.21it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:57<00:00,  0.21it/s]INFO:anomalib.callbacks.timer:Testing took 616.784078836441 seconds
Throughput (batch_size=17) : 0.3258838982666108 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [10:15<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9978217482566833     â”‚
â”‚        pixel_AUPRO        â”‚    0.8639156818389893     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pipe_fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.57it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.57it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.38it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.38it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:09,  2.45it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:09,  2.45it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.48it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.48it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:01<00:08,  2.50it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:01<00:08,  2.50it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.53it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.53it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:07,  2.54it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:07,  2.54it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.54it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.54it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.55it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.55it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:03<00:06,  2.55it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:03<00:06,  2.55it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.56it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.56it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.56it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.56it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.56it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.56it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.57it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.57it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.57it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.57it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.57it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.57it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.58it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.58it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:06<00:03,  2.57it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:06<00:03,  2.57it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.58it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.58it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.58it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.58it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.58it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.58it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.58it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.58it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:08<00:01,  2.59it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:08<00:01,  2.59it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.59it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.59it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.59it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.59it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.59it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.59it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.64it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.64it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:02<00:17,  0.47it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:04<00:14,  0.48it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:06<00:12,  0.48it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:08<00:10,  0.48it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:10<00:08,  0.48it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:12<00:06,  0.48it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:14<00:04,  0.48it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:16<00:02,  0.48it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:18<00:00,  0.48it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:29<00:00,  0.08it/s, pixel_AUPRO=0.944]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:29<00:00,  0.08it/s, pixel_AUPRO=0.944]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [06:06<00:00,  0.07it/s, pixel_AUPRO=0.944]INFO:anomalib.callbacks.timer:Training took 367.80 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:05<00:41,  0.19it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:10<00:35,  0.20it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:15<00:30,  0.20it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:20<00:25,  0.20it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:25<00:20,  0.20it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:29<00:14,  0.20it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:34<00:09,  0.20it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:39<00:04,  0.20it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:44<00:00,  0.20it/s]INFO:anomalib.callbacks.timer:Testing took 438.90681290626526 seconds
Throughput (batch_size=17) : 0.3417581946535759 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [07:18<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9922000169754028     â”‚
â”‚        pixel_AUPRO        â”‚    0.9450855255126953     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa candle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.57it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.57it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.35it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.35it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.42it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.42it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:19,  2.46it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:19,  2.46it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.48it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.48it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:18,  2.50it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:18,  2.50it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.51it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.51it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:17,  2.52it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:17,  2.52it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.52it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.52it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:03<00:17,  2.53it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:03<00:17,  2.53it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.53it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.53it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.53it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.53it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:15,  2.54it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:15,  2.54it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.54it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.54it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:05<00:14,  2.54it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:05<00:14,  2.54it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.54it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.54it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.54it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.54it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:13,  2.54it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:13,  2.54it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.54it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.54it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:07<00:12,  2.54it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:07<00:12,  2.54it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.54it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.54it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.54it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.54it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.55it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.55it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.55it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.55it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:10,  2.55it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:10,  2.55it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.55it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.55it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.55it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.55it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:10<00:09,  2.55it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:10<00:09,  2.55it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.55it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.55it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.55it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.55it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.55it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.55it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.55it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.55it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:12<00:07,  2.55it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:12<00:07,  2.55it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.55it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.55it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.55it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.55it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.56it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.56it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.56it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.56it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:14<00:05,  2.56it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:14<00:05,  2.56it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.56it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.56it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.56it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.56it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.56it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.56it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.56it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.56it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:16<00:03,  2.56it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:16<00:03,  2.56it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.56it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.56it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.56it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.56it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:17<00:02,  2.56it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:17<00:02,  2.56it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.56it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.56it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:18<00:01,  2.56it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:18<00:01,  2.56it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.56it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.56it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.56it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.56it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:19<00:00,  2.57it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:19<00:00,  2.57it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.57it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.57it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:20<00:00,  2.57it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:20<00:00,  2.57it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:34,  0.32it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:06<00:30,  0.33it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:09<00:27,  0.33it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:12<00:24,  0.33it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:15<00:21,  0.33it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:18<00:18,  0.33it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:21<00:15,  0.33it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:24<00:12,  0.33it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:27<00:09,  0.33it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:30<00:06,  0.33it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:33<00:03,  0.33it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:36<00:00,  0.33it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [06:53<00:00,  0.13it/s, pixel_AUPRO=0.951]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [06:53<00:00,  0.13it/s, pixel_AUPRO=0.951]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [07:32<00:00,  0.12it/s, pixel_AUPRO=0.951]INFO:anomalib.callbacks.timer:Training took 453.65 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:06<01:07,  0.16it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:12<01:00,  0.17it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:18<00:54,  0.17it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:23<00:47,  0.17it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:30<00:42,  0.17it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:36<00:36,  0.17it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:42<00:30,  0.17it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:48<00:24,  0.17it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:53<00:17,  0.17it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:59<00:11,  0.17it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:06<00:06,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:11<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 520.650514125824 seconds
Throughput (batch_size=17) : 0.38413483627458134 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [08:39<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8659999966621399     â”‚
â”‚        pixel_AUPRO        â”‚    0.9511160254478455     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa capsules with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/32 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/32 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:20,  1.50it/s]Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:20,  1.50it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.20it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.20it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.27it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.27it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:12,  2.30it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:12,  2.30it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:11,  2.31it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:11,  2.31it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:11,  2.33it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:11,  2.33it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:03<00:10,  2.33it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:03<00:10,  2.33it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:10,  2.34it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:10,  2.34it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:03<00:09,  2.33it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:03<00:09,  2.33it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.34it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.34it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:08,  2.35it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:08,  2.35it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:05<00:08,  2.35it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:05<00:08,  2.35it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:08,  2.35it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:08,  2.35it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:05<00:07,  2.36it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:05<00:07,  2.35it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:07,  2.36it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:07,  2.36it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:06<00:06,  2.36it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:06<00:06,  2.36it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:07<00:06,  2.36it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:07<00:06,  2.36it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:07<00:05,  2.36it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:07<00:05,  2.36it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:08<00:05,  2.36it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:08<00:05,  2.36it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:05,  2.36it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:05,  2.36it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:08<00:04,  2.36it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:08<00:04,  2.36it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.36it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.36it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:09<00:03,  2.36it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:09<00:03,  2.36it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:10<00:03,  2.37it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:10<00:03,  2.37it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:10<00:02,  2.37it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:10<00:02,  2.37it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:10<00:02,  2.37it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:10<00:02,  2.37it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.37it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.37it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:11<00:01,  2.37it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:11<00:01,  2.37it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:12<00:01,  2.37it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:12<00:01,  2.37it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:12<00:00,  2.37it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:12<00:00,  2.37it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:13<00:00,  2.37it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:13<00:00,  2.37it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.38it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.38it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|â–ˆ         | 1/10 [00:02<00:20,  0.44it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:04<00:17,  0.45it/s][A
Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:15,  0.46it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:08<00:13,  0.46it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:10<00:10,  0.46it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:13<00:08,  0.46it/s][A
Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:15<00:06,  0.46it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:04,  0.46it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:19<00:02,  0.46it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  0.46it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:23<00:00,  0.38it/s, pixel_AUPRO=nan.0]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:23<00:00,  0.38it/s, pixel_AUPRO=nan.0]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:51<00:00,  0.29it/s, pixel_AUPRO=nan.0]INFO:anomalib.callbacks.timer:Training took 112.65 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|â–ˆ         | 1/10 [00:05<00:47,  0.19it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:10<00:40,  0.20it/s]Testing DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:15<00:35,  0.20it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:19<00:29,  0.20it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:24<00:24,  0.20it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:29<00:19,  0.20it/s]Testing DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:34<00:14,  0.20it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:39<00:09,  0.20it/s]Testing DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:44<00:04,  0.20it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:47<00:00,  0.21it/s]INFO:anomalib.callbacks.timer:Testing took 158.5898859500885 seconds
Throughput (batch_size=17) : 1.008891576164922 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:37<00:00,  0.06it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7586666345596313     â”‚
â”‚        pixel_AUPRO        â”‚    0.8294951319694519     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa cashew with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.47it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.47it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.15it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.15it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.21it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.21it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.23it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.23it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.25it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.25it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.27it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.27it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.28it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.28it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.29it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.29it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.29it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.29it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.29it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.29it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.29it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.30it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.30it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.30it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.30it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.30it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.30it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.31it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.31it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.31it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.31it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.31it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.31it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.31it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.31it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.31it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.31it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.31it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.31it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.32it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.32it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.32it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.32it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.32it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.32it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.32it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.32it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.33it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.33it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.33it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.38it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.38it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:03<00:25,  0.32it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:06<00:21,  0.32it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:09<00:18,  0.32it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:12<00:15,  0.32it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:15<00:12,  0.32it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:18<00:09,  0.32it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:21<00:06,  0.32it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:24<00:03,  0.32it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:27<00:00,  0.32it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:11<00:00,  0.21it/s, pixel_AUPRO=0.838]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:11<00:00,  0.21it/s, pixel_AUPRO=0.838]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:52<00:00,  0.16it/s, pixel_AUPRO=0.838]INFO:anomalib.callbacks.timer:Training took 173.14 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:06<00:50,  0.16it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:12<00:43,  0.16it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:18<00:37,  0.16it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:24<00:31,  0.16it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:31<00:24,  0.16it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:37<00:18,  0.16it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:43<00:12,  0.16it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:49<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:55<00:00,  0.16it/s]INFO:anomalib.callbacks.timer:Testing took 219.48987913131714 seconds
Throughput (batch_size=17) : 0.6834028092487011 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [03:38<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9273999929428101     â”‚
â”‚        pixel_AUPRO        â”‚    0.8413073420524597     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa chewinggum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.53it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.53it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.36it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.36it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:09,  2.42it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:09,  2.42it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.45it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.45it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:08,  2.47it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:08,  2.47it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.48it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.48it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.48it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.48it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.48it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.48it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.49it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.49it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:06,  2.49it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:06,  2.49it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.50it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.50it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.50it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.50it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.51it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.51it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.52it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.52it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.52it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.52it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.51it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.51it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.52it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.52it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.52it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.52it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.52it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.52it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.52it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.52it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.52it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.52it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.53it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.53it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.53it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.53it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.53it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.53it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.53it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.53it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.53it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.53it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.57it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.57it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:02<00:22,  0.35it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:05<00:19,  0.35it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:08<00:16,  0.36it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:11<00:14,  0.36it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:14<00:11,  0.36it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:16<00:08,  0.36it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:19<00:05,  0.36it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:22<00:02,  0.36it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:25<00:00,  0.36it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:00<00:00,  0.11it/s, pixel_AUPRO=0.674]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:00<00:00,  0.11it/s, pixel_AUPRO=0.674]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:36<00:00,  0.10it/s, pixel_AUPRO=0.674]INFO:anomalib.callbacks.timer:Training took 277.18 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:06<00:48,  0.17it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:11<00:40,  0.17it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:17<00:34,  0.17it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:23<00:28,  0.17it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:28<00:23,  0.17it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:34<00:17,  0.17it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:40<00:11,  0.17it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:46<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:51<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 440.66715478897095 seconds
Throughput (batch_size=17) : 0.3403929663689884 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [07:19<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.973800003528595     â”‚
â”‚        pixel_AUPRO        â”‚    0.6822441220283508     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.56it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.56it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.32it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.32it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.38it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.38it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.42it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.42it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.44it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.44it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.46it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.46it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.47it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.47it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.49it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.49it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.49it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.49it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:03<00:06,  2.50it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:03<00:06,  2.50it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.51it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.51it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.52it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.52it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.52it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.52it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.53it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.53it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.53it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.53it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.53it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.53it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.53it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.53it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.54it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.54it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.54it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.54it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.54it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.54it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.54it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.54it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.54it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.54it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.55it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.55it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.55it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.55it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.55it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.55it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.55it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.55it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.60it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.60it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:02<00:18,  0.44it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:04<00:15,  0.45it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:06<00:13,  0.46it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:08<00:10,  0.46it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:10<00:08,  0.46it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:13<00:06,  0.46it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:15<00:04,  0.46it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:17<00:02,  0.46it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:19<00:00,  0.46it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:26<00:00,  0.13it/s, pixel_AUPRO=0.860]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:26<00:00,  0.13it/s, pixel_AUPRO=0.860]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:54<00:00,  0.12it/s, pixel_AUPRO=0.860]INFO:anomalib.callbacks.timer:Training took 235.51 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:05<00:42,  0.19it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:10<00:35,  0.20it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:15<00:31,  0.19it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:20<00:25,  0.20it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:25<00:20,  0.20it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:30<00:15,  0.20it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:35<00:10,  0.20it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:39<00:04,  0.20it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:44<00:00,  0.20it/s]INFO:anomalib.callbacks.timer:Testing took 283.2432861328125 seconds
Throughput (batch_size=17) : 0.5295800724811007 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:42<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9429999589920044     â”‚
â”‚        pixel_AUPRO        â”‚    0.8649203777313232     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.56it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.56it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:23,  2.13it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:23,  2.13it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:23,  2.13it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:23,  2.13it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:22,  2.13it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:22,  2.13it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:22,  2.14it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:22,  2.14it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:21,  2.14it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:21,  2.14it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:03<00:21,  2.15it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:03<00:21,  2.15it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:20,  2.15it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:20,  2.15it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:04<00:20,  2.15it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:04<00:20,  2.15it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:19,  2.16it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:19,  2.16it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:05<00:19,  2.16it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:05<00:19,  2.16it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:18,  2.16it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:18,  2.16it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:06<00:18,  2.16it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:06<00:18,  2.16it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:06<00:17,  2.17it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:06<00:17,  2.17it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:17,  2.17it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:17,  2.17it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:07<00:17,  2.16it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:07<00:17,  2.16it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:16,  2.17it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:16,  2.17it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:08<00:15,  2.19it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:08<00:15,  2.19it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:15,  2.21it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:15,  2.21it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:14,  2.22it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:14,  2.22it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:09<00:14,  2.24it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:09<00:14,  2.24it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.25it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.25it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:10<00:13,  2.26it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:10<00:13,  2.26it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.27it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.27it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:12,  2.28it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:12,  2.28it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:11<00:11,  2.29it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:11<00:11,  2.29it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:11,  2.30it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:11,  2.30it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:12<00:10,  2.30it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:12<00:10,  2.30it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:10,  2.31it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:10,  2.31it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.32it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.32it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:13<00:09,  2.33it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:13<00:09,  2.32it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:09,  2.33it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:09,  2.33it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:14<00:08,  2.34it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:14<00:08,  2.34it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:08,  2.34it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:08,  2.34it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.35it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.35it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:15<00:07,  2.35it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:15<00:07,  2.35it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.36it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.36it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:16<00:06,  2.36it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:16<00:06,  2.36it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.36it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.36it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.37it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.37it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:17<00:05,  2.37it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:17<00:05,  2.37it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.37it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.37it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:18<00:04,  2.38it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:18<00:04,  2.38it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.38it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.38it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.38it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.38it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.39it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.39it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.39it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.39it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:20<00:02,  2.39it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:20<00:02,  2.39it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.40it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.40it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.40it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.40it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.40it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.40it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.41it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.41it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.41it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.41it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:02<00:24,  0.44it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:04<00:22,  0.45it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:06<00:19,  0.45it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:08<00:17,  0.46it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:10<00:15,  0.46it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:13<00:13,  0.46it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:15<00:10,  0.46it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:17<00:08,  0.46it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:19<00:06,  0.46it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:21<00:04,  0.46it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:24<00:02,  0.46it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:26<00:00,  0.46it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:03<00:00,  0.17it/s, pixel_AUPRO=0.933]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:03<00:00,  0.17it/s, pixel_AUPRO=0.933]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:31<00:00,  0.16it/s, pixel_AUPRO=0.933]INFO:anomalib.callbacks.timer:Training took 331.97 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:05<00:56,  0.20it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:09<00:49,  0.20it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:15<00:46,  0.19it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:20<00:41,  0.19it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:25<00:35,  0.19it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:30<00:30,  0.20it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:36<00:25,  0.19it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:41<00:20,  0.19it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:46<00:15,  0.19it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:51<00:10,  0.19it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:56<00:05,  0.19it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:01<00:00,  0.20it/s]INFO:anomalib.callbacks.timer:Testing took 333.91279697418213 seconds
Throughput (batch_size=17) : 0.5989587755016884 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [05:33<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9487000703811646     â”‚
â”‚        pixel_AUPRO        â”‚    0.9330601692199707     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.57it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.57it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.33it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.33it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.40it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.40it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.44it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.44it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.38it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.38it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:20,  2.33it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:20,  2.33it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:03<00:20,  2.30it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:03<00:20,  2.30it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:19,  2.28it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:19,  2.28it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:19,  2.27it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:19,  2.27it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.29it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.29it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:18,  2.30it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:18,  2.30it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.32it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.32it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:17,  2.33it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:17,  2.33it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.34it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.34it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.35it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.35it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.35it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.35it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.36it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.36it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.36it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.36it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.37it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.37it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.38it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.38it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.38it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.38it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.38it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.38it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.39it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.39it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.39it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.39it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.40it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.40it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.40it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.40it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.40it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.40it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.41it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.41it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:09,  2.41it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:09,  2.41it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.41it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.41it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.42it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.42it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.42it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.42it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.42it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.42it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.42it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.42it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.42it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.42it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:07,  2.42it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:07,  2.42it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.43it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.43it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.43it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.43it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.43it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.43it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.43it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.43it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.43it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.43it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.43it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.43it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.43it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.43it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.43it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.43it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.43it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.43it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.44it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.44it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.44it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.44it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.44it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.44it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.44it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.44it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.44it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.44it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.44it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.44it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.44it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.44it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.45it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.45it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:02<00:24,  0.45it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:04<00:22,  0.45it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:06<00:19,  0.46it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:08<00:17,  0.46it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:10<00:15,  0.46it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:13<00:13,  0.46it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:15<00:10,  0.46it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:17<00:08,  0.46it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:19<00:06,  0.46it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:21<00:04,  0.46it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:23<00:02,  0.46it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:26<00:00,  0.46it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:03<00:00,  0.22it/s, pixel_AUPRO=0.859]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:03<00:00,  0.22it/s, pixel_AUPRO=0.859]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:32<00:00,  0.19it/s, pixel_AUPRO=0.859]INFO:anomalib.callbacks.timer:Training took 273.12 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:54,  0.20it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:09<00:48,  0.21it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:14<00:43,  0.21it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:19<00:38,  0.21it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:23<00:33,  0.21it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:28<00:28,  0.21it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:33<00:23,  0.21it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:38<00:19,  0.21it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:42<00:14,  0.21it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:47<00:09,  0.21it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:52<00:04,  0.21it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:56<00:00,  0.21it/s]INFO:anomalib.callbacks.timer:Testing took 258.4109101295471 seconds
Throughput (batch_size=17) : 0.7739611299682957 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [04:17<00:00,  0.05it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8700999617576599     â”‚
â”‚        pixel_AUPRO        â”‚    0.8835461139678955     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:36,  1.46it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:36,  1.46it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:24,  2.15it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:24,  2.15it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:23,  2.20it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:23,  2.20it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:22,  2.22it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:22,  2.22it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:21,  2.23it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:21,  2.23it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:21,  2.25it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:21,  2.25it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:21,  2.23it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:21,  2.23it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:20,  2.24it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:20,  2.24it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:04<00:20,  2.23it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:04<00:20,  2.23it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:19,  2.24it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:19,  2.24it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:19,  2.24it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:19,  2.24it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.24it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.24it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:18,  2.24it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:18,  2.24it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:06<00:17,  2.24it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:06<00:17,  2.24it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:17,  2.25it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:17,  2.24it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:07<00:16,  2.25it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:07<00:16,  2.25it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:16,  2.24it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:16,  2.24it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:08<00:16,  2.25it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:08<00:16,  2.25it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:15,  2.25it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:15,  2.25it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:15,  2.26it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:15,  2.26it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:09<00:14,  2.27it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:09<00:14,  2.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:14,  2.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:14,  2.27it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:10<00:13,  2.28it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:10<00:13,  2.28it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:13,  2.28it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:13,  2.28it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.29it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.29it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:11<00:12,  2.29it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:11<00:12,  2.29it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.30it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.30it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:12<00:11,  2.30it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:12<00:11,  2.30it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.30it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.30it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.31it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.31it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.31it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.31it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.31it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.31it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:14<00:09,  2.31it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:14<00:09,  2.31it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.31it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.31it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:15<00:08,  2.31it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:15<00:08,  2.31it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.31it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.31it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.31it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.31it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:16<00:06,  2.31it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:16<00:06,  2.31it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.31it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.31it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:17<00:06,  2.32it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:17<00:06,  2.32it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.32it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.32it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:18<00:05,  2.32it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:18<00:05,  2.32it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:18<00:04,  2.32it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:18<00:04,  2.32it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.32it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.32it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:19<00:03,  2.32it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:19<00:03,  2.32it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.32it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.32it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:20<00:03,  2.32it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:20<00:03,  2.32it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:20<00:02,  2.32it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:20<00:02,  2.32it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:21<00:02,  2.33it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:21<00:02,  2.33it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:21<00:01,  2.33it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:21<00:01,  2.33it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.33it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.33it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:22<00:00,  2.33it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:22<00:00,  2.33it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:22<00:00,  2.33it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:22<00:00,  2.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.36it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:02<00:28,  0.38it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:05<00:25,  0.39it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:07<00:22,  0.39it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:10<00:20,  0.39it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:12<00:17,  0.39it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:15<00:15,  0.39it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:17<00:12,  0.39it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:20<00:10,  0.39it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:22<00:07,  0.39it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:25<00:05,  0.40it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:27<00:02,  0.40it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:30<00:00,  0.40it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:45<00:00,  0.24it/s, pixel_AUPRO=0.864]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:45<00:00,  0.24it/s, pixel_AUPRO=0.864]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:18<00:00,  0.21it/s, pixel_AUPRO=0.864]INFO:anomalib.callbacks.timer:Training took 259.12 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:05<01:00,  0.18it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:10<00:53,  0.19it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:16<00:50,  0.18it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:22<00:44,  0.18it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:27<00:38,  0.18it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:32<00:32,  0.18it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:38<00:27,  0.18it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:43<00:21,  0.18it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:48<00:16,  0.18it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:54<00:10,  0.18it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:59<00:05,  0.18it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:04<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 375.71717834472656 seconds
Throughput (batch_size=17) : 0.532315293330817 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [06:14<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.947100043296814     â”‚
â”‚        pixel_AUPRO        â”‚    0.9038290977478027     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.54it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.54it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.29it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.29it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.36it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.36it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.39it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.39it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.42it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.42it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.43it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.43it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.44it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.44it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.44it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.44it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.44it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.44it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.44it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.44it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.45it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.45it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.45it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.45it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.45it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.45it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.45it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.45it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.46it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.46it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.46it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.46it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.46it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.46it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.46it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.46it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.46it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.46it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.47it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.47it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.47it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.47it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.47it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.47it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.47it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.47it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.48it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.48it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.48it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.48it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.48it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.48it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.48it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.48it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.48it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.48it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.48it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.48it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.48it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.48it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.48it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.48it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.48it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.48it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.48it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.48it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.48it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.48it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.49it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.49it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.49it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.49it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.49it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.49it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.49it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.49it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.49it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.49it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.49it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.49it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.49it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.49it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.49it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.49it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.49it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.49it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.49it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.49it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.49it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.49it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.49it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.49it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.49it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.49it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.49it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.49it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.49it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.49it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.49it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.49it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.49it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.49it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.49it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.49it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.49it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.49it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:02<00:28,  0.38it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:05<00:25,  0.39it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:07<00:22,  0.39it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:10<00:20,  0.39it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:12<00:17,  0.40it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:15<00:15,  0.40it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:17<00:12,  0.40it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:20<00:10,  0.40it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:22<00:07,  0.40it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:25<00:05,  0.40it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:27<00:02,  0.40it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:30<00:00,  0.40it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:04<00:00,  0.22it/s, pixel_AUPRO=0.893]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:04<00:00,  0.22it/s, pixel_AUPRO=0.893]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:37<00:00,  0.19it/s, pixel_AUPRO=0.893]INFO:anomalib.callbacks.timer:Training took 278.26 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:05<01:02,  0.17it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:10<00:54,  0.18it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:16<00:50,  0.18it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:22<00:44,  0.18it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:27<00:38,  0.18it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:33<00:33,  0.18it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:38<00:27,  0.18it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:43<00:21,  0.18it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:49<00:16,  0.18it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:54<00:10,  0.18it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:00<00:05,  0.18it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:05<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 410.6650240421295 seconds
Throughput (batch_size=17) : 0.48701493502276516 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [06:49<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9286999702453613     â”‚
â”‚        pixel_AUPRO        â”‚    0.9007935523986816     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb3 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.55it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.55it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:21,  2.39it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:21,  2.39it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:20,  2.46it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:20,  2.45it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:20,  2.49it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:20,  2.49it/s]Epoch 0:   9%|â–‰         | 5/54 [00:01<00:19,  2.52it/s]Epoch 0:   9%|â–‰         | 5/54 [00:01<00:19,  2.52it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:18,  2.53it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:18,  2.53it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:18,  2.55it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:18,  2.55it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:18,  2.55it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:18,  2.55it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:17,  2.56it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:17,  2.56it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:03<00:17,  2.57it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:03<00:17,  2.57it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:16,  2.57it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:16,  2.57it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:16,  2.58it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:16,  2.58it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:15,  2.59it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:15,  2.59it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:15,  2.59it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:15,  2.59it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:05<00:15,  2.59it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:05<00:15,  2.59it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:14,  2.60it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:14,  2.60it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:14,  2.60it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:14,  2.60it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:06<00:13,  2.60it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:06<00:13,  2.60it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:13,  2.61it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:13,  2.61it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:07<00:13,  2.61it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:07<00:13,  2.61it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:12,  2.61it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:12,  2.61it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:08<00:12,  2.62it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:08<00:12,  2.62it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:08<00:11,  2.62it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:08<00:11,  2.62it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:11,  2.62it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:11,  2.62it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:09<00:11,  2.63it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:09<00:11,  2.63it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:09<00:10,  2.63it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:09<00:10,  2.63it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:10<00:10,  2.63it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:10<00:10,  2.63it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:10<00:09,  2.63it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:10<00:09,  2.63it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:09,  2.64it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:09,  2.64it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:11<00:09,  2.64it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:11<00:09,  2.64it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:11<00:08,  2.64it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:11<00:08,  2.64it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:12<00:08,  2.64it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:12<00:08,  2.64it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:12<00:07,  2.64it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:12<00:07,  2.64it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:12<00:07,  2.64it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:12<00:07,  2.64it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:13<00:07,  2.64it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:13<00:07,  2.64it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:13<00:06,  2.64it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:13<00:06,  2.64it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:14<00:06,  2.64it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:14<00:06,  2.64it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:14<00:06,  2.64it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:14<00:06,  2.64it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:14<00:05,  2.64it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:14<00:05,  2.64it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:15<00:05,  2.64it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:15<00:05,  2.64it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:15<00:04,  2.64it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:15<00:04,  2.64it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:15<00:04,  2.64it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:15<00:04,  2.64it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:16<00:04,  2.64it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:16<00:04,  2.64it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:16<00:03,  2.64it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:16<00:03,  2.64it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:17<00:03,  2.64it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:17<00:03,  2.64it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:17<00:03,  2.64it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:17<00:03,  2.64it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:17<00:02,  2.65it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:17<00:02,  2.65it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:18<00:02,  2.64it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:18<00:02,  2.64it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:18<00:01,  2.64it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:18<00:01,  2.64it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:18<00:01,  2.65it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:18<00:01,  2.65it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:19<00:01,  2.65it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:19<00:01,  2.65it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:19<00:00,  2.65it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:19<00:00,  2.65it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:20<00:00,  2.65it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:20<00:00,  2.65it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:20<00:00,  2.68it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:20<00:00,  2.68it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:02<00:23,  0.47it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:04<00:20,  0.48it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:06<00:18,  0.48it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:08<00:16,  0.48it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:10<00:14,  0.48it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:12<00:12,  0.48it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:14<00:10,  0.48it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:16<00:08,  0.48it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:18<00:06,  0.48it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:20<00:04,  0.48it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:22<00:02,  0.49it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:24<00:00,  0.49it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:43<00:00,  0.24it/s, pixel_AUPRO=0.877]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:43<00:00,  0.24it/s, pixel_AUPRO=0.877]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:09<00:00,  0.22it/s, pixel_AUPRO=0.877]INFO:anomalib.callbacks.timer:Training took 250.72 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:05<00:55,  0.20it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:09<00:48,  0.20it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:14<00:44,  0.20it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:19<00:39,  0.20it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:24<00:34,  0.21it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:29<00:29,  0.21it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:34<00:24,  0.20it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:39<00:19,  0.20it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:43<00:14,  0.21it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:48<00:09,  0.21it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:53<00:04,  0.21it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:57<00:00,  0.21it/s]INFO:anomalib.callbacks.timer:Testing took 319.9982376098633 seconds
Throughput (batch_size=17) : 0.6281284593981294 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [05:19<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9528713226318359     â”‚
â”‚        pixel_AUPRO        â”‚    0.8877209424972534     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb4 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:35,  1.51it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:35,  1.51it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.21it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.21it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.26it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.26it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.30it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.30it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:21,  2.33it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:21,  2.33it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.35it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.35it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.36it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.36it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.37it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.37it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.38it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.37it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.38it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.38it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.39it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.39it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:17,  2.39it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:17,  2.39it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.39it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.39it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.40it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.40it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.40it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.40it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.40it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.40it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.40it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.40it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.40it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.40it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.40it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.40it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.40it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.40it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.40it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.40it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.41it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.41it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.41it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.41it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:12,  2.42it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:12,  2.42it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:11,  2.42it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:11,  2.42it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.42it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.42it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.42it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.42it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.43it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.43it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:10,  2.43it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:10,  2.43it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.43it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.43it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.43it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.43it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.43it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.43it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.43it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.43it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:08,  2.43it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:08,  2.43it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.43it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.43it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.43it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.43it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:06,  2.43it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:06,  2.43it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.43it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.43it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.43it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.43it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.43it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.43it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.43it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.43it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.43it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.43it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.43it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.43it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.43it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.43it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.43it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.43it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.43it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.43it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.43it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.43it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.44it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.44it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.44it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.44it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.44it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.44it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.44it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.44it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.44it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.44it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.44it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.44it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.47it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.47it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:02<00:30,  0.36it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:05<00:27,  0.37it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:08<00:24,  0.37it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:10<00:21,  0.37it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:13<00:18,  0.37it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:16<00:16,  0.37it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:18<00:13,  0.37it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:21<00:10,  0.37it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:24<00:08,  0.37it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:26<00:05,  0.37it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:29<00:02,  0.37it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:32<00:00,  0.37it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [06:18<00:00,  0.14it/s, pixel_AUPRO=0.846]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [06:18<00:00,  0.14it/s, pixel_AUPRO=0.846]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [06:52<00:00,  0.13it/s, pixel_AUPRO=0.846]INFO:anomalib.callbacks.timer:Training took 413.45 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:05<01:02,  0.18it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:11<00:56,  0.18it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:17<00:52,  0.17it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:23<00:46,  0.17it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:28<00:40,  0.17it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:34<00:34,  0.18it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:40<00:28,  0.17it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:45<00:22,  0.17it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:51<00:17,  0.17it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:56<00:11,  0.18it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:03<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:08<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 626.5855579376221 seconds
Throughput (batch_size=17) : 0.3207861998313245 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [10:25<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9950495362281799     â”‚
â”‚        pixel_AUPRO        â”‚     0.869530439376831     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pipe_fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.48it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.48it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.26it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.26it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.33it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.33it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.37it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.37it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.39it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.39it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.41it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.41it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.42it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.42it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.42it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.42it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.43it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.43it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:06,  2.43it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:06,  2.43it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.44it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.44it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:06,  2.44it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:06,  2.44it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.45it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.45it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.45it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.45it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:04,  2.45it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:04,  2.45it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.45it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.45it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:04,  2.45it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:04,  2.45it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.45it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.45it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.46it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.46it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.46it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.46it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.46it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.46it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:02,  2.47it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:02,  2.47it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.47it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.47it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.47it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.47it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.47it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.47it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.47it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.47it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.52it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.52it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:03<00:24,  0.33it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:05<00:20,  0.33it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:08<00:17,  0.34it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:11<00:14,  0.34it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:14<00:11,  0.34it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:17<00:08,  0.34it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:20<00:05,  0.34it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:23<00:02,  0.34it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:26<00:00,  0.34it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:42<00:00,  0.08it/s, pixel_AUPRO=0.945]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:42<00:00,  0.08it/s, pixel_AUPRO=0.945]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [06:20<00:00,  0.07it/s, pixel_AUPRO=0.945]INFO:anomalib.callbacks.timer:Training took 381.72 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:05<00:47,  0.17it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:11<00:39,  0.18it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:17<00:34,  0.17it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:23<00:28,  0.17it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:28<00:23,  0.17it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:34<00:17,  0.17it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:40<00:11,  0.17it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:45<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:50<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 446.2455060482025 seconds
Throughput (batch_size=17) : 0.33613783885096943 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [07:25<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.989799976348877     â”‚
â”‚        pixel_AUPRO        â”‚     0.946696937084198     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa candle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.53it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.53it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.31it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.31it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.33it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.33it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.35it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.35it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.37it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.37it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.39it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.39it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.40it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.40it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.42it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.42it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.43it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.43it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.43it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.43it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.44it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.44it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.44it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.44it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.44it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.44it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.45it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.45it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.45it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.45it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.44it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.44it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.44it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.44it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.45it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.45it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.45it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.45it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.45it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.45it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.46it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.46it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.46it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.46it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.47it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.47it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.48it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.48it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.48it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.48it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.48it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.48it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.48it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.48it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.49it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.49it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.49it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.49it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.49it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.49it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.49it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.49it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.49it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.49it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.49it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.49it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.49it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.49it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.50it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.50it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.50it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.50it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.50it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.50it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.50it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.50it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.50it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.50it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.50it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.50it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.50it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.50it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.50it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.50it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.50it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.50it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.50it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.50it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.50it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.50it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.51it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.51it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.51it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.51it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.51it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.51it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.51it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.51it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.51it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.51it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.51it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.51it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.52it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.52it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.52it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.52it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:47,  0.23it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:08<00:43,  0.23it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:12<00:38,  0.23it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:17<00:34,  0.23it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:21<00:29,  0.23it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:25<00:25,  0.23it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:29<00:21,  0.23it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:34<00:17,  0.23it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:38<00:12,  0.23it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:42<00:08,  0.23it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:47<00:04,  0.23it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:51<00:00,  0.23it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [07:05<00:00,  0.12it/s, pixel_AUPRO=0.950]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [07:05<00:00,  0.12it/s, pixel_AUPRO=0.950]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [07:45<00:00,  0.11it/s, pixel_AUPRO=0.950]INFO:anomalib.callbacks.timer:Training took 466.29 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:07<01:20,  0.14it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:14<01:11,  0.14it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:22<01:06,  0.14it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:29<00:58,  0.14it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:36<00:51,  0.14it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:44<00:44,  0.14it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:52<00:37,  0.13it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:59<00:29,  0.13it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [01:06<00:22,  0.13it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:14<00:14,  0.13it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:21<00:07,  0.13it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:28<00:00,  0.14it/s]INFO:anomalib.callbacks.timer:Testing took 533.1237165927887 seconds
Throughput (batch_size=17) : 0.3751474447210989 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [08:52<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8744999766349792     â”‚
â”‚        pixel_AUPRO        â”‚    0.9507652521133423     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa capsules with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/32 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/32 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:20,  1.52it/s]Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:20,  1.52it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.25it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.25it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.31it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.31it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:11,  2.34it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:11,  2.34it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:11,  2.37it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:11,  2.37it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:10,  2.38it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:10,  2.38it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:02<00:10,  2.39it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:02<00:10,  2.39it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:09,  2.40it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:09,  2.40it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:03<00:09,  2.41it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:03<00:09,  2.41it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.42it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.42it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:08,  2.42it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:08,  2.42it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:04<00:08,  2.43it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:04<00:08,  2.43it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:07,  2.43it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:07,  2.43it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:05<00:07,  2.43it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:05<00:07,  2.43it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:06,  2.43it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:06,  2.43it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:06<00:06,  2.43it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:06<00:06,  2.43it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:06<00:06,  2.43it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:06<00:06,  2.43it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:07<00:05,  2.44it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:07<00:05,  2.44it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:07<00:05,  2.44it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:07<00:05,  2.44it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:04,  2.44it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:04,  2.44it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:08<00:04,  2.44it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:08<00:04,  2.44it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.44it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.44it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:09<00:03,  2.45it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:09<00:03,  2.45it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:09<00:03,  2.45it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:09<00:03,  2.45it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:10<00:02,  2.45it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:10<00:02,  2.45it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:10<00:02,  2.45it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:10<00:02,  2.45it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.45it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.45it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:11<00:01,  2.45it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:11<00:01,  2.45it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:11<00:01,  2.45it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:11<00:01,  2.45it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:12<00:00,  2.45it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:12<00:00,  2.45it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:12<00:00,  2.45it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:12<00:00,  2.45it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.46it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.46it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|â–ˆ         | 1/10 [00:03<00:28,  0.32it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:06<00:24,  0.32it/s][A
Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:09<00:21,  0.33it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:12<00:18,  0.33it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:15<00:15,  0.33it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:18<00:12,  0.33it/s][A
Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:21<00:09,  0.33it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:24<00:06,  0.33it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:27<00:03,  0.33it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:30<00:00,  0.33it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:31<00:00,  0.35it/s, pixel_AUPRO=nan.0]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:31<00:00,  0.35it/s, pixel_AUPRO=nan.0]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:59<00:00,  0.27it/s, pixel_AUPRO=nan.0]INFO:anomalib.callbacks.timer:Training took 120.25 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|â–ˆ         | 1/10 [00:05<00:53,  0.17it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:11<00:46,  0.17it/s]Testing DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:18<00:42,  0.17it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:24<00:36,  0.17it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:29<00:29,  0.17it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:36<00:24,  0.17it/s]Testing DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:42<00:18,  0.17it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:47<00:11,  0.17it/s]Testing DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:53<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:57<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 170.56908559799194 seconds
Throughput (batch_size=17) : 0.9380363354769818 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:49<00:00,  0.06it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7568333148956299     â”‚
â”‚        pixel_AUPRO        â”‚    0.8400501012802124     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa cashew with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:18,  1.43it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:18,  1.42it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.13it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.13it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:11,  2.16it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:11,  2.16it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.18it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.18it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:10,  2.19it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:10,  2.19it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.20it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.20it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:09,  2.21it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:09,  2.21it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.21it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.21it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:04<00:08,  2.22it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:04<00:08,  2.22it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.22it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.22it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:07,  2.23it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:07,  2.23it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.23it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.23it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.23it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.23it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.24it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.24it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.23it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.23it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:04,  2.23it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:04,  2.23it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.23it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.23it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:08<00:04,  2.24it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:08<00:04,  2.24it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.24it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.24it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.24it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.24it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.24it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.24it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.24it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.24it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.24it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.24it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.24it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.24it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:11<00:00,  2.24it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:11<00:00,  2.24it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.24it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.24it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.29it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.28it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:04<00:35,  0.23it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:08<00:30,  0.23it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:13<00:26,  0.23it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:17<00:21,  0.23it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:21<00:17,  0.23it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:26<00:13,  0.23it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:30<00:08,  0.23it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:35<00:04,  0.23it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:39<00:00,  0.23it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:23<00:00,  0.19it/s, pixel_AUPRO=0.837]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:23<00:00,  0.19it/s, pixel_AUPRO=0.837]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:03<00:00,  0.15it/s, pixel_AUPRO=0.837]INFO:anomalib.callbacks.timer:Training took 184.27 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:07<00:59,  0.13it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:14<00:50,  0.14it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:21<00:43,  0.14it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:28<00:36,  0.14it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:35<00:28,  0.14it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:43<00:21,  0.14it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:50<00:14,  0.14it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:57<00:07,  0.14it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:04<00:00,  0.14it/s]INFO:anomalib.callbacks.timer:Testing took 226.99851036071777 seconds
Throughput (batch_size=17) : 0.66079728788369 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [03:46<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9291999340057373     â”‚
â”‚        pixel_AUPRO        â”‚     0.839982807636261     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa chewinggum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.52it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.52it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.29it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.29it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.34it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.34it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.37it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.36it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.38it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.38it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.38it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.38it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.39it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.39it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.39it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.39it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.38it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.38it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.38it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.38it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.38it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.38it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.38it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.38it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.38it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.38it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.38it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.38it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.38it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.38it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.38it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.38it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.38it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.38it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.38it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.38it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.39it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.39it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.39it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.39it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.40it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.40it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.40it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.40it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.40it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.40it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.41it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.41it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.41it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.41it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.41it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.41it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.44it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.44it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:03<00:31,  0.25it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:07<00:27,  0.25it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:11<00:23,  0.25it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:15<00:19,  0.25it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:19<00:15,  0.26it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:23<00:11,  0.26it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:27<00:07,  0.26it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:31<00:03,  0.26it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:35<00:00,  0.26it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:22<00:00,  0.10it/s, pixel_AUPRO=0.678]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:22<00:00,  0.10it/s, pixel_AUPRO=0.678]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:58<00:00,  0.09it/s, pixel_AUPRO=0.678]INFO:anomalib.callbacks.timer:Training took 299.60 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:07<00:56,  0.14it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:13<00:47,  0.15it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:20<00:41,  0.15it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:27<00:34,  0.15it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:33<00:27,  0.15it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:40<00:20,  0.15it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:47<00:13,  0.15it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:54<00:06,  0.15it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:01<00:00,  0.15it/s]INFO:anomalib.callbacks.timer:Testing took 449.45394492149353 seconds
Throughput (batch_size=17) : 0.33373830999792564 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [07:28<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9712000489234924     â”‚
â”‚        pixel_AUPRO        â”‚    0.6858818531036377     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.52it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.52it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.28it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.28it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.36it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.36it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.40it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.40it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.43it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.43it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.45it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.45it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.39it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.39it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.35it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.35it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.32it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.32it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.30it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:07,  2.28it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:07,  2.28it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.26it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.26it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.25it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.25it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.24it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.24it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.22it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.22it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:04,  2.23it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:04,  2.23it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.24it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.24it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.25it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.25it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.26it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.26it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.27it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.27it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.28it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.28it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.29it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.29it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.30it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.30it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.31it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.31it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.31it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.31it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.32it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.32it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.37it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.37it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:03<00:25,  0.32it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:06<00:21,  0.32it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:09<00:18,  0.33it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:12<00:15,  0.33it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:15<00:12,  0.33it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:18<00:09,  0.33it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:21<00:06,  0.33it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:24<00:03,  0.33it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:27<00:00,  0.33it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:35<00:00,  0.13it/s, pixel_AUPRO=0.860]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:35<00:00,  0.13it/s, pixel_AUPRO=0.860]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:03<00:00,  0.11it/s, pixel_AUPRO=0.860]INFO:anomalib.callbacks.timer:Training took 244.69 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:06<00:48,  0.17it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:11<00:41,  0.17it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:17<00:35,  0.17it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:23<00:29,  0.17it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:29<00:23,  0.17it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:35<00:17,  0.17it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:41<00:11,  0.17it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:47<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:53<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 293.56748843193054 seconds
Throughput (batch_size=17) : 0.5109557628510368 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:52<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9421999454498291     â”‚
â”‚        pixel_AUPRO        â”‚    0.8659604787826538     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.56it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.56it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.38it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.38it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.46it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.46it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:19,  2.48it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:19,  2.48it/s]Epoch 0:   9%|â–‰         | 5/53 [00:01<00:19,  2.50it/s]Epoch 0:   9%|â–‰         | 5/53 [00:01<00:19,  2.50it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:18,  2.52it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:18,  2.51it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.52it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.52it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:17,  2.53it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:17,  2.53it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.53it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.53it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:03<00:16,  2.54it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:03<00:16,  2.54it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.54it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.54it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.54it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.54it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:15,  2.54it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:15,  2.54it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.54it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.54it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:05<00:14,  2.54it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:05<00:14,  2.54it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.54it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.54it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.54it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.54it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:13,  2.54it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:13,  2.54it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.54it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.54it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:07<00:12,  2.54it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:07<00:12,  2.54it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.54it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.54it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.55it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.55it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.55it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.55it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.55it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.55it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:10,  2.55it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:10,  2.55it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.55it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.55it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.54it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.54it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.54it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.54it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.54it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.54it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.53it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.53it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.52it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.52it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.52it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.52it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.51it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.51it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.51it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.51it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.51it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.51it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.50it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.50it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.50it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.50it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.50it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.50it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.50it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.50it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.50it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.50it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.50it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.50it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.50it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.50it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.50it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.50it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.50it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.50it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.49it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.49it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.49it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.49it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.49it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.49it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.49it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.49it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.49it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.49it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.48it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.48it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.48it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.48it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.48it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.48it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.49it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.48it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:34,  0.32it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:06<00:30,  0.32it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:09<00:27,  0.33it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:12<00:24,  0.33it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:15<00:21,  0.33it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:18<00:18,  0.33it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:21<00:15,  0.33it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:24<00:12,  0.33it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:27<00:09,  0.33it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:30<00:06,  0.33it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:33<00:03,  0.33it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:36<00:00,  0.33it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:08<00:00,  0.17it/s, pixel_AUPRO=0.936]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:08<00:00,  0.17it/s, pixel_AUPRO=0.936]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:36<00:00,  0.16it/s, pixel_AUPRO=0.936]INFO:anomalib.callbacks.timer:Training took 337.75 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:06<01:06,  0.17it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:11<00:58,  0.17it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:18<00:54,  0.17it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:23<00:47,  0.17it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:29<00:41,  0.17it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:35<00:35,  0.17it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:41<00:29,  0.17it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:47<00:23,  0.17it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:53<00:17,  0.17it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:59<00:11,  0.17it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:05<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:10<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 428.14425230026245 seconds
Throughput (batch_size=17) : 0.4671322782577908 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [07:07<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9499000310897827     â”‚
â”‚        pixel_AUPRO        â”‚     0.936066210269928     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.55it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.55it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.36it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.36it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.41it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.41it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.41it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.41it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.42it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.42it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.46it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.46it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.46it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.46it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.47it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.47it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.45it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.45it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.46it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.46it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.46it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.46it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.47it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.47it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.48it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.48it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.48it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.48it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.48it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.48it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.47it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.47it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.47it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.47it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.47it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.47it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.47it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.47it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.47it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.47it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.47it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.47it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.48it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.48it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.48it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.48it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.48it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.48it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.49it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.49it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.49it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.49it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.49it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.49it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.49it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.49it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.49it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.49it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.49it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.49it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.49it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.49it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.49it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.49it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.50it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.50it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.50it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.50it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.50it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.50it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.50it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.50it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.50it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.50it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:05,  2.51it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:05,  2.51it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.51it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.51it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.51it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.51it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.51it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.51it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.51it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.51it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.51it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.51it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.51it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.51it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.51it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.51it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.51it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.51it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.51it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.51it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.51it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.51it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.51it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.51it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.52it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.52it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.52it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.52it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.52it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.52it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.52it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.52it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:34,  0.32it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:06<00:30,  0.32it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:09<00:27,  0.33it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:12<00:24,  0.33it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:15<00:21,  0.33it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:18<00:18,  0.33it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:21<00:15,  0.33it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:24<00:12,  0.33it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:27<00:09,  0.33it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:30<00:06,  0.33it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:33<00:03,  0.33it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:36<00:00,  0.33it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:10<00:00,  0.21it/s, pixel_AUPRO=0.884]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:10<00:00,  0.21it/s, pixel_AUPRO=0.884]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:40<00:00,  0.19it/s, pixel_AUPRO=0.884]INFO:anomalib.callbacks.timer:Training took 280.99 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:06<01:06,  0.17it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:11<00:58,  0.17it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:17<00:52,  0.17it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:23<00:46,  0.17it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:28<00:40,  0.17it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:34<00:34,  0.17it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:40<00:28,  0.17it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:45<00:22,  0.17it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:51<00:17,  0.18it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:57<00:11,  0.18it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:02<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:07<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 234.214697599411 seconds
Throughput (batch_size=17) : 0.8539173760225325 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [03:53<00:00,  0.05it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8921999931335449     â”‚
â”‚        pixel_AUPRO        â”‚    0.9057179689407349     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:35,  1.49it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:35,  1.49it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.22it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.22it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.27it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.27it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:22,  2.20it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:22,  2.20it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:22,  2.15it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:22,  2.15it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:22,  2.12it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:22,  2.12it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:22,  2.11it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:22,  2.11it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:21,  2.10it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:21,  2.10it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:04<00:21,  2.13it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:04<00:21,  2.13it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:20,  2.15it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:20,  2.15it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:05<00:19,  2.17it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:05<00:19,  2.17it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:19,  2.19it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:19,  2.19it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:18,  2.20it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:18,  2.20it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:06<00:18,  2.21it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:06<00:18,  2.21it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:17,  2.22it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:17,  2.22it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:07<00:17,  2.22it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:07<00:17,  2.22it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:16,  2.23it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:16,  2.23it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:08<00:16,  2.24it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:08<00:16,  2.24it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:15,  2.25it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:15,  2.25it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:15,  2.26it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:15,  2.26it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:09<00:14,  2.27it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:09<00:14,  2.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:14,  2.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:14,  2.27it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:10<00:13,  2.28it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:10<00:13,  2.28it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:13,  2.29it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:13,  2.29it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.29it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.29it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:11<00:12,  2.29it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:11<00:12,  2.29it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.30it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.30it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:12<00:11,  2.30it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:12<00:11,  2.30it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.30it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.30it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.31it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.31it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.31it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.31it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.32it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.32it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:14<00:09,  2.32it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:14<00:09,  2.32it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.32it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.32it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:15<00:08,  2.33it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:15<00:08,  2.33it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.33it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.33it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.33it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.33it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:16<00:06,  2.33it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:16<00:06,  2.33it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.33it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.33it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:17<00:05,  2.34it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:17<00:05,  2.34it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.34it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.34it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:05,  2.34it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:05,  2.34it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:18<00:04,  2.34it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:18<00:04,  2.34it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.34it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.34it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:19<00:03,  2.35it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:19<00:03,  2.35it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.35it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.35it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:20<00:02,  2.35it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:20<00:02,  2.35it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:20<00:02,  2.35it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:20<00:02,  2.35it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.35it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.35it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:21<00:01,  2.36it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:21<00:01,  2.36it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.36it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.36it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:22<00:00,  2.36it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:22<00:00,  2.36it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:22<00:00,  2.36it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:22<00:00,  2.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.40it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.40it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:39,  0.28it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:07<00:35,  0.28it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:10<00:31,  0.28it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:14<00:28,  0.28it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:17<00:24,  0.28it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:21<00:21,  0.28it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:24<00:17,  0.28it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:28<00:14,  0.28it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:31<00:10,  0.28it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:35<00:07,  0.28it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:38<00:03,  0.28it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:42<00:00,  0.28it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:56<00:00,  0.23it/s, pixel_AUPRO=0.861]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:56<00:00,  0.23it/s, pixel_AUPRO=0.861]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:28<00:00,  0.20it/s, pixel_AUPRO=0.861]INFO:anomalib.callbacks.timer:Training took 269.91 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:06<01:14,  0.15it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:13<01:05,  0.15it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:19<00:58,  0.15it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:25<00:51,  0.15it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:32<00:44,  0.16it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:38<00:38,  0.16it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:45<00:32,  0.16it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:51<00:25,  0.16it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:57<00:19,  0.16it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:03<00:12,  0.16it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:10<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:16<00:00,  0.16it/s]INFO:anomalib.callbacks.timer:Testing took 385.775999546051 seconds
Throughput (batch_size=17) : 0.5184355694375578 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [06:24<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9426000118255615     â”‚
â”‚        pixel_AUPRO        â”‚    0.9031338095664978     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.50it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.50it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:23,  2.21it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:23,  2.20it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:22,  2.27it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:22,  2.27it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:21,  2.29it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:21,  2.29it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.31it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.31it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:20,  2.33it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:20,  2.33it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.33it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.33it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:19,  2.34it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:19,  2.34it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.34it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.34it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.35it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.35it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.35it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.35it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.35it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.35it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.36it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.36it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.36it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.36it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.36it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.36it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.36it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.36it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.36it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.36it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.36it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.36it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.36it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.36it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.36it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.36it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.37it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.37it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.37it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.37it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.37it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.37it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.37it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.37it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.37it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.37it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.38it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.38it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.38it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.38it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.38it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.38it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:10,  2.38it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:10,  2.38it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.39it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.39it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.39it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.39it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.39it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.39it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.39it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.39it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.39it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.39it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.39it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.39it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:15<00:07,  2.40it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:15<00:07,  2.40it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.40it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.40it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.40it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.40it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.40it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.40it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.40it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.40it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:17<00:05,  2.40it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:17<00:05,  2.40it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.40it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.40it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.40it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.40it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.40it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.40it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.40it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.40it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.40it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.40it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.40it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.40it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.40it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.40it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.41it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.41it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.41it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.41it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.41it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.41it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.41it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.41it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:22<00:00,  2.41it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:22<00:00,  2.41it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:39,  0.28it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:07<00:35,  0.28it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:10<00:31,  0.28it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:14<00:28,  0.28it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:17<00:24,  0.28it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:21<00:21,  0.28it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:24<00:17,  0.28it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:28<00:14,  0.28it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:31<00:10,  0.28it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:35<00:07,  0.28it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:38<00:03,  0.28it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:42<00:00,  0.28it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:18<00:00,  0.20it/s, pixel_AUPRO=0.902]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:18<00:00,  0.20it/s, pixel_AUPRO=0.902]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:51<00:00,  0.18it/s, pixel_AUPRO=0.902]INFO:anomalib.callbacks.timer:Training took 292.18 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:06<01:11,  0.15it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:12<01:04,  0.16it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:19<00:58,  0.15it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:25<00:51,  0.15it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:32<00:45,  0.16it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:38<00:38,  0.16it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:45<00:32,  0.16it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:51<00:25,  0.16it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:57<00:19,  0.16it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:03<00:12,  0.16it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:10<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:16<00:00,  0.16it/s]INFO:anomalib.callbacks.timer:Testing took 422.8933091163635 seconds
Throughput (batch_size=17) : 0.4729325238507567 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [07:02<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9302999377250671     â”‚
â”‚        pixel_AUPRO        â”‚     0.909121036529541     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb3 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.52it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.52it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.32it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.32it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.38it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.38it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:20,  2.43it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:20,  2.43it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.44it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.44it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:19,  2.46it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:19,  2.46it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.47it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.47it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:18,  2.43it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:18,  2.43it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.41it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.41it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.42it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.42it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:17,  2.43it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:17,  2.43it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:17,  2.44it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:17,  2.44it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:16,  2.44it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:16,  2.44it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.45it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.45it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:15,  2.45it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:15,  2.45it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.46it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.46it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:15,  2.46it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:15,  2.46it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:14,  2.46it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:14,  2.46it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.47it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.47it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:13,  2.47it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:13,  2.47it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.47it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.47it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:08<00:12,  2.48it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:08<00:12,  2.48it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.48it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.48it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:12,  2.48it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:12,  2.48it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:11,  2.49it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:11,  2.49it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.49it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.49it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:10<00:10,  2.49it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:10<00:10,  2.49it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.49it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.49it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:10,  2.50it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:10,  2.50it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.50it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.50it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.50it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.50it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:12<00:08,  2.50it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:12<00:08,  2.50it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.50it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.50it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:07,  2.51it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:07,  2.51it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:13<00:07,  2.51it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:13<00:07,  2.51it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.51it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.51it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:14<00:06,  2.51it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:14<00:06,  2.51it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.51it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.51it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:15<00:05,  2.51it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:15<00:05,  2.51it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:15<00:05,  2.51it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:15<00:05,  2.51it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.51it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.51it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:16<00:04,  2.51it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:16<00:04,  2.51it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.52it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.51it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:17<00:03,  2.52it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:17<00:03,  2.52it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:17<00:03,  2.52it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:17<00:03,  2.52it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.52it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.52it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:18<00:02,  2.52it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:18<00:02,  2.52it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.52it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.52it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:19<00:01,  2.52it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:19<00:01,  2.52it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:19<00:01,  2.52it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:19<00:01,  2.52it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.52it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.52it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:20<00:00,  2.52it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:20<00:00,  2.52it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.52it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.52it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.56it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.56it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:02<00:32,  0.34it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:05<00:28,  0.35it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:08<00:25,  0.35it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:11<00:22,  0.35it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:14<00:20,  0.35it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:17<00:17,  0.35it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:20<00:14,  0.35it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:22<00:11,  0.35it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:25<00:08,  0.35it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:28<00:05,  0.35it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:31<00:02,  0.35it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:34<00:00,  0.35it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:49<00:00,  0.24it/s, pixel_AUPRO=0.873]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [03:49<00:00,  0.24it/s, pixel_AUPRO=0.873]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:15<00:00,  0.21it/s, pixel_AUPRO=0.873]INFO:anomalib.callbacks.timer:Training took 256.68 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:05<01:03,  0.17it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:11<00:55,  0.18it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:16<00:50,  0.18it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:22<00:44,  0.18it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:28<00:39,  0.18it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:33<00:33,  0.18it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:39<00:28,  0.18it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:44<00:22,  0.18it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:50<00:16,  0.18it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:56<00:11,  0.18it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:01<00:05,  0.18it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:06<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 333.6555154323578 seconds
Throughput (batch_size=17) : 0.6024177353685881 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [05:32<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9587128758430481     â”‚
â”‚        pixel_AUPRO        â”‚     0.885266125202179     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb4 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.53it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.53it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.30it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.30it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.34it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.34it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.35it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.35it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.36it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.36it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.37it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.37it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.37it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.37it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.38it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.38it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.38it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.38it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.38it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.38it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.38it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.38it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:17,  2.38it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:17,  2.38it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.38it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.38it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.38it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.38it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.38it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.38it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.38it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.38it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.38it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.38it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.38it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.38it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.38it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.38it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.38it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.38it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.38it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.38it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.38it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.38it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.38it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.38it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.38it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.38it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.38it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.38it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.38it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.38it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.38it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.38it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.38it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.38it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.38it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.38it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.38it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.38it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.38it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.38it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.39it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.39it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.39it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.39it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.39it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.39it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.39it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.39it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.39it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.39it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.39it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.39it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.39it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.39it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.39it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.39it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.39it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.39it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.39it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.39it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:05,  2.39it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:05,  2.39it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.39it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.39it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.40it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.40it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.40it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.40it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.40it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.40it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.40it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.40it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:20<00:02,  2.40it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:20<00:02,  2.40it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.40it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.40it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.40it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.40it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.40it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.40it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.40it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.40it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:22<00:00,  2.40it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:22<00:00,  2.40it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.44it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.44it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:41,  0.26it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:07<00:37,  0.27it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:11<00:33,  0.27it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:15<00:30,  0.27it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:18<00:26,  0.27it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:22<00:22,  0.27it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:26<00:18,  0.27it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:29<00:14,  0.27it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:33<00:11,  0.27it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:37<00:07,  0.27it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:41<00:03,  0.27it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:44<00:00,  0.27it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [06:29<00:00,  0.14it/s, pixel_AUPRO=0.851]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [06:29<00:00,  0.14it/s, pixel_AUPRO=0.851]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [07:04<00:00,  0.13it/s, pixel_AUPRO=0.851]INFO:anomalib.callbacks.timer:Training took 425.43 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:06<01:14,  0.15it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:13<01:05,  0.15it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:19<00:58,  0.15it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:25<00:51,  0.15it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:32<00:45,  0.15it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:38<00:38,  0.16it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:45<00:32,  0.16it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:51<00:25,  0.16it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:57<00:19,  0.16it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:03<00:12,  0.16it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:10<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:16<00:00,  0.16it/s]INFO:anomalib.callbacks.timer:Testing took 623.68492603302 seconds
Throughput (batch_size=17) : 0.3222781112868493 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [10:22<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9963366389274597     â”‚
â”‚        pixel_AUPRO        â”‚    0.8761773109436035     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pipe_fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.52it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.52it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.27it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.27it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.34it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.34it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.38it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.37it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.39it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.39it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.40it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.40it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.35it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.35it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.31it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.31it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.32it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.32it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.33it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.33it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.34it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.34it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.35it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.35it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.36it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.36it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.36it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.36it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.37it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.37it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.38it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.38it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.38it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.38it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.38it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.38it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.39it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.39it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.39it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.39it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.40it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.40it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.40it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.40it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.41it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.41it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.41it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.41it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.42it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.42it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.42it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.42it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.47it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.47it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:04<00:33,  0.24it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:08<00:29,  0.24it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:12<00:24,  0.24it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:16<00:20,  0.24it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:20<00:16,  0.24it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:24<00:12,  0.24it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:28<00:08,  0.24it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:33<00:04,  0.24it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:37<00:00,  0.24it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:51<00:00,  0.08it/s, pixel_AUPRO=0.945]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:51<00:00,  0.08it/s, pixel_AUPRO=0.945]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [06:28<00:00,  0.07it/s, pixel_AUPRO=0.945]INFO:anomalib.callbacks.timer:Training took 389.50 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:07<00:58,  0.14it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:14<00:49,  0.14it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:21<00:42,  0.14it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:27<00:34,  0.14it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:34<00:27,  0.14it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:41<00:20,  0.14it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:48<00:13,  0.14it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:55<00:06,  0.14it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:01<00:00,  0.15it/s]INFO:anomalib.callbacks.timer:Testing took 452.5300133228302 seconds
Throughput (batch_size=17) : 0.33146972705430605 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [07:31<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.989799976348877     â”‚
â”‚        pixel_AUPRO        â”‚    0.9468327164649963     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa candle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.55it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.55it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.33it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.33it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.39it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.39it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.42it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.42it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.44it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.44it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.45it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.45it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.46it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.46it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.47it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.47it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.48it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.48it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.48it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.48it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.48it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.48it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.49it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.49it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.49it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.49it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.50it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.50it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.50it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.50it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.49it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.49it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.50it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.50it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:13,  2.50it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:13,  2.50it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.51it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.51it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:07<00:13,  2.51it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:07<00:13,  2.51it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.52it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.52it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.52it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.52it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.52it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.52it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.52it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.52it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:11,  2.53it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:11,  2.53it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.53it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.53it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.53it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.53it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.53it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.53it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.52it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.52it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.52it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.52it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.52it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.52it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.52it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.52it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.52it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.52it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.52it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.52it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.52it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.52it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.52it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.52it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.52it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.52it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:05,  2.52it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:05,  2.52it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.52it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.52it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.52it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.52it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.52it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.52it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.52it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.52it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.52it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.52it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.52it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.52it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.51it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.51it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.51it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.51it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.51it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.51it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.51it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.51it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.51it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.51it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.51it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.51it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.51it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.51it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.52it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.52it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.52it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.52it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:05<01:03,  0.17it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:11<00:57,  0.17it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:17<00:51,  0.18it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:22<00:45,  0.18it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:28<00:39,  0.18it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:34<00:34,  0.18it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:39<00:28,  0.18it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:45<00:22,  0.18it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:51<00:17,  0.18it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:57<00:11,  0.18it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:02<00:05,  0.18it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:08<00:00,  0.18it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [07:28<00:00,  0.12it/s, pixel_AUPRO=0.949]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [07:28<00:00,  0.12it/s, pixel_AUPRO=0.949]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [08:07<00:00,  0.11it/s, pixel_AUPRO=0.949]INFO:anomalib.callbacks.timer:Training took 488.36 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:08<01:38,  0.11it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:17<01:27,  0.11it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:26<01:19,  0.11it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:34<01:09,  0.11it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:43<01:00,  0.12it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:51<00:51,  0.12it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [01:00<00:43,  0.11it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [01:09<00:34,  0.12it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [01:17<00:25,  0.12it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:26<00:17,  0.12it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:35<00:08,  0.12it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:43<00:00,  0.12it/s]INFO:anomalib.callbacks.timer:Testing took 551.602205991745 seconds
Throughput (batch_size=17) : 0.3625801308035253 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [09:10<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8878499269485474     â”‚
â”‚        pixel_AUPRO        â”‚    0.9492781162261963     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa capsules with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/32 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/32 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:20,  1.50it/s]Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:20,  1.50it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.21it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.21it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.26it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.26it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:12,  2.28it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:12,  2.28it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:12,  2.24it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:12,  2.24it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:11,  2.21it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:11,  2.21it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:03<00:11,  2.19it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:03<00:11,  2.19it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:11,  2.18it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:11,  2.18it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:04<00:10,  2.19it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:04<00:10,  2.19it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.20it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.20it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:09,  2.21it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:09,  2.21it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:05<00:09,  2.22it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:05<00:09,  2.22it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:08,  2.23it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:08,  2.23it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:06<00:08,  2.23it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:06<00:08,  2.23it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:07,  2.24it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:07,  2.24it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:07<00:07,  2.24it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:07<00:07,  2.24it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:07<00:06,  2.25it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:07<00:06,  2.25it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:08<00:06,  2.25it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:08<00:06,  2.25it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:08<00:05,  2.25it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:08<00:05,  2.25it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:05,  2.26it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:05,  2.26it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:09<00:04,  2.26it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:09<00:04,  2.26it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.26it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.26it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:10<00:03,  2.26it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:10<00:03,  2.26it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:10<00:03,  2.26it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:10<00:03,  2.26it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:11<00:03,  2.27it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:11<00:03,  2.27it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:11<00:02,  2.27it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:11<00:02,  2.27it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.28it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.28it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:12<00:01,  2.28it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:12<00:01,  2.28it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:12<00:01,  2.28it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:12<00:01,  2.28it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:13<00:00,  2.28it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:13<00:00,  2.28it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:13<00:00,  2.28it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:13<00:00,  2.28it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.29it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.29it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|â–ˆ         | 1/10 [00:04<00:37,  0.24it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:08<00:32,  0.24it/s][A
Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:12<00:28,  0.25it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:16<00:24,  0.25it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:20<00:20,  0.25it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:24<00:16,  0.25it/s][A
Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:28<00:12,  0.25it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:32<00:08,  0.25it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:36<00:04,  0.25it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:40<00:00,  0.25it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:41<00:00,  0.31it/s, pixel_AUPRO=nan.0]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:41<00:00,  0.31it/s, pixel_AUPRO=nan.0]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [02:10<00:00,  0.25it/s, pixel_AUPRO=nan.0]INFO:anomalib.callbacks.timer:Training took 130.99 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|â–ˆ         | 1/10 [00:08<01:15,  0.12it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:15<01:00,  0.13it/s]Testing DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:21<00:51,  0.14it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:28<00:42,  0.14it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:35<00:35,  0.14it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:41<00:27,  0.14it/s]Testing DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:48<00:20,  0.14it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:55<00:13,  0.15it/s]Testing DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [01:01<00:06,  0.15it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:06<00:00,  0.15it/s]INFO:anomalib.callbacks.timer:Testing took 180.09700322151184 seconds
Throughput (batch_size=17) : 0.8884101186470418 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:59<00:00,  0.06it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7418333292007446     â”‚
â”‚        pixel_AUPRO        â”‚    0.8461707830429077     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa cashew with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.46it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.46it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.14it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.14it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.19it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.19it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.22it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.22it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.23it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.23it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.25it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.25it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.26it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.26it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.26it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.26it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.27it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.27it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.27it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:07,  2.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:07,  2.27it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.28it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.28it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.28it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:06,  2.28it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.28it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:05,  2.28it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.28it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.28it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:04,  2.28it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:04,  2.28it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.29it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.29it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.29it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.29it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.29it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.29it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.29it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:03,  2.29it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.29it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.29it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.29it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.29it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.30it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.30it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.30it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.30it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.30it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.30it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.30it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:11<00:00,  2.30it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.34it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:05<00:47,  0.17it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:11<00:41,  0.17it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:17<00:35,  0.17it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:23<00:29,  0.17it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:29<00:23,  0.17it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:35<00:17,  0.17it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:40<00:11,  0.17it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:46<00:05,  0.17it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:52<00:00,  0.17it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:36<00:00,  0.17it/s, pixel_AUPRO=0.836]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:36<00:00,  0.17it/s, pixel_AUPRO=0.836]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:15<00:00,  0.14it/s, pixel_AUPRO=0.836]INFO:anomalib.callbacks.timer:Training took 196.82 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:08<01:11,  0.11it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:17<01:01,  0.11it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:26<00:52,  0.11it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:34<00:43,  0.11it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:43<00:34,  0.11it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:52<00:26,  0.12it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [01:00<00:17,  0.11it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [01:09<00:08,  0.12it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:17<00:00,  0.12it/s]INFO:anomalib.callbacks.timer:Testing took 238.526864528656 seconds
Throughput (batch_size=17) : 0.6288599831151488 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [03:57<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9277999401092529     â”‚
â”‚        pixel_AUPRO        â”‚    0.8388766646385193     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa chewinggum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.51it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.51it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.27it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.27it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.35it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.35it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.38it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.38it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.40it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.40it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.39it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.39it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.37it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.37it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.35it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.35it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.32it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.32it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.33it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.33it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.33it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.33it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.34it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:06,  2.34it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.35it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.35it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.36it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.36it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.36it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:05,  2.36it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.36it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.36it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.36it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:07<00:04,  2.36it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.37it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.37it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.37it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:08<00:03,  2.37it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.37it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.37it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.37it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.37it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.38it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:09<00:02,  2.38it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.38it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.38it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.38it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:10<00:01,  2.38it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.38it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.38it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.39it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.39it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.42it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.42it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:05<00:42,  0.19it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:10<00:36,  0.19it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:15<00:31,  0.19it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:20<00:26,  0.19it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:26<00:20,  0.19it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:31<00:15,  0.19it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:36<00:10,  0.19it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:41<00:05,  0.19it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:46<00:00,  0.19it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:29<00:00,  0.10it/s, pixel_AUPRO=0.678]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:29<00:00,  0.10it/s, pixel_AUPRO=0.678]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:06<00:00,  0.09it/s, pixel_AUPRO=0.678]INFO:anomalib.callbacks.timer:Training took 307.12 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:08<01:06,  0.12it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:16<00:56,  0.12it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:25<00:50,  0.12it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:33<00:41,  0.12it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:41<00:32,  0.12it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:49<00:24,  0.12it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:57<00:16,  0.12it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [01:04<00:08,  0.12it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:12<00:00,  0.12it/s]INFO:anomalib.callbacks.timer:Testing took 459.01796793937683 seconds
Throughput (batch_size=17) : 0.32678459336435106 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [07:38<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9722000360488892     â”‚
â”‚        pixel_AUPRO        â”‚    0.6865776181221008     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.56it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.56it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.33it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.33it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:09,  2.42it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:09,  2.42it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.46it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.46it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:08,  2.48it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:08,  2.48it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.50it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.50it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:07,  2.51it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:07,  2.51it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.47it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.47it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.41it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.41it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.40it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.40it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.41it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.40it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:06,  2.41it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:06,  2.41it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.42it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.42it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.42it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.42it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:04,  2.43it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:04,  2.43it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.43it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.43it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:04,  2.43it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:04,  2.43it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.43it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.43it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.43it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.43it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.43it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.43it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.44it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.44it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:02,  2.44it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:02,  2.44it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.45it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.45it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.45it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.45it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.45it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.45it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.45it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.45it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.50it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.50it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:04<00:32,  0.24it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:08<00:28,  0.25it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:12<00:24,  0.25it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:16<00:20,  0.25it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:20<00:16,  0.25it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:24<00:12,  0.25it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:28<00:08,  0.25it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:32<00:04,  0.25it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:36<00:00,  0.25it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:43<00:00,  0.12it/s, pixel_AUPRO=0.865]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:43<00:00,  0.12it/s, pixel_AUPRO=0.865]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:12<00:00,  0.11it/s, pixel_AUPRO=0.865]INFO:anomalib.callbacks.timer:Training took 253.23 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:07<00:56,  0.14it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:13<00:48,  0.14it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:20<00:41,  0.14it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:27<00:34,  0.14it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:34<00:27,  0.14it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:41<00:20,  0.15it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:48<00:13,  0.14it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:55<00:06,  0.15it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:01<00:00,  0.15it/s]INFO:anomalib.callbacks.timer:Testing took 302.01586747169495 seconds
Throughput (batch_size=17) : 0.4966626464222383 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [05:01<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9393999576568604     â”‚
â”‚        pixel_AUPRO        â”‚    0.8692646622657776     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.55it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.55it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:23,  2.15it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:23,  2.15it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:23,  2.11it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:23,  2.11it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:23,  2.10it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:23,  2.10it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:22,  2.09it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:22,  2.09it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:22,  2.09it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:22,  2.09it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:03<00:21,  2.10it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:03<00:21,  2.10it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:21,  2.12it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:21,  2.12it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:04<00:20,  2.17it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:04<00:20,  2.17it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:19,  2.20it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:19,  2.20it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:18,  2.23it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:18,  2.23it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:18,  2.26it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:18,  2.26it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:17,  2.28it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:17,  2.28it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:06<00:16,  2.30it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:06<00:16,  2.30it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.31it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.31it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.33it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.33it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.34it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.34it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.35it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.35it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.36it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.36it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.38it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.38it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.39it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.39it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:12,  2.40it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:12,  2.40it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.40it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.40it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:12,  2.41it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:12,  2.41it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.42it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.42it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.43it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.43it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.43it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.43it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.44it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.44it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.45it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.45it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.45it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.45it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.45it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.45it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.46it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.46it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.46it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.46it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.47it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.47it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.47it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.47it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.47it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.47it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.48it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.48it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.48it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.48it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.48it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.48it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.48it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.48it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.48it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.48it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.49it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.49it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.49it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.49it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.49it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.49it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.50it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.50it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.50it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.50it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.50it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.50it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.51it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.51it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.51it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.51it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.51it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.51it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.51it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.51it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.51it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.51it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.52it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.52it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:45,  0.24it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:08<00:40,  0.25it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:12<00:36,  0.25it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:16<00:32,  0.25it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:20<00:28,  0.25it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:24<00:24,  0.25it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:28<00:20,  0.25it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:32<00:16,  0.25it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:36<00:12,  0.25it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:40<00:08,  0.25it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:44<00:04,  0.25it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:48<00:00,  0.25it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:26<00:00,  0.16it/s, pixel_AUPRO=0.937]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:26<00:00,  0.16it/s, pixel_AUPRO=0.937]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:55<00:00,  0.15it/s, pixel_AUPRO=0.937]INFO:anomalib.callbacks.timer:Training took 355.87 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:07<01:18,  0.14it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:14<01:10,  0.14it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:21<01:03,  0.14it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:27<00:55,  0.14it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:34<00:48,  0.14it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:41<00:41,  0.14it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:48<00:34,  0.14it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:55<00:27,  0.15it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [01:01<00:20,  0.15it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:08<00:13,  0.15it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:15<00:06,  0.15it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:21<00:00,  0.15it/s]INFO:anomalib.callbacks.timer:Testing took 420.5183675289154 seconds
Throughput (batch_size=17) : 0.47560348237642136 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [06:59<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9532999992370605     â”‚
â”‚        pixel_AUPRO        â”‚    0.9368420839309692     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.52it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.52it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.26it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.26it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.31it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.31it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.34it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.33it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.35it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.35it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.37it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.37it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.37it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.37it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.38it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.38it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.34it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.34it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.35it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.35it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.35it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.35it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.36it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.36it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.36it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.36it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.36it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.36it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.36it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.36it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.36it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.36it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.36it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.36it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.37it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.37it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.37it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.37it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.38it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.38it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.38it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.38it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.38it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.38it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.38it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.38it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.39it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.39it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.39it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.39it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.39it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.39it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.40it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.40it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.40it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.40it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:09,  2.40it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:09,  2.40it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.40it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.40it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.40it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:09,  2.40it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.41it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.41it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.41it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.41it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.41it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.41it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.41it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.41it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:07,  2.41it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:07,  2.41it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.41it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.41it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.41it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.41it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.41it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.41it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.42it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.42it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.42it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.42it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.42it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.42it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.42it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.42it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.42it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.42it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.42it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.42it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.42it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.42it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.42it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.42it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.42it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.42it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.42it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.42it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.42it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.42it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.42it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.42it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.42it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.42it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.43it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.43it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:45,  0.24it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:08<00:40,  0.24it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:12<00:36,  0.24it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:16<00:32,  0.25it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:20<00:28,  0.25it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:24<00:24,  0.25it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:28<00:20,  0.25it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:32<00:16,  0.25it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:36<00:12,  0.25it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:40<00:08,  0.25it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:44<00:04,  0.25it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:48<00:00,  0.25it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:29<00:00,  0.20it/s, pixel_AUPRO=0.889]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:29<00:00,  0.20it/s, pixel_AUPRO=0.889]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:57<00:00,  0.18it/s, pixel_AUPRO=0.889]INFO:anomalib.callbacks.timer:Training took 298.75 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:07<01:17,  0.14it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:13<01:07,  0.15it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:20<01:00,  0.15it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:26<00:53,  0.15it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:33<00:46,  0.15it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:39<00:39,  0.15it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:47<00:33,  0.15it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:54<00:27,  0.15it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [01:00<00:20,  0.15it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:07<00:13,  0.15it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:14<00:06,  0.15it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:20<00:00,  0.15it/s]INFO:anomalib.callbacks.timer:Testing took 326.2859079837799 seconds
Throughput (batch_size=17) : 0.6129593559092422 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [05:25<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8814000487327576     â”‚
â”‚        pixel_AUPRO        â”‚    0.9078359603881836     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.53it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.53it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.29it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:22,  2.28it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.35it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:21,  2.35it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:20,  2.39it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:20,  2.39it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.41it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.41it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.36it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.36it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:20,  2.31it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:20,  2.31it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:20,  2.26it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:20,  2.26it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:19,  2.28it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:19,  2.28it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:19,  2.30it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:19,  2.30it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.31it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.31it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.32it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.32it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.33it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.33it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:17,  2.34it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:17,  2.34it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.34it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.34it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.34it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.34it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.35it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.35it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.35it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.35it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:14,  2.36it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:14,  2.36it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.36it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.36it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.36it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.36it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.37it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.37it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.37it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.37it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.37it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.37it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.37it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.38it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.37it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.38it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.38it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.38it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.38it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.38it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.38it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.38it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.38it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.38it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.38it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.38it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.38it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.38it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.38it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.39it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.39it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.39it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.39it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.39it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.39it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.39it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.39it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.39it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.39it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.39it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.39it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.40it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.40it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.40it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.40it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:05,  2.40it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:05,  2.40it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.40it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.40it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.40it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.40it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.40it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.40it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.40it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.40it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.41it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.40it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.41it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.41it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.41it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.41it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.41it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.41it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.41it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.41it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.41it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.41it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.41it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.41it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.45it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.45it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:52,  0.21it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:09<00:47,  0.21it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:14<00:42,  0.21it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:18<00:37,  0.21it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:23<00:32,  0.21it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:28<00:28,  0.21it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:32<00:23,  0.21it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:37<00:18,  0.21it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:42<00:14,  0.21it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:46<00:09,  0.21it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:51<00:04,  0.21it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:56<00:00,  0.21it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:12<00:00,  0.21it/s, pixel_AUPRO=0.855]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:12<00:00,  0.21it/s, pixel_AUPRO=0.855]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:44<00:00,  0.19it/s, pixel_AUPRO=0.855]INFO:anomalib.callbacks.timer:Training took 285.38 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:07<01:25,  0.13it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:15<01:15,  0.13it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:22<01:07,  0.13it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:29<00:59,  0.13it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:37<00:51,  0.13it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:44<00:44,  0.14it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:51<00:36,  0.14it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:58<00:29,  0.14it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [01:06<00:22,  0.14it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:13<00:14,  0.14it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:21<00:07,  0.14it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:27<00:00,  0.14it/s]INFO:anomalib.callbacks.timer:Testing took 396.7127628326416 seconds
Throughput (batch_size=17) : 0.5041430947972112 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [06:35<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9460999965667725     â”‚
â”‚        pixel_AUPRO        â”‚    0.8986272215843201     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.53it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:34,  1.52it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.30it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.30it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.35it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.35it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.37it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.37it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.39it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.39it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.40it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.40it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.41it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.41it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.42it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.42it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.42it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.42it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.43it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.43it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.43it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.43it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.44it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.44it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.44it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.44it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.44it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.44it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.45it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.45it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.44it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.44it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.45it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.45it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.45it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.45it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.45it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.45it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.46it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.46it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.46it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.46it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.47it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.47it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.47it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.47it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.47it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.47it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.47it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.47it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.47it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.47it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.48it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.48it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.48it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.48it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.48it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.48it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.48it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.48it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.48it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.48it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.48it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.48it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.48it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.48it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.48it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.48it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.48it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.48it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.48it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.48it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.49it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.49it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.49it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.49it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.49it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.49it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.49it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.49it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.49it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.49it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.49it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.49it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.49it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.49it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.49it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.49it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.49it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.49it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.49it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.49it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.49it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.49it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.49it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:02,  2.49it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.49it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.49it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.49it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.49it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.49it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.49it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.50it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.50it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.50it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.50it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:52,  0.21it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:09<00:47,  0.21it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:14<00:42,  0.21it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:18<00:37,  0.21it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:23<00:32,  0.21it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:28<00:28,  0.21it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:32<00:23,  0.21it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:37<00:18,  0.21it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:42<00:14,  0.21it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:46<00:09,  0.21it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:51<00:04,  0.21it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:56<00:00,  0.21it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:33<00:00,  0.19it/s, pixel_AUPRO=0.900]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:33<00:00,  0.19it/s, pixel_AUPRO=0.900]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:05<00:00,  0.17it/s, pixel_AUPRO=0.900]INFO:anomalib.callbacks.timer:Training took 306.17 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:07<01:24,  0.13it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:15<01:15,  0.13it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:22<01:08,  0.13it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:30<01:00,  0.13it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:37<00:52,  0.13it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:45<00:45,  0.13it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:53<00:38,  0.13it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [01:00<00:30,  0.13it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [01:08<00:22,  0.13it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:15<00:15,  0.13it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:23<00:07,  0.13it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:30<00:00,  0.13it/s]INFO:anomalib.callbacks.timer:Testing took 440.4978892803192 seconds
Throughput (batch_size=17) : 0.4540316874769999 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [07:19<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9330999851226807     â”‚
â”‚        pixel_AUPRO        â”‚    0.9078558087348938     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb3 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:33,  1.57it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:33,  1.57it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:21,  2.41it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:21,  2.41it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:20,  2.48it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:20,  2.48it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:19,  2.51it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:19,  2.51it/s]Epoch 0:   9%|â–‰         | 5/54 [00:01<00:19,  2.54it/s]Epoch 0:   9%|â–‰         | 5/54 [00:01<00:19,  2.54it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:18,  2.56it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:18,  2.55it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:18,  2.56it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:18,  2.56it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:17,  2.57it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:17,  2.57it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:17,  2.58it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:17,  2.58it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:03<00:17,  2.59it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:03<00:17,  2.59it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:16,  2.59it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:16,  2.59it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:16,  2.60it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:16,  2.59it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:15,  2.60it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:15,  2.60it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:15,  2.60it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:15,  2.60it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:05<00:14,  2.60it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:05<00:14,  2.60it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:14,  2.60it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:14,  2.60it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:14,  2.61it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:14,  2.61it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:06<00:13,  2.61it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:06<00:13,  2.60it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:13,  2.61it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:13,  2.61it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:07<00:13,  2.61it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:07<00:13,  2.61it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:12,  2.61it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:12,  2.61it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:08<00:12,  2.61it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:08<00:12,  2.61it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:08<00:11,  2.61it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:08<00:11,  2.61it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:11,  2.62it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:11,  2.62it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:09<00:11,  2.62it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:09<00:11,  2.62it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:09<00:10,  2.62it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:09<00:10,  2.62it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:10<00:10,  2.62it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:10<00:10,  2.62it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:10<00:09,  2.62it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:10<00:09,  2.62it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:09,  2.62it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:09,  2.62it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:11<00:09,  2.62it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:11<00:09,  2.62it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:11<00:08,  2.62it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:11<00:08,  2.62it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:12<00:08,  2.62it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:12<00:08,  2.62it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:12<00:08,  2.62it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:12<00:08,  2.62it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:12<00:07,  2.62it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:12<00:07,  2.62it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:13<00:07,  2.62it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:13<00:07,  2.62it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:13<00:06,  2.62it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:13<00:06,  2.62it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:14<00:06,  2.62it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:14<00:06,  2.62it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:14<00:06,  2.63it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:14<00:06,  2.63it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:14<00:05,  2.63it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:14<00:05,  2.63it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:15<00:05,  2.63it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:15<00:05,  2.63it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:15<00:04,  2.63it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:15<00:04,  2.63it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:15<00:04,  2.63it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:15<00:04,  2.63it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:16<00:04,  2.63it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:16<00:04,  2.63it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:16<00:03,  2.63it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:16<00:03,  2.63it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:17<00:03,  2.63it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:17<00:03,  2.63it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:17<00:03,  2.63it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:17<00:03,  2.63it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:17<00:02,  2.63it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:17<00:02,  2.63it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:18<00:02,  2.63it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:18<00:02,  2.63it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:18<00:01,  2.63it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:18<00:01,  2.63it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:19<00:01,  2.63it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:19<00:01,  2.63it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:19<00:01,  2.63it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:19<00:01,  2.63it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:19<00:00,  2.63it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:19<00:00,  2.63it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:20<00:00,  2.63it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:20<00:00,  2.63it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:20<00:00,  2.67it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:20<00:00,  2.67it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:03<00:42,  0.26it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:07<00:38,  0.26it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:11<00:34,  0.26it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:15<00:30,  0.26it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:18<00:26,  0.26it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:22<00:22,  0.26it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:26<00:18,  0.26it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:30<00:15,  0.26it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:34<00:11,  0.26it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:37<00:07,  0.26it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:41<00:03,  0.26it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:45<00:00,  0.26it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:04<00:00,  0.22it/s, pixel_AUPRO=0.872]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:04<00:00,  0.22it/s, pixel_AUPRO=0.872]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:31<00:00,  0.20it/s, pixel_AUPRO=0.872]INFO:anomalib.callbacks.timer:Training took 272.05 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:06<01:16,  0.14it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:13<01:05,  0.15it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:19<00:59,  0.15it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:25<00:51,  0.15it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:32<00:44,  0.16it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:38<00:38,  0.16it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:44<00:32,  0.16it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:50<00:25,  0.16it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:57<00:19,  0.16it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:03<00:12,  0.16it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:09<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:15<00:00,  0.16it/s]INFO:anomalib.callbacks.timer:Testing took 337.2402226924896 seconds
Throughput (batch_size=17) : 0.5960143140555347 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [05:36<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9511881470680237     â”‚
â”‚        pixel_AUPRO        â”‚    0.8832906484603882     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb4 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.52it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.52it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.23it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.23it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.30it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.30it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.32it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.32it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.35it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.35it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.36it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.36it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.38it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.38it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.39it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.39it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.40it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.40it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.41it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.41it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:17,  2.42it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:17,  2.42it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:17,  2.42it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:17,  2.42it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:16,  2.43it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:16,  2.43it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.43it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:16,  2.43it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.44it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.44it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.43it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:15,  2.43it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:15,  2.43it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:15,  2.43it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:14,  2.43it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:14,  2.43it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.43it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:07<00:14,  2.43it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:13,  2.44it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:13,  2.44it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.44it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.44it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.44it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.44it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.44it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:12,  2.44it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:12,  2.45it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:09<00:12,  2.45it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:11,  2.45it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:11,  2.45it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.45it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.45it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.45it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.45it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.46it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.46it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:10,  2.46it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:11<00:10,  2.46it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.46it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.46it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.46it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.46it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:08,  2.46it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:08,  2.46it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.46it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.46it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:08,  2.45it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:08,  2.45it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.46it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.46it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.46it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.46it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:06,  2.46it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:06,  2.46it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.46it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.46it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:15<00:06,  2.46it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:15<00:06,  2.46it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.46it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.46it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.46it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.46it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.46it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.46it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.46it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.46it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:17<00:04,  2.46it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:17<00:04,  2.46it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.46it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.46it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.46it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.46it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.46it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.46it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.46it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.46it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:19<00:02,  2.46it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:19<00:02,  2.46it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.46it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.46it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.46it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.46it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.46it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:21<00:00,  2.46it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.47it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.47it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.50it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.50it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:05<00:55,  0.20it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:10<00:50,  0.20it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:15<00:45,  0.20it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:20<00:40,  0.20it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:25<00:35,  0.20it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:30<00:30,  0.20it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:35<00:25,  0.20it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:40<00:20,  0.20it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:45<00:15,  0.20it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:50<00:10,  0.20it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:55<00:05,  0.20it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:00<00:00,  0.20it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [06:39<00:00,  0.14it/s, pixel_AUPRO=0.845]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [06:39<00:00,  0.14it/s, pixel_AUPRO=0.845]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [07:11<00:00,  0.13it/s, pixel_AUPRO=0.845]INFO:anomalib.callbacks.timer:Training took 432.20 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:08<01:29,  0.12it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:16<01:20,  0.12it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:24<01:13,  0.12it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:32<01:04,  0.12it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:40<00:56,  0.12it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:47<00:47,  0.13it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:56<00:40,  0.12it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [01:03<00:31,  0.13it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [01:11<00:23,  0.13it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:19<00:15,  0.13it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:27<00:07,  0.13it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:34<00:00,  0.13it/s]INFO:anomalib.callbacks.timer:Testing took 651.1522047519684 seconds
Throughput (batch_size=17) : 0.3086835896939999 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [10:50<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.994257390499115     â”‚
â”‚        pixel_AUPRO        â”‚    0.8693385124206543     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pipe_fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.54it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.54it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.34it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.34it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:09,  2.41it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:09,  2.41it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.46it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.46it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:08,  2.48it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:08,  2.48it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.50it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.50it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:07,  2.51it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:07,  2.51it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.52it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.52it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.53it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.53it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:03<00:06,  2.54it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:03<00:06,  2.54it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.54it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.54it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.55it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.55it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.55it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.55it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.56it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.56it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.56it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.56it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.56it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.56it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.55it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.55it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.56it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.56it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.56it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.56it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.56it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.56it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.56it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.56it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.57it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.57it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:08<00:01,  2.57it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:08<00:01,  2.57it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.57it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.57it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.57it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.57it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.57it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.57it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.62it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.62it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:05<00:44,  0.18it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:11<00:38,  0.18it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:16<00:33,  0.18it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:22<00:27,  0.18it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:27<00:22,  0.18it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:33<00:16,  0.18it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:38<00:11,  0.18it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:44<00:05,  0.18it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:49<00:00,  0.18it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [06:01<00:00,  0.07it/s, pixel_AUPRO=0.946]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [06:01<00:00,  0.07it/s, pixel_AUPRO=0.946]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [06:39<00:00,  0.07it/s, pixel_AUPRO=0.946]INFO:anomalib.callbacks.timer:Training took 400.37 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:08<01:08,  0.12it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:16<00:59,  0.12it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:25<00:50,  0.12it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:33<00:41,  0.12it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:41<00:33,  0.12it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:49<00:24,  0.12it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:58<00:16,  0.12it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [01:06<00:08,  0.12it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:14<00:00,  0.12it/s]INFO:anomalib.callbacks.timer:Testing took 475.62175393104553 seconds
Throughput (batch_size=17) : 0.3153766596255112 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [07:54<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9911999702453613     â”‚
â”‚        pixel_AUPRO        â”‚    0.9475376605987549     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa candle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.57it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.57it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.35it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.35it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.40it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.40it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.43it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.43it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.46it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.46it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.47it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.47it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.48it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.48it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.49it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.49it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.49it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.49it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.49it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.49it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.49it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.49it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.50it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.50it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:15,  2.51it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:15,  2.51it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.51it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.51it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:05<00:15,  2.51it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:05<00:15,  2.51it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.51it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.51it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.51it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.51it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:13,  2.51it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:13,  2.51it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.51it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.51it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:07<00:13,  2.51it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:07<00:13,  2.51it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.51it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.51it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.51it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.51it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.51it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.51it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.51it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.51it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:11,  2.51it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:11,  2.51it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.52it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.52it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.52it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.52it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.52it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.52it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.52it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.52it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.52it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.52it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.52it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.52it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.52it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.52it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.52it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.52it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.52it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.52it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.53it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.53it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.53it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.53it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.53it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.53it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:05,  2.53it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:05,  2.53it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.53it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.53it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.52it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.52it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.52it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.52it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.51it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.51it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.51it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.51it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.51it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.51it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.51it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.51it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.51it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.51it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.51it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.51it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.51it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.51it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.51it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.51it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.51it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.51it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.51it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.51it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.51it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.51it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.51it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.51it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:07<01:21,  0.13it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:14<01:13,  0.14it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:22<01:06,  0.14it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:29<00:59,  0.14it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:36<00:51,  0.14it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:44<00:44,  0.14it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:51<00:36,  0.14it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:59<00:29,  0.14it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [01:06<00:22,  0.14it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:14<00:14,  0.14it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:21<00:07,  0.13it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:28<00:00,  0.13it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [07:48<00:00,  0.11it/s, pixel_AUPRO=0.951]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [07:48<00:00,  0.11it/s, pixel_AUPRO=0.951]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [08:27<00:00,  0.10it/s, pixel_AUPRO=0.951]INFO:anomalib.callbacks.timer:Training took 508.63 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:10<01:56,  0.09it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:20<01:44,  0.10it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:31<01:35,  0.09it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:41<01:23,  0.10it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:52<01:13,  0.10it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [01:02<01:02,  0.10it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [01:13<00:52,  0.10it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [01:23<00:41,  0.10it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [01:33<00:31,  0.10it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:43<00:20,  0.10it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:54<00:10,  0.10it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [02:03<00:00,  0.10it/s]INFO:anomalib.callbacks.timer:Testing took 570.8749656677246 seconds
Throughput (batch_size=17) : 0.3503394123546296 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [09:30<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8715999126434326     â”‚
â”‚        pixel_AUPRO        â”‚    0.9511387944221497     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa capsules with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/32 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/32 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:20,  1.51it/s]Epoch 0:   3%|â–Ž         | 1/32 [00:00<00:20,  1.51it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.19it/s]Epoch 0:   6%|â–‹         | 2/32 [00:00<00:13,  2.19it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.23it/s]Epoch 0:   9%|â–‰         | 3/32 [00:01<00:12,  2.23it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:12,  2.24it/s]Epoch 0:  12%|â–ˆâ–Ž        | 4/32 [00:01<00:12,  2.24it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:11,  2.26it/s]Epoch 0:  16%|â–ˆâ–Œ        | 5/32 [00:02<00:11,  2.26it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:11,  2.27it/s]Epoch 0:  19%|â–ˆâ–‰        | 6/32 [00:02<00:11,  2.27it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:03<00:10,  2.27it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 7/32 [00:03<00:10,  2.27it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:10,  2.28it/s]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:03<00:10,  2.28it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:03<00:10,  2.28it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:03<00:10,  2.28it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.29it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:04<00:09,  2.29it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:09,  2.29it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:04<00:09,  2.29it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:05<00:08,  2.30it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:05<00:08,  2.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:08,  2.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:05<00:08,  2.30it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:06<00:07,  2.30it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:06<00:07,  2.30it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:07,  2.30it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:06<00:07,  2.30it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:06<00:06,  2.31it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:06<00:06,  2.31it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:07<00:06,  2.31it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:07<00:06,  2.31it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:07<00:06,  2.31it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:07<00:06,  2.31it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:08<00:05,  2.31it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:08<00:05,  2.31it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:05,  2.31it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:08<00:05,  2.31it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:09<00:04,  2.31it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:09<00:04,  2.31it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.31it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:09<00:04,  2.31it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:09<00:03,  2.32it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:09<00:03,  2.32it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:10<00:03,  2.32it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:10<00:03,  2.32it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:10<00:03,  2.32it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:10<00:03,  2.32it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:11<00:02,  2.32it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:11<00:02,  2.32it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.32it/s]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:11<00:02,  2.32it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:12<00:01,  2.33it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:12<00:01,  2.33it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:12<00:01,  2.33it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:12<00:01,  2.33it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:12<00:00,  2.33it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:12<00:00,  2.33it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:13<00:00,  2.33it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:13<00:00,  2.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.34it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|â–ˆ         | 1/10 [00:05<00:47,  0.19it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:10<00:42,  0.19it/s][A
Validation DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:15<00:36,  0.19it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:21<00:31,  0.19it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:26<00:26,  0.19it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:31<00:21,  0.19it/s][A
Validation DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:36<00:15,  0.19it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:42<00:10,  0.19it/s][A
Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:47<00:05,  0.19it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:52<00:00,  0.19it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:53<00:00,  0.28it/s, pixel_AUPRO=nan.0]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:53<00:00,  0.28it/s, pixel_AUPRO=nan.0]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [02:21<00:00,  0.23it/s, pixel_AUPRO=nan.0]INFO:anomalib.callbacks.timer:Training took 142.78 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|â–ˆ         | 1/10 [00:08<01:13,  0.12it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 2/10 [00:15<01:03,  0.13it/s]Testing DataLoader 0:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:24<00:56,  0.12it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:31<00:47,  0.13it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:39<00:39,  0.13it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:47<00:31,  0.13it/s]Testing DataLoader 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:55<00:23,  0.13it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:03<00:15,  0.13it/s]Testing DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [01:10<00:07,  0.13it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:17<00:00,  0.13it/s]INFO:anomalib.callbacks.timer:Testing took 188.87911081314087 seconds
Throughput (batch_size=17) : 0.847102674886525 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:08<00:00,  0.05it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7529166340827942     â”‚
â”‚        pixel_AUPRO        â”‚    0.8428258299827576     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa cashew with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:18,  1.44it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:18,  1.44it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.12it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:11,  2.11it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:11,  2.18it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:11,  2.18it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.22it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:10,  2.22it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.23it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.23it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.24it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:09,  2.24it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.24it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:03<00:08,  2.24it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.20it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:08,  2.20it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:04<00:08,  2.17it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:04<00:08,  2.17it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.14it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:07,  2.14it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:05<00:07,  2.12it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:05<00:07,  2.12it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:07,  2.11it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:05<00:07,  2.11it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:06<00:06,  2.10it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:06<00:06,  2.10it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:06,  2.09it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:06<00:06,  2.09it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:07<00:05,  2.08it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:07<00:05,  2.08it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:05,  2.07it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:07<00:05,  2.07it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:08<00:04,  2.08it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:08<00:04,  2.08it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:08<00:04,  2.10it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:08<00:04,  2.10it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:09<00:03,  2.11it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:09<00:03,  2.11it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:09<00:03,  2.12it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:09<00:03,  2.12it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.13it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:09<00:02,  2.13it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:10<00:02,  2.14it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:10<00:02,  2.14it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.15it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:10<00:01,  2.15it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:11<00:01,  2.15it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:11<00:01,  2.15it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:11<00:00,  2.16it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:11<00:00,  2.16it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:12<00:00,  2.16it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:12<00:00,  2.16it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:12<00:00,  2.21it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:12<00:00,  2.21it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:07<01:00,  0.13it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:15<00:53,  0.13it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:22<00:45,  0.13it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:30<00:37,  0.13it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:37<00:30,  0.13it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:45<00:22,  0.13it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:52<00:15,  0.13it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [01:00<00:07,  0.13it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:08<00:00,  0.13it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:52<00:00,  0.16it/s, pixel_AUPRO=0.836]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:52<00:00,  0.16it/s, pixel_AUPRO=0.836]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:31<00:00,  0.13it/s, pixel_AUPRO=0.836]INFO:anomalib.callbacks.timer:Training took 212.68 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:10<01:25,  0.09it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:21<01:13,  0.09it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:31<01:03,  0.09it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:42<00:52,  0.09it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:53<00:42,  0.09it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [01:03<00:31,  0.09it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [01:14<00:21,  0.09it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [01:25<00:10,  0.09it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:35<00:00,  0.09it/s]INFO:anomalib.callbacks.timer:Testing took 256.87859082221985 seconds
Throughput (batch_size=17) : 0.583933443109752 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [04:16<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9344000220298767     â”‚
â”‚        pixel_AUPRO        â”‚    0.8395913243293762     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa chewinggum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.52it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.52it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.32it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.32it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.37it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.37it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.42it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.41it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:08,  2.45it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:08,  2.45it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.46it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.46it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.48it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.48it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.49it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.49it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.50it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.50it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:03<00:06,  2.50it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:03<00:06,  2.50it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.51it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.51it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.51it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.51it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.52it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.52it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.52it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.52it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.52it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.52it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.52it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.52it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.52it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.52it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.52it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.52it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.53it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.53it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.53it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.53it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.53it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.53it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.54it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.54it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.54it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.54it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.54it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.54it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.54it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.54it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.54it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.54it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.58it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.58it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:06<00:54,  0.15it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:13<00:47,  0.15it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:20<00:40,  0.15it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:26<00:33,  0.15it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:33<00:26,  0.15it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:40<00:20,  0.15it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:47<00:13,  0.15it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:53<00:06,  0.15it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:00<00:00,  0.15it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:42<00:00,  0.10it/s, pixel_AUPRO=0.678]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:42<00:00,  0.10it/s, pixel_AUPRO=0.678]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:18<00:00,  0.08it/s, pixel_AUPRO=0.678]INFO:anomalib.callbacks.timer:Training took 319.15 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:09<01:18,  0.10it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:19<01:07,  0.10it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:28<00:57,  0.10it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:38<00:47,  0.11it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:47<00:37,  0.11it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:56<00:28,  0.11it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [01:06<00:18,  0.11it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [01:15<00:09,  0.11it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:24<00:00,  0.11it/s]INFO:anomalib.callbacks.timer:Testing took 478.3498978614807 seconds
Throughput (batch_size=17) : 0.3135779910701196 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [07:57<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9709999561309814     â”‚
â”‚        pixel_AUPRO        â”‚    0.6872228384017944     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.51it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:17,  1.51it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.28it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.28it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.33it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:10,  2.33it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.36it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.36it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.38it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:09,  2.38it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.40it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.40it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.41it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:08,  2.41it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.43it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.43it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.43it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.43it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:06,  2.44it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:04<00:06,  2.44it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.44it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.44it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:06,  2.45it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:06,  2.45it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.45it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.45it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.45it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.45it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:04,  2.45it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:06<00:04,  2.45it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.45it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.45it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:04,  2.45it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:04,  2.45it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.45it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.45it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.45it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.45it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.45it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:08<00:02,  2.45it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.46it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.46it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:02,  2.46it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:02,  2.46it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.46it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:09<00:01,  2.46it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.46it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.46it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.46it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:10<00:00,  2.46it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.46it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.46it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.51it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.51it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:05<00:42,  0.19it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:10<00:36,  0.19it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:15<00:31,  0.19it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:20<00:26,  0.19it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:26<00:20,  0.19it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:31<00:15,  0.19it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:36<00:10,  0.19it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:41<00:05,  0.19it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:47<00:00,  0.19it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:54<00:00,  0.12it/s, pixel_AUPRO=0.861]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [03:54<00:00,  0.12it/s, pixel_AUPRO=0.861]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [04:22<00:00,  0.10it/s, pixel_AUPRO=0.861]INFO:anomalib.callbacks.timer:Training took 263.81 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:08<01:05,  0.12it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:16<00:56,  0.12it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:24<00:48,  0.12it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:32<00:40,  0.12it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:39<00:31,  0.13it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:47<00:23,  0.13it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:55<00:15,  0.13it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [01:03<00:07,  0.13it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:11<00:00,  0.13it/s]INFO:anomalib.callbacks.timer:Testing took 313.3547863960266 seconds
Throughput (batch_size=17) : 0.478690629638016 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [05:12<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9521999955177307     â”‚
â”‚        pixel_AUPRO        â”‚    0.8660639524459839     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.53it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.53it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.29it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:22,  2.29it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.35it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:21,  2.35it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.38it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.38it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.40it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.40it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.42it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.42it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.43it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.43it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.44it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.44it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.44it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.44it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.45it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.45it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.46it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.46it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.46it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.46it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.47it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.47it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.47it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.47it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.47it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.47it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.47it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.47it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.48it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.48it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.48it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.48it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.48it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.48it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.48it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.48it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.48it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.48it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.48it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.48it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.48it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.48it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.49it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.49it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.49it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.49it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.49it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.49it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.50it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.50it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.50it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.50it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.50it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.50it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.50it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.50it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.50it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.50it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.50it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.50it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.51it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.51it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.51it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.51it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.51it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.51it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.51it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.51it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.51it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.51it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:05,  2.51it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:05,  2.51it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.51it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.51it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.51it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.51it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.52it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.52it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.52it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.52it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.52it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.52it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.52it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.52it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.52it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.52it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.52it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.52it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.52it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.52it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.52it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.52it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.52it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.52it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.53it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.53it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.53it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.53it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.53it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.53it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:20<00:00,  2.53it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:20<00:00,  2.53it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:05<00:58,  0.19it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:10<00:52,  0.19it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:15<00:47,  0.19it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:21<00:42,  0.19it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:26<00:36,  0.19it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:31<00:31,  0.19it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:36<00:26,  0.19it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:41<00:20,  0.19it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:47<00:15,  0.19it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:52<00:10,  0.19it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:57<00:05,  0.19it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:03<00:00,  0.19it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:36<00:00,  0.16it/s, pixel_AUPRO=0.935]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:36<00:00,  0.16it/s, pixel_AUPRO=0.935]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [06:05<00:00,  0.15it/s, pixel_AUPRO=0.935]INFO:anomalib.callbacks.timer:Training took 366.34 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:08<01:29,  0.12it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:16<01:20,  0.12it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:24<01:12,  0.12it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:32<01:04,  0.12it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:39<00:55,  0.13it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:47<00:47,  0.13it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:55<00:39,  0.13it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [01:03<00:31,  0.13it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [01:11<00:23,  0.13it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:19<00:15,  0.13it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:27<00:07,  0.13it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:34<00:00,  0.13it/s]INFO:anomalib.callbacks.timer:Testing took 443.75696444511414 seconds
Throughput (batch_size=17) : 0.45069715187475534 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [07:22<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9447000026702881     â”‚
â”‚        pixel_AUPRO        â”‚    0.9349977970123291     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.56it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:33,  1.56it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.33it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:21,  2.33it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.38it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:20,  2.38it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.41it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:20,  2.41it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.43it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:19,  2.43it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.44it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:19,  2.44it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.46it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:18,  2.46it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.46it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:18,  2.46it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.47it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:17,  2.47it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.47it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:17,  2.47it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.48it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:16,  2.48it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.48it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:04<00:16,  2.48it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.49it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.49it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.49it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:15,  2.49it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.49it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:15,  2.49it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.49it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:14,  2.49it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.49it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:06<00:14,  2.49it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.49it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.49it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.49it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:07<00:13,  2.49it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.49it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:13,  2.49it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.50it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:12,  2.50it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.50it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:08<00:12,  2.50it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.50it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:11,  2.50it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.50it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:09<00:11,  2.50it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:11,  2.50it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:09<00:11,  2.50it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.50it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:10,  2.50it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.51it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:10<00:10,  2.51it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.51it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:09,  2.51it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.51it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:11<00:09,  2.51it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.51it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:11<00:09,  2.51it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.51it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:12<00:08,  2.51it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.51it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:12<00:08,  2.51it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.51it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:07,  2.51it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.51it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:13<00:07,  2.51it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.51it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:13<00:07,  2.51it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.52it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:14<00:06,  2.52it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.52it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:14<00:06,  2.52it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:05,  2.52it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:05,  2.52it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.51it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:15<00:05,  2.51it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.52it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:15<00:05,  2.52it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.52it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:16<00:04,  2.52it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.52it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:16<00:04,  2.52it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.52it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:03,  2.52it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.52it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:17<00:03,  2.52it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.52it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:17<00:03,  2.52it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.52it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:18<00:02,  2.52it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.52it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:18<00:02,  2.52it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.52it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:19<00:01,  2.52it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.52it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:19<00:01,  2.52it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.52it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:19<00:01,  2.52it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.52it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:20<00:00,  2.52it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.52it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:20<00:00,  2.52it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.52it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:21<00:00,  2.52it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:05<00:58,  0.19it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:10<00:52,  0.19it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:15<00:47,  0.19it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:21<00:42,  0.19it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:26<00:36,  0.19it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:31<00:31,  0.19it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:36<00:26,  0.19it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:42<00:21,  0.19it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:47<00:15,  0.19it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:52<00:10,  0.19it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:57<00:05,  0.19it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:03<00:00,  0.19it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:43<00:00,  0.19it/s, pixel_AUPRO=0.896]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:43<00:00,  0.19it/s, pixel_AUPRO=0.896]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:11<00:00,  0.17it/s, pixel_AUPRO=0.896]INFO:anomalib.callbacks.timer:Training took 312.92 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:08<01:30,  0.12it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:16<01:20,  0.12it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:24<01:13,  0.12it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:32<01:04,  0.12it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:40<00:56,  0.12it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:48<00:48,  0.12it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:56<00:40,  0.12it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [01:03<00:31,  0.13it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [01:11<00:23,  0.13it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:19<00:15,  0.13it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:27<00:07,  0.13it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:35<00:00,  0.13it/s]INFO:anomalib.callbacks.timer:Testing took 350.5064158439636 seconds
Throughput (batch_size=17) : 0.5706029646231492 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [05:49<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8730000257492065     â”‚
â”‚        pixel_AUPRO        â”‚    0.9118391871452332     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:35,  1.48it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:35,  1.48it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:24,  2.13it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:24,  2.13it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:23,  2.19it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:23,  2.19it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:22,  2.22it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:22,  2.22it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:21,  2.24it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:21,  2.24it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:21,  2.25it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:21,  2.25it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:20,  2.26it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:20,  2.26it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:20,  2.26it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:20,  2.26it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:19,  2.27it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:19,  2.27it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:19,  2.27it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:19,  2.27it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.28it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.28it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.28it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.28it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.28it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.28it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:06<00:17,  2.29it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:06<00:17,  2.29it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:17,  2.29it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:17,  2.29it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.29it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.29it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:16,  2.29it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:16,  2.29it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.29it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.29it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:15,  2.30it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:15,  2.30it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.30it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.30it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:09<00:14,  2.30it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:09<00:14,  2.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.30it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.31it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.31it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.31it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.31it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.31it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.31it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:11<00:12,  2.31it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:11<00:12,  2.31it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.31it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.31it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:12<00:11,  2.31it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:12<00:11,  2.31it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.31it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.31it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.32it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.32it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.32it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.32it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.32it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.32it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:14<00:09,  2.32it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:14<00:09,  2.32it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.32it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.32it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:15<00:08,  2.32it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:15<00:08,  2.32it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.32it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.32it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.33it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.33it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:16<00:06,  2.33it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:16<00:06,  2.33it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.33it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.33it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:17<00:06,  2.33it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:17<00:06,  2.33it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.33it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.33it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:18<00:05,  2.33it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:18<00:05,  2.33it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:18<00:04,  2.33it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:18<00:04,  2.33it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.33it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.33it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:19<00:03,  2.33it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:19<00:03,  2.33it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.33it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.33it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:20<00:02,  2.34it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:20<00:02,  2.34it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:20<00:02,  2.34it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:20<00:02,  2.34it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.34it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:21<00:01,  2.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:21<00:01,  2.34it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.34it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.34it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:22<00:00,  2.34it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:22<00:00,  2.34it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:22<00:00,  2.34it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:22<00:00,  2.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.37it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.37it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:06<01:06,  0.16it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:12<01:00,  0.17it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:18<00:54,  0.17it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:24<00:48,  0.17it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:30<00:42,  0.17it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:36<00:36,  0.17it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:42<00:30,  0.17it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:48<00:24,  0.17it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:54<00:18,  0.17it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:00<00:12,  0.16it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:06<00:06,  0.16it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:12<00:00,  0.16it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:28<00:00,  0.20it/s, pixel_AUPRO=0.860]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:28<00:00,  0.20it/s, pixel_AUPRO=0.860]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [05:00<00:00,  0.18it/s, pixel_AUPRO=0.860]INFO:anomalib.callbacks.timer:Training took 301.68 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:08<01:38,  0.11it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:17<01:28,  0.11it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:27<01:21,  0.11it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:36<01:12,  0.11it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:45<01:03,  0.11it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:53<00:53,  0.11it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [01:02<00:44,  0.11it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [01:11<00:35,  0.11it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [01:20<00:26,  0.11it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:29<00:17,  0.11it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:38<00:08,  0.11it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:46<00:00,  0.11it/s]INFO:anomalib.callbacks.timer:Testing took 415.7190990447998 seconds
Throughput (batch_size=17) : 0.4810940860295838 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [06:54<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9527000784873962     â”‚
â”‚        pixel_AUPRO        â”‚    0.9039201140403748     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/53 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/53 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/53 [00:00<00:35,  1.46it/s]Epoch 0:   2%|â–         | 1/53 [00:00<00:35,  1.46it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:23,  2.21it/s]Epoch 0:   4%|â–         | 2/53 [00:00<00:23,  2.20it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:22,  2.24it/s]Epoch 0:   6%|â–Œ         | 3/53 [00:01<00:22,  2.24it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:21,  2.31it/s]Epoch 0:   8%|â–Š         | 4/53 [00:01<00:21,  2.31it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.33it/s]Epoch 0:   9%|â–‰         | 5/53 [00:02<00:20,  2.33it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:20,  2.34it/s]Epoch 0:  11%|â–ˆâ–        | 6/53 [00:02<00:20,  2.34it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.35it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/53 [00:02<00:19,  2.35it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:19,  2.36it/s]Epoch 0:  15%|â–ˆâ–Œ        | 8/53 [00:03<00:19,  2.36it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.36it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/53 [00:03<00:18,  2.36it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.37it/s]Epoch 0:  19%|â–ˆâ–‰        | 10/53 [00:04<00:18,  2.37it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.37it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 11/53 [00:04<00:17,  2.37it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.38it/s]Epoch 0:  23%|â–ˆâ–ˆâ–Ž       | 12/53 [00:05<00:17,  2.38it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.37it/s]Epoch 0:  25%|â–ˆâ–ˆâ–       | 13/53 [00:05<00:16,  2.37it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.34it/s]Epoch 0:  26%|â–ˆâ–ˆâ–‹       | 14/53 [00:05<00:16,  2.34it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.34it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/53 [00:06<00:16,  2.34it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.34it/s]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 16/53 [00:06<00:15,  2.34it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.35it/s]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 17/53 [00:07<00:15,  2.35it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.35it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 18/53 [00:07<00:14,  2.35it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.35it/s]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/53 [00:08<00:14,  2.35it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:14,  2.35it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 20/53 [00:08<00:14,  2.35it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.35it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 21/53 [00:08<00:13,  2.35it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.36it/s]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/53 [00:09<00:13,  2.36it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.36it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/53 [00:09<00:12,  2.36it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.36it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 24/53 [00:10<00:12,  2.36it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.36it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/53 [00:10<00:11,  2.36it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.37it/s]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 26/53 [00:10<00:11,  2.37it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.37it/s]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/53 [00:11<00:10,  2.37it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.37it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 28/53 [00:11<00:10,  2.37it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:10,  2.37it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/53 [00:12<00:10,  2.37it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.37it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 30/53 [00:12<00:09,  2.37it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:13<00:09,  2.37it/s]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 31/53 [00:13<00:09,  2.37it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.38it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 32/53 [00:13<00:08,  2.38it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.38it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 33/53 [00:13<00:08,  2.38it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.38it/s]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/53 [00:14<00:07,  2.38it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.38it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 35/53 [00:14<00:07,  2.38it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:15<00:07,  2.38it/s]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 36/53 [00:15<00:07,  2.38it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.38it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 37/53 [00:15<00:06,  2.38it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.39it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/53 [00:15<00:06,  2.39it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.39it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 39/53 [00:16<00:05,  2.39it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.39it/s]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 40/53 [00:16<00:05,  2.39it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:17<00:05,  2.39it/s]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 41/53 [00:17<00:05,  2.39it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.39it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 42/53 [00:17<00:04,  2.39it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.39it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 43/53 [00:17<00:04,  2.39it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.39it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 44/53 [00:18<00:03,  2.39it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.39it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/53 [00:18<00:03,  2.39it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.40it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 46/53 [00:19<00:02,  2.40it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.39it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 47/53 [00:19<00:02,  2.39it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:20<00:02,  2.40it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 48/53 [00:20<00:02,  2.40it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.39it/s]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 49/53 [00:20<00:01,  2.39it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.39it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 50/53 [00:20<00:01,  2.39it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.39it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 51/53 [00:21<00:00,  2.39it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.39it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 52/53 [00:21<00:00,  2.39it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:22<00:00,  2.40it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:22<00:00,  2.40it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:06<01:07,  0.16it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:12<01:00,  0.16it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:18<00:54,  0.16it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:24<00:48,  0.16it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:30<00:42,  0.16it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:36<00:36,  0.17it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:42<00:30,  0.17it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:48<00:24,  0.16it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:54<00:18,  0.16it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:00<00:12,  0.16it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:06<00:06,  0.16it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:12<00:00,  0.16it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:49<00:00,  0.18it/s, pixel_AUPRO=0.899]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:49<00:00,  0.18it/s, pixel_AUPRO=0.899]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [05:22<00:00,  0.16it/s, pixel_AUPRO=0.899]INFO:anomalib.callbacks.timer:Training took 323.79 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:09<01:43,  0.11it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:18<01:32,  0.11it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:27<01:23,  0.11it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:36<01:13,  0.11it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:45<01:04,  0.11it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:54<00:54,  0.11it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [01:04<00:45,  0.11it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [01:13<00:36,  0.11it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [01:22<00:27,  0.11it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:30<00:18,  0.11it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:39<00:09,  0.11it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:48<00:00,  0.11it/s]INFO:anomalib.callbacks.timer:Testing took 454.09553575515747 seconds
Throughput (batch_size=17) : 0.4404359529044951 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [07:33<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9321999549865723     â”‚
â”‚        pixel_AUPRO        â”‚    0.9072372317314148     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb3 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:36,  1.46it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:36,  1.46it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.18it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.18it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.28it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.28it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.32it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.32it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.36it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.36it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.38it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.38it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.40it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:02<00:19,  2.40it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.41it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:19,  2.41it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.39it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:18,  2.39it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.37it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:18,  2.37it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.36it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.36it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:17,  2.35it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:17,  2.35it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.34it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.34it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:06<00:17,  2.32it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:06<00:17,  2.32it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.30it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.30it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.32it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.32it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.33it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.33it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.34it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.34it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:14,  2.35it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:14,  2.35it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.36it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.36it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.37it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:08<00:13,  2.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.38it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.38it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.38it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.38it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.39it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.39it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.40it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.40it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.40it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:10<00:11,  2.40it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.41it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.41it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.41it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:10,  2.41it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.41it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.41it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.42it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:09,  2.42it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.42it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:12<00:09,  2.42it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.42it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.42it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.43it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:13<00:08,  2.43it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:08,  2.43it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:13<00:08,  2.43it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.44it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:07,  2.44it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.44it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:14<00:07,  2.44it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:06,  2.44it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:06,  2.44it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.45it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:15<00:06,  2.45it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:15<00:06,  2.45it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:15<00:06,  2.45it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.45it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:16<00:05,  2.45it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.46it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:16<00:05,  2.46it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.46it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:04,  2.46it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.46it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:17<00:04,  2.46it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:17<00:04,  2.46it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:17<00:04,  2.46it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.46it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:18<00:03,  2.46it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.47it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:18<00:03,  2.47it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.47it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:19<00:02,  2.47it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.47it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:19<00:02,  2.47it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:19<00:02,  2.47it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:19<00:02,  2.47it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.47it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:20<00:01,  2.47it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.47it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:20<00:01,  2.47it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:20<00:00,  2.48it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:20<00:00,  2.48it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.48it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:21<00:00,  2.48it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.51it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:21<00:00,  2.51it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:04<00:54,  0.20it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:09<00:48,  0.20it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:14<00:43,  0.20it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:19<00:39,  0.20it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:24<00:34,  0.21it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:29<00:29,  0.21it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:34<00:24,  0.21it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:38<00:19,  0.21it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:43<00:14,  0.21it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:48<00:09,  0.21it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:53<00:04,  0.21it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:58<00:00,  0.21it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:16<00:00,  0.21it/s, pixel_AUPRO=0.876]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:16<00:00,  0.21it/s, pixel_AUPRO=0.876]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [04:42<00:00,  0.19it/s, pixel_AUPRO=0.876]INFO:anomalib.callbacks.timer:Training took 283.17 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:07<01:26,  0.13it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:15<01:16,  0.13it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:23<01:10,  0.13it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:31<01:02,  0.13it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:38<00:54,  0.13it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:46<00:46,  0.13it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:54<00:38,  0.13it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [01:02<00:31,  0.13it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [01:09<00:23,  0.13it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:17<00:15,  0.13it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:24<00:07,  0.13it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:31<00:00,  0.13it/s]INFO:anomalib.callbacks.timer:Testing took 404.45745968818665 seconds
Throughput (batch_size=17) : 0.49696202946772056 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [06:43<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9497029185295105     â”‚
â”‚        pixel_AUPRO        â”‚    0.8879282474517822     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb4 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/54 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.52it/s]Epoch 0:   2%|â–         | 1/54 [00:00<00:34,  1.52it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.25it/s]Epoch 0:   4%|â–Ž         | 2/54 [00:00<00:23,  2.25it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.31it/s]Epoch 0:   6%|â–Œ         | 3/54 [00:01<00:22,  2.31it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.32it/s]Epoch 0:   7%|â–‹         | 4/54 [00:01<00:21,  2.32it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.34it/s]Epoch 0:   9%|â–‰         | 5/54 [00:02<00:20,  2.34it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.35it/s]Epoch 0:  11%|â–ˆ         | 6/54 [00:02<00:20,  2.35it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:20,  2.29it/s]Epoch 0:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:20,  2.29it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:20,  2.28it/s]Epoch 0:  15%|â–ˆâ–        | 8/54 [00:03<00:20,  2.28it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:19,  2.28it/s]Epoch 0:  17%|â–ˆâ–‹        | 9/54 [00:03<00:19,  2.28it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:19,  2.29it/s]Epoch 0:  19%|â–ˆâ–Š        | 10/54 [00:04<00:19,  2.29it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.30it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:18,  2.30it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.31it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 12/54 [00:05<00:18,  2.31it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.32it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 13/54 [00:05<00:17,  2.32it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:06<00:17,  2.32it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:06<00:17,  2.32it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.32it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:06<00:16,  2.32it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.31it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:06<00:16,  2.31it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.31it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:07<00:15,  2.31it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.32it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:07<00:15,  2.32it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:15,  2.32it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:08<00:15,  2.32it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.32it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:08<00:14,  2.32it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:09<00:14,  2.33it/s]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:09<00:14,  2.33it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.33it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:09<00:13,  2.33it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.33it/s]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:09<00:13,  2.33it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.34it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:10<00:12,  2.34it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.34it/s]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:10<00:12,  2.34it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:11<00:11,  2.34it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:11<00:11,  2.34it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.34it/s]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:11<00:11,  2.34it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:11,  2.34it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:11<00:11,  2.34it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.34it/s]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:12<00:10,  2.34it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.34it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:12<00:10,  2.34it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.34it/s]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:13<00:09,  2.34it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.34it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:13<00:09,  2.34it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:14<00:08,  2.34it/s]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:14<00:08,  2.34it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.34it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:14<00:08,  2.34it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:08,  2.34it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:14<00:08,  2.34it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.34it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:15<00:07,  2.34it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.34it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:15<00:07,  2.34it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:16<00:06,  2.34it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:16<00:06,  2.34it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.33it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:16<00:06,  2.33it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:17<00:05,  2.34it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:17<00:05,  2.34it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.34it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:17<00:05,  2.34it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:05,  2.34it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:17<00:05,  2.34it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:18<00:04,  2.34it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:18<00:04,  2.34it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.34it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:18<00:04,  2.34it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:19<00:03,  2.34it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:19<00:03,  2.34it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.34it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:19<00:03,  2.34it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:20<00:02,  2.34it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:20<00:02,  2.34it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:20<00:02,  2.34it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:20<00:02,  2.34it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.34it/s]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:20<00:02,  2.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:21<00:01,  2.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:21<00:01,  2.34it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.34it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:21<00:01,  2.34it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:22<00:00,  2.34it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:22<00:00,  2.34it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:22<00:00,  2.34it/s]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:22<00:00,  2.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.38it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:22<00:00,  2.38it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s][A
Validation DataLoader 0:   8%|â–Š         | 1/12 [00:06<01:11,  0.15it/s][A
Validation DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:12<01:04,  0.15it/s][A
Validation DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:19<00:58,  0.15it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:25<00:51,  0.16it/s][A
Validation DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:32<00:45,  0.16it/s][A
Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:38<00:38,  0.16it/s][A
Validation DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:45<00:32,  0.16it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:51<00:25,  0.16it/s][A
Validation DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:58<00:19,  0.16it/s][A
Validation DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:04<00:12,  0.15it/s][A
Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:11<00:06,  0.15it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:17<00:00,  0.15it/s][A
                                                                        [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [07:19<00:00,  0.12it/s, pixel_AUPRO=0.851]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [07:19<00:00,  0.12it/s, pixel_AUPRO=0.851]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [07:52<00:00,  0.11it/s, pixel_AUPRO=0.851]INFO:anomalib.callbacks.timer:Training took 473.76 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/12 [00:00<?, ?it/s]Testing DataLoader 0:   8%|â–Š         | 1/12 [00:09<01:45,  0.10it/s]Testing DataLoader 0:  17%|â–ˆâ–‹        | 2/12 [00:18<01:34,  0.11it/s]Testing DataLoader 0:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:28<01:25,  0.11it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:37<01:15,  0.11it/s]Testing DataLoader 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:47<01:05,  0.11it/s]Testing DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:56<00:56,  0.11it/s]Testing DataLoader 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [01:06<00:47,  0.11it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [01:15<00:37,  0.11it/s]Testing DataLoader 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [01:24<00:28,  0.11it/s]Testing DataLoader 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [01:34<00:18,  0.11it/s]Testing DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [01:43<00:09,  0.11it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:52<00:00,  0.11it/s]INFO:anomalib.callbacks.timer:Testing took 667.862067937851 seconds
Throughput (batch_size=17) : 0.30096034742716427 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [11:07<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9957424402236938     â”‚
â”‚        pixel_AUPRO        â”‚    0.8724612593650818     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pipe_fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/27 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/27 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.53it/s]Epoch 0:   4%|â–Ž         | 1/27 [00:00<00:16,  1.53it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.33it/s]Epoch 0:   7%|â–‹         | 2/27 [00:00<00:10,  2.33it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:09,  2.41it/s]Epoch 0:  11%|â–ˆ         | 3/27 [00:01<00:09,  2.41it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.45it/s]Epoch 0:  15%|â–ˆâ–        | 4/27 [00:01<00:09,  2.45it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:08,  2.48it/s]Epoch 0:  19%|â–ˆâ–Š        | 5/27 [00:02<00:08,  2.48it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.50it/s]Epoch 0:  22%|â–ˆâ–ˆâ–       | 6/27 [00:02<00:08,  2.50it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:07,  2.51it/s]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 7/27 [00:02<00:07,  2.51it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.52it/s]Epoch 0:  30%|â–ˆâ–ˆâ–‰       | 8/27 [00:03<00:07,  2.52it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.53it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 9/27 [00:03<00:07,  2.53it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:03<00:06,  2.53it/s]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 10/27 [00:03<00:06,  2.53it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.53it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 11/27 [00:04<00:06,  2.53it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.54it/s]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/27 [00:04<00:05,  2.54it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.54it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 13/27 [00:05<00:05,  2.54it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.54it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 14/27 [00:05<00:05,  2.54it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.55it/s]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 15/27 [00:05<00:04,  2.55it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.55it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 16/27 [00:06<00:04,  2.55it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.55it/s]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 17/27 [00:06<00:03,  2.55it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.55it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 18/27 [00:07<00:03,  2.55it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.55it/s]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 19/27 [00:07<00:03,  2.55it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.55it/s]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 20/27 [00:07<00:02,  2.55it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.55it/s]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 21/27 [00:08<00:02,  2.55it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.56it/s]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 22/27 [00:08<00:01,  2.56it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:08<00:01,  2.56it/s]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 23/27 [00:08<00:01,  2.56it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.56it/s]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 24/27 [00:09<00:01,  2.56it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.56it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 25/27 [00:09<00:00,  2.56it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.56it/s]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:10<00:00,  2.56it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.62it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:10<00:00,  2.62it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s][A
Validation DataLoader 0:  11%|â–ˆ         | 1/9 [00:07<00:57,  0.14it/s][A
Validation DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:14<00:49,  0.14it/s][A
Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:21<00:42,  0.14it/s][A
Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:28<00:35,  0.14it/s][A
Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:35<00:28,  0.14it/s][A
Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:42<00:21,  0.14it/s][A
Validation DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:49<00:14,  0.14it/s][A
Validation DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:57<00:07,  0.14it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:04<00:00,  0.14it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [07:06<00:00,  0.06it/s, pixel_AUPRO=0.946]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [07:06<00:00,  0.06it/s, pixel_AUPRO=0.946]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [07:42<00:00,  0.06it/s, pixel_AUPRO=0.946]INFO:anomalib.callbacks.timer:Training took 463.89 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]Testing DataLoader 0:  11%|â–ˆ         | 1/9 [00:10<01:21,  0.10it/s]Testing DataLoader 0:  22%|â–ˆâ–ˆâ–       | 2/9 [00:19<01:09,  0.10it/s]Testing DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3/9 [00:30<01:00,  0.10it/s]Testing DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:40<00:50,  0.10it/s]Testing DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:50<00:40,  0.10it/s]Testing DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [01:00<00:30,  0.10it/s]Testing DataLoader 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [01:10<00:20,  0.10it/s]Testing DataLoader 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [01:20<00:10,  0.10it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [01:30<00:00,  0.10it/s]INFO:anomalib.callbacks.timer:Testing took 533.6570565700531 seconds
Throughput (batch_size=17) : 0.2810793901313465 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [08:52<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9938000440597534     â”‚
â”‚        pixel_AUPRO        â”‚    0.9480736255645752     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
