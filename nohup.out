/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa candle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  1.00it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  1.00it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.29it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.29it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.31it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.31it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:18,  1.32it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:18,  1.32it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.32it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.32it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.33it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.33it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.33it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.33it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.33it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.33it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.34it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.34it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.34it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.34it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.34it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.34it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.34it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.34it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.34it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.34it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.34it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.34it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.35it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.35it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.35it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.35it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.35it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.35it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.35it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.35it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.35it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.35it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.35it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.35it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.35it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.35it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.36it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.36it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.36it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.36it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.36it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.36it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.36it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.36it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.36it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.36it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.36it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.36it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.36it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.40it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.40it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.90it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.86it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.82it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.86it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.89it/s][A
Detected KeyboardInterrupt, attempting graceful shutdown ...
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa candle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.01it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.01it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.31it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.31it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.34it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.34it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.35it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.35it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.36it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.36it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:16,  1.36it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:16,  1.36it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.37it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.37it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.37it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.37it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.37it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.37it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:13,  1.37it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:13,  1.37it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.37it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.37it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.37it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.37it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:10,  1.37it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:10,  1.37it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:10<00:10,  1.37it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:10<00:10,  1.37it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.38it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.38it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.38it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.38it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:07,  1.38it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:07,  1.38it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.38it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.38it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.38it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.38it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.38it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.38it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:15<00:05,  1.38it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:15<00:05,  1.38it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.38it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.38it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.38it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.38it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.38it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.38it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:18<00:02,  1.38it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:18<00:02,  1.38it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.38it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.38it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.38it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.38it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.43it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.43it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.89it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.85it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.81it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.85it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.87it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.89it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  0.91it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:52<00:00,  0.08it/s, pixel_AUPRO=0.935]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:52<00:00,  0.08it/s, pixel_AUPRO=0.935]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [06:30<00:00,  0.07it/s, pixel_AUPRO=0.935]INFO:anomalib.callbacks.timer:Training took 391.85 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:38,  0.15it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:13<00:32,  0.15it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:19<00:25,  0.16it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:25<00:18,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:37<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:40<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 487.41269063949585 seconds
Throughput (batch_size=32) : 0.41032989875088344 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [08:05<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8519001007080078     â”‚
â”‚        pixel_AUPRO        â”‚     0.943231463432312     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa candle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.30it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.30it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.32it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.32it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:18,  1.33it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:18,  1.33it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.33it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.33it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.33it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.33it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.33it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.33it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.34it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.34it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.34it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.34it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.34it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.34it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.34it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.34it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.34it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.34it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.34it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.34it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.34it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.34it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.34it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.34it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.34it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.34it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.34it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.34it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.35it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.35it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.35it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.35it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.35it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.35it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.35it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.35it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.35it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.35it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.35it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.35it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.35it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.35it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.35it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.35it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.35it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.35it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.35it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.35it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.35it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.35it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.40it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.40it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.88it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:06,  0.80it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:05,  0.76it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.81it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.84it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.86it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  0.88it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:09<00:00,  0.07it/s, pixel_AUPRO=0.947]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:09<00:00,  0.07it/s, pixel_AUPRO=0.947]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:47<00:00,  0.06it/s, pixel_AUPRO=0.947]INFO:anomalib.callbacks.timer:Training took 468.34 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:38,  0.15it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:31,  0.16it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:19<00:25,  0.16it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:25<00:19,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:37<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:40<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 491.2535524368286 seconds
Throughput (batch_size=32) : 0.4071217378641113 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [08:09<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.882099986076355     â”‚
â”‚        pixel_AUPRO        â”‚    0.9468481540679932     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa candle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.01it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.00it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.23it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.23it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:21,  1.22it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:21,  1.22it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:20,  1.22it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:20,  1.22it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:04<00:19,  1.22it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:04<00:19,  1.22it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.23it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.23it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.25it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.25it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.26it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.26it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.27it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.27it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.28it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.28it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.28it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.28it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.29it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.29it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.29it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.29it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.30it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.30it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.30it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.30it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:09,  1.30it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:09,  1.30it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.30it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.30it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.31it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.31it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.31it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.31it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.31it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.31it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.31it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.31it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.31it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.31it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.31it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.31it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.31it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.31it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.31it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.31it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.32it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.32it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.32it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.32it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.32it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.32it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.36it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.87it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:06,  0.83it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:05,  0.77it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.82it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.84it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.86it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  0.88it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:11<00:00,  0.07it/s, pixel_AUPRO=0.942]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:11<00:00,  0.07it/s, pixel_AUPRO=0.942]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:50<00:00,  0.06it/s, pixel_AUPRO=0.942]INFO:anomalib.callbacks.timer:Training took 471.13 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:40,  0.15it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:13<00:32,  0.15it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:19<00:25,  0.16it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:25<00:19,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:32<00:12,  0.15it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:38<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:41<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 477.78524351119995 seconds
Throughput (batch_size=32) : 0.41859811016811305 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [07:56<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8737000226974487     â”‚
â”‚        pixel_AUPRO        â”‚    0.9417703151702881     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa candle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.01it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.01it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.31it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.31it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.34it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.34it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.35it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.35it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.36it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.36it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:16,  1.37it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:16,  1.37it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:15,  1.38it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:15,  1.38it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.38it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.38it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.38it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.38it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:13,  1.37it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:13,  1.37it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.37it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.37it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.37it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.37it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:10,  1.37it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:10,  1.37it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:10<00:10,  1.37it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:10<00:10,  1.37it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.37it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.37it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.37it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.37it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.37it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.37it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.37it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.37it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.37it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.37it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.37it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.37it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.37it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.37it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.37it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.37it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.37it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.37it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.37it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.37it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:18<00:02,  1.37it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:18<00:02,  1.37it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.37it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.37it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.37it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.37it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.41it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.41it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.88it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.84it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.81it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.84it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.87it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.88it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  0.90it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:13<00:00,  0.07it/s, pixel_AUPRO=0.947]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:13<00:00,  0.07it/s, pixel_AUPRO=0.947]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:51<00:00,  0.06it/s, pixel_AUPRO=0.947]INFO:anomalib.callbacks.timer:Training took 472.64 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:38,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:31,  0.16it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:18<00:24,  0.16it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:24<00:18,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:30<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:36<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 492.77712893486023 seconds
Throughput (batch_size=32) : 0.40586299212445354 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [08:11<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.909600019454956     â”‚
â”‚        pixel_AUPRO        â”‚    0.9472154974937439     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa candle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.01it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.01it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.30it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.30it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.32it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.32it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.34it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.34it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.34it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.34it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.35it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.35it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.35it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.35it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.35it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.35it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.35it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.35it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.35it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.35it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.35it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.35it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.35it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.35it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.35it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.35it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.35it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.35it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.35it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.35it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.35it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.35it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.35it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.35it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.35it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.35it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.35it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.35it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.35it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.35it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.35it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.35it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.35it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.35it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.35it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.35it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.35it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.35it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.35it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.35it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.35it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.35it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.35it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.35it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.35it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.35it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.39it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.39it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.96it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.87it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.82it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.87it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.91it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.93it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  0.95it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:11<00:00,  0.07it/s, pixel_AUPRO=0.946]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:11<00:00,  0.07it/s, pixel_AUPRO=0.946]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:50<00:00,  0.06it/s, pixel_AUPRO=0.946]INFO:anomalib.callbacks.timer:Training took 471.60 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:41,  0.15it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:13<00:33,  0.15it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:19<00:25,  0.16it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:25<00:18,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:37<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 479.0321044921875 seconds
Throughput (batch_size=32) : 0.41750855135694104 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [07:57<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9228000044822693     â”‚
â”‚        pixel_AUPRO        â”‚    0.9456433653831482     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa candle with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.32it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.32it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.34it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.34it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.36it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.36it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.37it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.37it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:16,  1.37it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:16,  1.37it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.37it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.37it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.38it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.38it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.38it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.38it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:13,  1.38it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:13,  1.38it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:07<00:13,  1.38it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:07<00:13,  1.38it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.38it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.38it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.38it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.38it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:10,  1.38it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:10,  1.38it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:10<00:10,  1.38it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:10<00:10,  1.38it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.38it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.38it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.38it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.38it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:07,  1.38it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:07,  1.38it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.38it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.38it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.38it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.38it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.38it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.38it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:15<00:05,  1.38it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:15<00:05,  1.38it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.38it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.38it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.38it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.38it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.38it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.38it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:18<00:02,  1.38it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:18<00:02,  1.38it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.38it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.38it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.38it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.38it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.43it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.43it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:09,  0.64it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:03<00:07,  0.65it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:04<00:06,  0.66it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:06<00:04,  0.66it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:07<00:03,  0.67it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:08<00:01,  0.67it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:10<00:00,  0.67it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:18<00:00,  0.07it/s, pixel_AUPRO=0.953]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:18<00:00,  0.07it/s, pixel_AUPRO=0.953]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:56<00:00,  0.06it/s, pixel_AUPRO=0.953]INFO:anomalib.callbacks.timer:Training took 477.05 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:41,  0.15it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:13<00:33,  0.15it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:20<00:26,  0.15it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:26<00:20,  0.15it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:33<00:13,  0.15it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:40<00:06,  0.15it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:42<00:00,  0.16it/s]INFO:anomalib.callbacks.timer:Testing took 484.18067622184753 seconds
Throughput (batch_size=32) : 0.41306894269436245 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [08:02<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.933899998664856     â”‚
â”‚        pixel_AUPRO        â”‚     0.952895998954773     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa capsules with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/17 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/17 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.95it/s]Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.95it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:12,  1.22it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:12,  1.22it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:11,  1.24it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:11,  1.24it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:10,  1.25it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:10,  1.25it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:03<00:09,  1.25it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:03<00:09,  1.25it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:04<00:08,  1.26it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:04<00:08,  1.26it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:05<00:07,  1.26it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:05<00:07,  1.26it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:06<00:07,  1.26it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:06<00:07,  1.26it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:07<00:06,  1.26it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:07<00:06,  1.26it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:07<00:05,  1.26it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:07<00:05,  1.26it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:08<00:04,  1.26it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:08<00:04,  1.26it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:09<00:03,  1.26it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:09<00:03,  1.26it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:10<00:03,  1.26it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:10<00:03,  1.26it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:11<00:02,  1.27it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:11<00:02,  1.27it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:11<00:01,  1.27it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:11<00:01,  1.27it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:12<00:00,  1.27it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:12<00:00,  1.27it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:13<00:00,  1.27it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:13<00:00,  1.27it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.14it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.91it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.82it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.88it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.93it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:57<00:00,  0.29it/s, pixel_AUPRO=nan.0]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:57<00:00,  0.29it/s, pixel_AUPRO=nan.0]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:25<00:00,  0.20it/s, pixel_AUPRO=nan.0]INFO:anomalib.callbacks.timer:Training took 86.39 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:05<00:23,  0.17it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:11<00:17,  0.17it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:17<00:11,  0.18it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:22<00:05,  0.18it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:28<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 125.48399806022644 seconds
Throughput (batch_size=32) : 1.2750629759438132 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:03<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.6308333277702332     â”‚
â”‚        pixel_AUPRO        â”‚    0.6663526296615601     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa capsules with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/17 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/17 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.97it/s]Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.97it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:12,  1.16it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:12,  1.16it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:12,  1.13it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:12,  1.13it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:11,  1.11it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:11,  1.11it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:04<00:10,  1.12it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:04<00:10,  1.12it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:05<00:09,  1.14it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:05<00:09,  1.14it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:06<00:08,  1.16it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:06<00:08,  1.16it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:06<00:07,  1.17it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:06<00:07,  1.17it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:07<00:06,  1.18it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:07<00:06,  1.18it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:08<00:05,  1.19it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:08<00:05,  1.19it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:09<00:04,  1.20it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:09<00:04,  1.20it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:09<00:04,  1.21it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:09<00:04,  1.21it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:10<00:03,  1.22it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:10<00:03,  1.22it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:11<00:02,  1.22it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:11<00:02,  1.22it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:12<00:01,  1.23it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:12<00:01,  1.22it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:13<00:00,  1.23it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:13<00:00,  1.23it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:13<00:00,  1.24it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:13<00:00,  1.24it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.17it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.93it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.85it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.91it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.97it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:17<00:00,  0.22it/s, pixel_AUPRO=nan.0]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:17<00:00,  0.22it/s, pixel_AUPRO=nan.0]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:49<00:00,  0.16it/s, pixel_AUPRO=nan.0]INFO:anomalib.callbacks.timer:Training took 110.01 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:05<00:23,  0.17it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:11<00:17,  0.17it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:17<00:11,  0.17it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:22<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:28<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 125.03853988647461 seconds
Throughput (batch_size=32) : 1.2796054732026438 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:03<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.6192499995231628     â”‚
â”‚        pixel_AUPRO        â”‚    0.5750835537910461     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa capsules with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/17 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/17 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.98it/s]Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.98it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:11,  1.27it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:11,  1.27it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:10,  1.28it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:10,  1.28it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:10,  1.29it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:10,  1.29it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:03<00:09,  1.29it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:03<00:09,  1.29it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:04<00:08,  1.30it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:04<00:08,  1.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:05<00:07,  1.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:05<00:07,  1.30it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:06<00:06,  1.31it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:06<00:06,  1.31it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:06<00:06,  1.31it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:06<00:06,  1.31it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:07<00:05,  1.31it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:07<00:05,  1.31it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:08<00:04,  1.31it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:08<00:04,  1.31it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:09<00:03,  1.31it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:09<00:03,  1.31it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:09<00:03,  1.31it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:09<00:03,  1.31it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:10<00:02,  1.31it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:10<00:02,  1.31it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:11<00:01,  1.31it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:11<00:01,  1.31it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:12<00:00,  1.32it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:12<00:00,  1.32it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:12<00:00,  1.32it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:12<00:00,  1.32it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.16it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.95it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.85it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.92it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.98it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:19<00:00,  0.22it/s, pixel_AUPRO=0.406]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:19<00:00,  0.22it/s, pixel_AUPRO=0.406]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:46<00:00,  0.16it/s, pixel_AUPRO=0.406]INFO:anomalib.callbacks.timer:Training took 107.03 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:05<00:23,  0.17it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:11<00:17,  0.18it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:16<00:11,  0.18it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:22<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:28<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 125.48413872718811 seconds
Throughput (batch_size=32) : 1.2750615466059176 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:03<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.6296666860580444     â”‚
â”‚        pixel_AUPRO        â”‚    0.5842023491859436     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa capsules with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/17 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/17 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.95it/s]Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.95it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:12,  1.18it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:12,  1.18it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:12,  1.15it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:12,  1.15it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:11,  1.14it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:11,  1.14it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:04<00:10,  1.12it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:04<00:10,  1.12it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:05<00:09,  1.12it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:05<00:09,  1.12it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:06<00:08,  1.12it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:06<00:08,  1.12it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:07<00:08,  1.11it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:07<00:08,  1.11it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:08<00:07,  1.11it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:08<00:07,  1.11it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:08<00:06,  1.13it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:08<00:06,  1.13it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:09<00:05,  1.14it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:09<00:05,  1.14it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:10<00:04,  1.15it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:10<00:04,  1.15it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:11<00:03,  1.16it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:11<00:03,  1.16it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:12<00:02,  1.15it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:12<00:02,  1.15it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:13<00:01,  1.14it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:13<00:01,  1.14it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:13<00:00,  1.14it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:13<00:00,  1.14it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:14<00:00,  1.16it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:14<00:00,  1.16it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.16it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.92it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.82it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.87it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.91it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:21<00:00,  0.21it/s, pixel_AUPRO=0.402]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:21<00:00,  0.21it/s, pixel_AUPRO=0.402]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:48<00:00,  0.16it/s, pixel_AUPRO=0.402]INFO:anomalib.callbacks.timer:Training took 109.63 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:05<00:22,  0.17it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:11<00:17,  0.17it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:17<00:11,  0.17it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:23<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:28<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 124.85121726989746 seconds
Throughput (batch_size=32) : 1.2815253507230095 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:03<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.5809999704360962     â”‚
â”‚        pixel_AUPRO        â”‚    0.5645555853843689     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa capsules with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/17 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/17 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.99it/s]Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.99it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:11,  1.25it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:11,  1.25it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:11,  1.27it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:11,  1.27it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:10,  1.28it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:10,  1.28it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:03<00:09,  1.28it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:03<00:09,  1.28it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:04<00:08,  1.29it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:04<00:08,  1.29it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:05<00:07,  1.29it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:05<00:07,  1.29it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:06<00:06,  1.30it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:06<00:06,  1.30it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:06<00:06,  1.30it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:06<00:06,  1.30it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:07<00:05,  1.30it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:07<00:05,  1.30it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:08<00:04,  1.30it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:08<00:04,  1.30it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:09<00:03,  1.30it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:09<00:03,  1.30it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:09<00:03,  1.30it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:09<00:03,  1.30it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:10<00:02,  1.30it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:10<00:02,  1.30it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:11<00:01,  1.30it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:11<00:01,  1.30it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:12<00:00,  1.30it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:12<00:00,  1.30it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:12<00:00,  1.31it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:12<00:00,  1.31it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.32it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.98it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.87it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.93it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.98it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:20<00:00,  0.21it/s, pixel_AUPRO=0.573]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:20<00:00,  0.21it/s, pixel_AUPRO=0.573]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:48<00:00,  0.16it/s, pixel_AUPRO=0.573]INFO:anomalib.callbacks.timer:Training took 109.02 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:05<00:23,  0.17it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:11<00:17,  0.17it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:16<00:11,  0.18it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:22<00:05,  0.18it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:27<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 127.57454800605774 seconds
Throughput (batch_size=32) : 1.2541686606046416 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:06<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.6444999575614929     â”‚
â”‚        pixel_AUPRO        â”‚    0.6352010369300842     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa capsules with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/17 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/17 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.98it/s]Epoch 0:   6%|â–Œ         | 1/17 [00:01<00:16,  0.98it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:11,  1.25it/s]Epoch 0:  12%|â–ˆâ–        | 2/17 [00:01<00:11,  1.25it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:11,  1.27it/s]Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:02<00:11,  1.27it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:10,  1.28it/s]Epoch 0:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:03<00:10,  1.28it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:03<00:09,  1.29it/s]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:03<00:09,  1.29it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:04<00:08,  1.29it/s]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:04<00:08,  1.29it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:05<00:07,  1.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:05<00:07,  1.30it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:06<00:06,  1.30it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:06<00:06,  1.30it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:06<00:06,  1.30it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:06<00:06,  1.30it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:07<00:05,  1.30it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:07<00:05,  1.30it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:08<00:04,  1.30it/s]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:08<00:04,  1.30it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:09<00:03,  1.30it/s]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:09<00:03,  1.30it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:09<00:03,  1.30it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:09<00:03,  1.30it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:10<00:02,  1.30it/s]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:10<00:02,  1.30it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:11<00:01,  1.30it/s]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:11<00:01,  1.30it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:12<00:00,  1.30it/s]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:12<00:00,  1.30it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:12<00:00,  1.31it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:12<00:00,  1.31it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.88it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.81it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.77it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.81it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.83it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:28<00:00,  0.19it/s, pixel_AUPRO=0.702]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:28<00:00,  0.19it/s, pixel_AUPRO=0.702]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:55<00:00,  0.15it/s, pixel_AUPRO=0.702]INFO:anomalib.callbacks.timer:Training took 116.55 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:25,  0.16it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:18,  0.16it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:18<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:24<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:31<00:00,  0.16it/s]INFO:anomalib.callbacks.timer:Testing took 136.61621952056885 seconds
Throughput (batch_size=32) : 1.1711640137715165 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:14<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.6696667075157166     â”‚
â”‚        pixel_AUPRO        â”‚    0.7047271132469177     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa cashew with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.96it/s]Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.96it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.22it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.22it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.24it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.24it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.25it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.25it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.25it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.25it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.25it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.25it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.25it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.25it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.26it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.26it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.25it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.25it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.25it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.25it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.26it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.26it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.26it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.26it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.26it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.26it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.26it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.26it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.35it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.35it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.87it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.82it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.76it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.80it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:06<00:00,  0.83it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:22<00:00,  0.18it/s, pixel_AUPRO=0.638]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:22<00:00,  0.18it/s, pixel_AUPRO=0.638]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:55<00:00,  0.13it/s, pixel_AUPRO=0.638]INFO:anomalib.callbacks.timer:Training took 116.80 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:26,  0.15it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:19,  0.16it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:19<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:25<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:29<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 191.4469919204712 seconds
Throughput (batch_size=32) : 0.7835066954842067 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:09<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.767799973487854     â”‚
â”‚        pixel_AUPRO        â”‚    0.8231343626976013     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa cashew with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:01<00:15,  0.92it/s]Epoch 0:   7%|â–‹         | 1/15 [00:01<00:15,  0.92it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:11,  1.16it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:11,  1.16it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:10,  1.15it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:10,  1.15it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:09,  1.12it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:09,  1.12it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:09,  1.10it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:09,  1.10it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:05<00:08,  1.10it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:05<00:08,  1.10it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:06<00:07,  1.12it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:06<00:07,  1.12it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:07<00:06,  1.13it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:07<00:06,  1.13it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:05,  1.14it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:05,  1.13it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:08<00:04,  1.14it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:08<00:04,  1.14it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:09<00:03,  1.15it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:09<00:03,  1.15it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:10<00:02,  1.15it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:10<00:02,  1.15it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:11<00:01,  1.16it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:11<00:01,  1.16it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:12<00:00,  1.17it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:12<00:00,  1.17it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:12<00:00,  1.24it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:12<00:00,  1.24it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.86it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.80it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.76it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:05<00:01,  0.80it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:06<00:00,  0.82it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:18<00:00,  0.11it/s, pixel_AUPRO=0.871]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:18<00:00,  0.11it/s, pixel_AUPRO=0.871]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:56<00:00,  0.08it/s, pixel_AUPRO=0.871]INFO:anomalib.callbacks.timer:Training took 177.88 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:26,  0.15it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:19,  0.15it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:19<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:25<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:30<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 191.09497666358948 seconds
Throughput (batch_size=32) : 0.7849499898894017 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:09<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8040000200271606     â”‚
â”‚        pixel_AUPRO        â”‚     0.871076762676239     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa cashew with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.95it/s]Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.95it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.19it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.19it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.22it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.22it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.22it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.22it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:08,  1.23it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:08,  1.23it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.23it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.23it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.24it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.24it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.24it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.24it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.24it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.24it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:08<00:04,  1.24it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:08<00:04,  1.24it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.24it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.24it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.24it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.24it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.25it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.25it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.25it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.25it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.33it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.89it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.81it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.77it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.81it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.84it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:19<00:00,  0.11it/s, pixel_AUPRO=0.866]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:19<00:00,  0.11it/s, pixel_AUPRO=0.866]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:57<00:00,  0.08it/s, pixel_AUPRO=0.866]INFO:anomalib.callbacks.timer:Training took 178.33 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:25,  0.15it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:19,  0.15it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:19<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:25<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:29<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 190.58218622207642 seconds
Throughput (batch_size=32) : 0.7870620175655456 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:09<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8077999949455261     â”‚
â”‚        pixel_AUPRO        â”‚    0.8660513758659363     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa cashew with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.96it/s]Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.96it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.21it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.21it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.23it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.23it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.25it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.25it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.25it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.25it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.26it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.26it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.26it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.26it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.26it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.26it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.26it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.26it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.26it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.26it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.27it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.27it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.27it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.27it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.27it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.27it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.27it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.27it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.36it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.88it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.81it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.78it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.82it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.84it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:18<00:00,  0.11it/s, pixel_AUPRO=0.868]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:18<00:00,  0.11it/s, pixel_AUPRO=0.868]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:57<00:00,  0.08it/s, pixel_AUPRO=0.868]INFO:anomalib.callbacks.timer:Training took 178.34 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:26,  0.15it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:19,  0.16it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:18<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:25<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:29<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 190.84082698822021 seconds
Throughput (batch_size=32) : 0.785995336360908 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:09<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9220000505447388     â”‚
â”‚        pixel_AUPRO        â”‚    0.8680670857429504     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa cashew with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.95it/s]Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.95it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.21it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.21it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.23it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.23it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.25it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.25it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.25it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.25it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.26it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.26it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.26it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.26it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.26it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.26it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.26it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.26it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.26it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.26it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.27it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.27it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.27it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.27it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.27it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.27it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.27it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.27it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.36it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.96it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.84it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.79it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.84it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.88it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:21<00:00,  0.11it/s, pixel_AUPRO=0.884]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:21<00:00,  0.11it/s, pixel_AUPRO=0.884]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:02<00:00,  0.08it/s, pixel_AUPRO=0.884]INFO:anomalib.callbacks.timer:Training took 182.99 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:25,  0.16it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:19,  0.16it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:18<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:24<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:29<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 191.26061701774597 seconds
Throughput (batch_size=32) : 0.7842701876575163 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:09<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9320000410079956     â”‚
â”‚        pixel_AUPRO        â”‚    0.8837656378746033     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa cashew with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.95it/s]Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.95it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.20it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.20it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.23it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.23it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.24it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.24it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:08,  1.25it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:08,  1.25it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.25it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.25it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.26it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.26it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.26it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.26it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.26it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.26it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.26it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.26it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.26it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.26it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.26it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.26it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.26it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.26it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.26it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.26it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.34it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:06,  0.63it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:03<00:04,  0.65it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:04<00:03,  0.65it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:06<00:01,  0.66it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:07<00:00,  0.66it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:25<00:00,  0.10it/s, pixel_AUPRO=0.889]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:25<00:00,  0.10it/s, pixel_AUPRO=0.889]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:03<00:00,  0.08it/s, pixel_AUPRO=0.889]INFO:anomalib.callbacks.timer:Training took 184.59 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:07<00:28,  0.14it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:13<00:20,  0.14it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:20<00:13,  0.15it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:27<00:06,  0.15it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:32<00:00,  0.15it/s]INFO:anomalib.callbacks.timer:Testing took 193.19635343551636 seconds
Throughput (batch_size=32) : 0.7764121699639941 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:11<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9751999974250793     â”‚
â”‚        pixel_AUPRO        â”‚    0.8894824981689453     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa chewinggum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:02<00:37,  0.37it/s]Epoch 0:   7%|â–‹         | 1/15 [00:02<00:37,  0.37it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:05<00:33,  0.39it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:05<00:33,  0.39it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:06<00:26,  0.45it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:06<00:26,  0.45it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:08<00:22,  0.49it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:08<00:22,  0.49it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:10<00:20,  0.49it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:10<00:20,  0.49it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:11<00:17,  0.52it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:11<00:17,  0.52it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:13<00:14,  0.54it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:13<00:14,  0.54it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:15<00:13,  0.52it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:15<00:13,  0.52it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:16<00:11,  0.54it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:16<00:11,  0.54it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:17<00:08,  0.56it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:17<00:08,  0.56it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:20<00:07,  0.54it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:20<00:07,  0.54it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:21<00:05,  0.56it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:21<00:05,  0.56it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:22<00:03,  0.57it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:22<00:03,  0.57it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:24<00:01,  0.56it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:24<00:01,  0.56it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:25<00:00,  0.60it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:25<00:00,  0.60it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.97it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:03<00:05,  0.52it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:06<00:04,  0.50it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:07<00:01,  0.54it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:08<00:00,  0.56it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:08<00:00,  0.08it/s, pixel_AUPRO=nan.0]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:08<00:00,  0.08it/s, pixel_AUPRO=nan.0]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:42<00:00,  0.07it/s, pixel_AUPRO=nan.0]INFO:anomalib.callbacks.timer:Training took 223.57 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:24,  0.16it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:18,  0.16it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:18<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:24<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:29<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 412.5996105670929 seconds
Throughput (batch_size=32) : 0.36354857386761513 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [06:51<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8388000130653381     â”‚
â”‚        pixel_AUPRO        â”‚    0.6617763638496399     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa chewinggum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.98it/s]Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.98it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.26it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.26it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.29it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.29it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.29it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.29it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.26it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.26it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.26it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.26it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.26it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.26it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.26it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.26it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.26it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.26it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.25it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.25it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.26it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.26it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.27it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.27it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.27it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.27it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.27it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.27it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.35it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.35it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.89it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.85it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.75it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.81it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.85it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:48<00:00,  0.05it/s, pixel_AUPRO=0.764]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:48<00:00,  0.05it/s, pixel_AUPRO=0.764]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:22<00:00,  0.05it/s, pixel_AUPRO=0.764]INFO:anomalib.callbacks.timer:Training took 323.27 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:25,  0.16it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:18,  0.16it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:18<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:24<00:06,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:28<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 407.7519052028656 seconds
Throughput (batch_size=32) : 0.3678707520088022 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [06:46<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.885200023651123     â”‚
â”‚        pixel_AUPRO        â”‚    0.7640833854675293     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa chewinggum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.02it/s]Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.02it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:09,  1.31it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:09,  1.31it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.33it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.33it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.33it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.33it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.34it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.34it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.34it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.34it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:05,  1.34it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:05,  1.34it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:05<00:05,  1.34it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:05<00:05,  1.34it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.35it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.35it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.35it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.35it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:02,  1.35it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:02,  1.35it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:08<00:02,  1.35it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:08<00:02,  1.35it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.35it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.35it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.36it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.43it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.43it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.98it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.89it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.84it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.89it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.92it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:55<00:00,  0.05it/s, pixel_AUPRO=0.766]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:55<00:00,  0.05it/s, pixel_AUPRO=0.766]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:42<00:00,  0.04it/s, pixel_AUPRO=0.766]INFO:anomalib.callbacks.timer:Training took 343.18 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:26,  0.15it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:13<00:20,  0.15it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:19<00:13,  0.15it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:26<00:06,  0.15it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:31<00:00,  0.16it/s]INFO:anomalib.callbacks.timer:Testing took 414.53968954086304 seconds
Throughput (batch_size=32) : 0.36184713740230134 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [06:53<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8830000162124634     â”‚
â”‚        pixel_AUPRO        â”‚    0.7660638689994812     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa chewinggum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.97it/s]Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.97it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.21it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.21it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.21it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.21it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:09,  1.21it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:09,  1.21it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:08,  1.20it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:08,  1.20it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.22it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.22it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.22it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.22it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.23it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.23it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.24it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.24it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:08<00:04,  1.24it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:08<00:04,  1.24it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.25it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.25it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.25it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.25it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.25it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.25it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.25it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.25it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.33it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.96it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.86it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.81it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.87it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.90it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:55<00:00,  0.05it/s, pixel_AUPRO=0.767]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:55<00:00,  0.05it/s, pixel_AUPRO=0.767]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:31<00:00,  0.05it/s, pixel_AUPRO=0.767]INFO:anomalib.callbacks.timer:Training took 332.49 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:25,  0.16it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:18,  0.16it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:18<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:24<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:29<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 398.99386858940125 seconds
Throughput (batch_size=32) : 0.3759456267593997 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [06:37<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9952000379562378     â”‚
â”‚        pixel_AUPRO        â”‚     0.767408013343811     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa chewinggum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.95it/s]Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.95it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.23it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.23it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.25it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.25it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.27it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.27it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.27it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.27it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.28it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.28it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.29it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.29it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.29it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.29it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.29it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.29it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.30it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.30it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.30it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.30it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.31it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.31it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.31it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.31it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.31it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.31it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.39it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.39it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.05it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.89it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.83it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.89it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.94it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:50<00:00,  0.05it/s, pixel_AUPRO=0.782]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:50<00:00,  0.05it/s, pixel_AUPRO=0.782]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:25<00:00,  0.05it/s, pixel_AUPRO=0.782]INFO:anomalib.callbacks.timer:Training took 326.03 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:25,  0.16it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:19,  0.16it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:18<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:24<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:29<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 412.07693362236023 seconds
Throughput (batch_size=32) : 0.3640096976101665 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [06:50<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.991599977016449     â”‚
â”‚        pixel_AUPRO        â”‚     0.78184974193573      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa chewinggum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.01it/s]Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.01it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.27it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.27it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.29it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.29it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.29it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.29it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.30it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.30it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.31it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.31it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.31it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.31it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.32it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.32it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.32it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.32it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.32it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.32it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.32it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.32it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.33it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.33it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.33it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.33it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.33it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.41it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.41it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:05,  0.69it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:04,  0.71it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:04<00:02,  0.72it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:05<00:01,  0.72it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:06<00:00,  0.73it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:09<00:00,  0.05it/s, pixel_AUPRO=0.786]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:09<00:00,  0.05it/s, pixel_AUPRO=0.786]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:44<00:00,  0.04it/s, pixel_AUPRO=0.786]INFO:anomalib.callbacks.timer:Training took 345.34 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:27,  0.14it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:13<00:20,  0.15it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:19<00:13,  0.15it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:26<00:06,  0.15it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:31<00:00,  0.16it/s]INFO:anomalib.callbacks.timer:Testing took 408.02433347702026 seconds
Throughput (batch_size=32) : 0.36762513333888686 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [06:46<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9923999309539795     â”‚
â”‚        pixel_AUPRO        â”‚    0.7858366966247559     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:04<00:56,  0.25it/s]Epoch 0:   7%|â–‹         | 1/15 [00:04<00:56,  0.25it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:06<00:40,  0.32it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:06<00:40,  0.32it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:07<00:30,  0.40it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:07<00:30,  0.40it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:09<00:24,  0.44it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:09<00:24,  0.44it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:11<00:22,  0.44it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:11<00:22,  0.44it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:12<00:19,  0.47it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:12<00:19,  0.47it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:14<00:16,  0.50it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:14<00:16,  0.50it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:16<00:14,  0.48it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:16<00:14,  0.48it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:17<00:11,  0.50it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:17<00:11,  0.50it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:19<00:09,  0.52it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:19<00:09,  0.52it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:21<00:07,  0.51it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:21<00:07,  0.51it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:22<00:05,  0.52it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:22<00:05,  0.52it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:24<00:03,  0.53it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:24<00:03,  0.53it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:26<00:01,  0.53it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:26<00:01,  0.53it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:26<00:00,  0.56it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:26<00:00,  0.56it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.17it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:04<00:06,  0.49it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:06<00:04,  0.46it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:09<00:02,  0.44it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:09<00:00,  0.50it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:12<00:00,  0.08it/s, pixel_AUPRO=0.765]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:12<00:00,  0.08it/s, pixel_AUPRO=0.765]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:40<00:00,  0.07it/s, pixel_AUPRO=0.765]INFO:anomalib.callbacks.timer:Training took 221.17 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:24,  0.16it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:18,  0.16it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:18<00:12,  0.17it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:23<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:28<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 266.8121085166931 seconds
Throughput (batch_size=32) : 0.5621933758325486 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [04:25<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8046000003814697     â”‚
â”‚        pixel_AUPRO        â”‚    0.8287172913551331     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.99it/s]Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.99it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.24it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.24it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.24it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.24it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.25it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.25it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.25it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.25it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.26it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.26it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.26it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.26it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.27it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.27it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.27it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.27it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.27it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.27it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.27it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.27it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.27it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.27it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.27it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.27it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.27it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.27it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.36it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.18it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.96it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.87it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.94it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.00it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:38<00:00,  0.07it/s, pixel_AUPRO=0.801]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:38<00:00,  0.07it/s, pixel_AUPRO=0.801]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:06<00:00,  0.06it/s, pixel_AUPRO=0.801]INFO:anomalib.callbacks.timer:Training took 247.04 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:05<00:23,  0.17it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:11<00:17,  0.18it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:16<00:11,  0.18it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:22<00:05,  0.18it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:26<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 264.25163745880127 seconds
Throughput (batch_size=32) : 0.5676407587952452 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [04:22<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8503999710083008     â”‚
â”‚        pixel_AUPRO        â”‚    0.8012158870697021     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.01it/s]Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.01it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.30it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.30it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.30it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.30it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.26it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.26it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:08,  1.23it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:08,  1.23it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.24it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.24it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.23it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.23it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.22it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.22it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.22it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.22it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:08<00:04,  1.23it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:08<00:04,  1.23it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.24it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.24it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.25it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.25it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.25it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.25it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.26it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:11<00:00,  1.26it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:11<00:00,  1.34it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.18it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.97it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.87it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.94it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.00it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:41<00:00,  0.07it/s, pixel_AUPRO=0.806]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:41<00:00,  0.07it/s, pixel_AUPRO=0.806]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:09<00:00,  0.06it/s, pixel_AUPRO=0.806]INFO:anomalib.callbacks.timer:Training took 249.98 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:05<00:23,  0.17it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:11<00:17,  0.17it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:16<00:11,  0.18it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:22<00:05,  0.18it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:26<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 265.76020407676697 seconds
Throughput (batch_size=32) : 0.5644185912676049 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [04:24<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8540000319480896     â”‚
â”‚        pixel_AUPRO        â”‚    0.8060246706008911     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.98it/s]Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.98it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.25it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.25it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.27it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.27it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.28it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.28it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.29it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.29it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.30it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.30it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.31it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.31it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.31it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.31it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.31it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.31it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.31it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.31it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.31it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.31it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.31it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.31it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.31it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.31it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.31it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.31it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.40it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.40it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.14it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.94it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.85it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.92it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.98it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:42<00:00,  0.07it/s, pixel_AUPRO=0.816]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:42<00:00,  0.07it/s, pixel_AUPRO=0.816]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:10<00:00,  0.06it/s, pixel_AUPRO=0.816]INFO:anomalib.callbacks.timer:Training took 251.06 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:05<00:23,  0.17it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:11<00:17,  0.17it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:16<00:11,  0.18it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:22<00:05,  0.18it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:26<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 266.10627937316895 seconds
Throughput (batch_size=32) : 0.5636845562357077 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [04:24<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8931999802589417     â”‚
â”‚        pixel_AUPRO        â”‚    0.8154991865158081     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.00it/s]Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.00it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.30it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.30it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.32it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.32it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:02<00:08,  1.33it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.33it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.34it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.34it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.34it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.34it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:05,  1.35it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:05,  1.35it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:05<00:05,  1.35it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:05<00:05,  1.35it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.35it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.35it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.35it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.35it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:02,  1.35it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:02,  1.35it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:08<00:02,  1.35it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:08<00:02,  1.35it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.35it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.35it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.35it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.35it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.44it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.44it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.32it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  1.00it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.88it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.94it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.02it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:48<00:00,  0.07it/s, pixel_AUPRO=0.820]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:48<00:00,  0.07it/s, pixel_AUPRO=0.820]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:16<00:00,  0.06it/s, pixel_AUPRO=0.820]INFO:anomalib.callbacks.timer:Training took 257.28 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:24,  0.17it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:18,  0.16it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:18<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:24<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:28<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 262.55643248558044 seconds
Throughput (batch_size=32) : 0.5713057516053733 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [04:21<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8988000154495239     â”‚
â”‚        pixel_AUPRO        â”‚    0.8200944662094116     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.01it/s]Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.01it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.29it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.29it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.31it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.31it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.32it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.32it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.32it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.32it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.32it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.32it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.33it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.33it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.33it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.33it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.33it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.33it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.33it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.33it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.33it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.33it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.33it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.33it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.33it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.33it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.33it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.42it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.42it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.87it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.83it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.79it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.83it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.85it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:50<00:00,  0.07it/s, pixel_AUPRO=0.845]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [03:50<00:00,  0.07it/s, pixel_AUPRO=0.845]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:17<00:00,  0.06it/s, pixel_AUPRO=0.845]INFO:anomalib.callbacks.timer:Training took 258.60 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:24,  0.16it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:19,  0.15it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:19<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:25<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:29<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 266.6200063228607 seconds
Throughput (batch_size=32) : 0.5625984413876244 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [04:25<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.913599967956543     â”‚
â”‚        pixel_AUPRO        â”‚    0.8446060419082642     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:02<01:10,  0.40it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:02<01:10,  0.40it/s]Epoch 0:   7%|â–‹         | 2/29 [00:03<00:52,  0.52it/s]Epoch 0:   7%|â–‹         | 2/29 [00:03<00:52,  0.52it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:06<00:52,  0.49it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:06<00:52,  0.49it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:07<00:45,  0.55it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:07<00:45,  0.55it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:08<00:41,  0.58it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:08<00:41,  0.58it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:11<00:42,  0.54it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:11<00:42,  0.54it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:12<00:39,  0.56it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:12<00:39,  0.56it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:13<00:35,  0.59it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:13<00:35,  0.59it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:15<00:35,  0.57it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:15<00:35,  0.57it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:17<00:32,  0.58it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:17<00:32,  0.58it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:18<00:30,  0.60it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:18<00:30,  0.60it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:20<00:29,  0.58it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:20<00:29,  0.58it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:21<00:26,  0.59it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:21<00:26,  0.59it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:23<00:24,  0.61it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:23<00:24,  0.61it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:25<00:23,  0.59it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:25<00:23,  0.59it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:26<00:21,  0.60it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:26<00:21,  0.60it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:27<00:19,  0.61it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:27<00:19,  0.61it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:28<00:17,  0.62it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:28<00:17,  0.62it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:31<00:16,  0.61it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:31<00:16,  0.61it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:32<00:14,  0.62it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:32<00:14,  0.62it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:33<00:12,  0.62it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:33<00:12,  0.62it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:35<00:11,  0.61it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:35<00:11,  0.61it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:37<00:09,  0.62it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:37<00:09,  0.62it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:38<00:08,  0.62it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:38<00:08,  0.62it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:40<00:06,  0.61it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:40<00:06,  0.61it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:41<00:04,  0.62it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:41<00:04,  0.62it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:43<00:03,  0.63it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:43<00:03,  0.63it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:44<00:01,  0.63it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:44<00:01,  0.63it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:44<00:00,  0.65it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:44<00:00,  0.65it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.17it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:06,  0.81it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:05<00:07,  0.57it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:06<00:04,  0.60it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:09<00:03,  0.54it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:10<00:01,  0.57it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:11<00:00,  0.62it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:42<00:00,  0.10it/s, pixel_AUPRO=0.895]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:42<00:00,  0.10it/s, pixel_AUPRO=0.895]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:10<00:00,  0.09it/s, pixel_AUPRO=0.895]INFO:anomalib.callbacks.timer:Training took 311.79 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:36,  0.17it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:29,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:23,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:22<00:17,  0.18it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:28<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:34<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:36<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 249.26806688308716 seconds
Throughput (batch_size=32) : 0.8023490634033156 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [04:07<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.698199987411499     â”‚
â”‚        pixel_AUPRO        â”‚    0.9071648120880127     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.33it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.33it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.35it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.35it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.36it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.36it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.36it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.36it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:16,  1.36it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:16,  1.36it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.36it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.36it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.37it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.37it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.37it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.37it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:13,  1.37it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:13,  1.37it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.37it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.37it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.37it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.37it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:10,  1.38it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:10,  1.38it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:10<00:10,  1.38it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:10<00:10,  1.38it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.38it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.38it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.37it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.37it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:07,  1.38it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:07,  1.38it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.38it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.38it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.37it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.37it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.37it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.37it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.37it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.37it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.37it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.37it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.38it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.38it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.38it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.38it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:18<00:02,  1.38it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:18<00:02,  1.38it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.38it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.38it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.38it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.38it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.42it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.42it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.08it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.94it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.85it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.93it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.99it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:05<00:00,  1.03it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.07it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:20<00:00,  0.09it/s, pixel_AUPRO=0.890]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:20<00:00,  0.09it/s, pixel_AUPRO=0.890]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:47<00:00,  0.08it/s, pixel_AUPRO=0.890]INFO:anomalib.callbacks.timer:Training took 348.51 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:05<00:34,  0.17it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:28,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:23,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:23<00:17,  0.17it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:29<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:35<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 343.53928399086 seconds
Throughput (batch_size=32) : 0.5821750504822065 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [05:42<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7021999359130859     â”‚
â”‚        pixel_AUPRO        â”‚    0.8895091414451599     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.03it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.03it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.31it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.31it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.32it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.32it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:18,  1.33it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:18,  1.33it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.33it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.33it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.34it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.34it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.34it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.34it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.35it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.35it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.35it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.35it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.35it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.35it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.35it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.35it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.35it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.35it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.35it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.35it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.35it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.35it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.35it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.35it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.35it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.35it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.35it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.35it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.36it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.36it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.36it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.36it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.36it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.36it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.36it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.36it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.36it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.36it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.36it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.36it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.36it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.36it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.36it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.36it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.36it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.36it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.37it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.37it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.37it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.37it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.41it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.41it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.17it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.96it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.88it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.95it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:04<00:01,  1.01it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:05<00:00,  1.05it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.09it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:26<00:00,  0.09it/s, pixel_AUPRO=0.881]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:26<00:00,  0.09it/s, pixel_AUPRO=0.881]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:54<00:00,  0.08it/s, pixel_AUPRO=0.881]INFO:anomalib.callbacks.timer:Training took 355.19 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:05<00:35,  0.17it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:29,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:22,  0.18it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:22<00:16,  0.18it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:28<00:11,  0.18it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:34<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:36<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 303.377876996994 seconds
Throughput (batch_size=32) : 0.6592438511987533 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [05:01<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.6975000500679016     â”‚
â”‚        pixel_AUPRO        â”‚    0.8812823295593262     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.01it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.01it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.31it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.31it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.31it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.31it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:18,  1.32it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:18,  1.32it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.33it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.33it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.34it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.34it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.35it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.34it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.35it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.35it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.35it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.35it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.35it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.35it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.35it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.35it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.35it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.35it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.35it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.35it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.35it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.35it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.35it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.35it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.35it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.35it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.35it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.35it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.35it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.35it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.36it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.36it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.36it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.36it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.36it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.36it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.36it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.36it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.36it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.36it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.36it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.36it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.36it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.36it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.36it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.36it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.36it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.36it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.36it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.40it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.40it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.17it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.95it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.87it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.94it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:04<00:01,  1.00it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:05<00:00,  1.05it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.08it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:28<00:00,  0.09it/s, pixel_AUPRO=0.887]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:28<00:00,  0.09it/s, pixel_AUPRO=0.887]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:56<00:00,  0.08it/s, pixel_AUPRO=0.887]INFO:anomalib.callbacks.timer:Training took 357.27 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:05<00:35,  0.17it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:29,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:22,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:22<00:17,  0.18it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:28<00:11,  0.18it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:34<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 331.7459456920624 seconds
Throughput (batch_size=32) : 0.60287096978013 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [05:30<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8499000072479248     â”‚
â”‚        pixel_AUPRO        â”‚    0.8865618109703064     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.98it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.98it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.27it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.27it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.29it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.29it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.30it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.30it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.31it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.31it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.32it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.32it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.32it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.32it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:15,  1.32it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:15,  1.32it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:15,  1.33it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:15,  1.33it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.32it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.32it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.33it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.33it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:12,  1.33it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:12,  1.33it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:12,  1.33it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:12,  1.33it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.33it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.33it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.33it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.33it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.34it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.34it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.34it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.34it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.34it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.34it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.34it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.34it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.34it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.34it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.34it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.34it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.34it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.34it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.34it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.34it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.34it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.34it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.34it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.34it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.34it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.34it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.34it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.38it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.38it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:04,  1.32it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.99it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.89it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.95it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:04<00:01,  1.01it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:05<00:00,  1.06it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.11it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:31<00:00,  0.09it/s, pixel_AUPRO=0.896]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:31<00:00,  0.09it/s, pixel_AUPRO=0.896]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:59<00:00,  0.08it/s, pixel_AUPRO=0.896]INFO:anomalib.callbacks.timer:Training took 360.80 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:37,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:30,  0.16it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:23,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:23<00:17,  0.17it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:29<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:34<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:36<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 294.35272455215454 seconds
Throughput (batch_size=32) : 0.6794569348875289 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [04:52<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8578499555587769     â”‚
â”‚        pixel_AUPRO        â”‚    0.8959689736366272     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.01it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.01it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.28it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.28it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.29it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.29it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.30it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.30it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.31it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.31it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.32it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.32it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.32it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.32it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:15,  1.32it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:15,  1.32it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:15,  1.33it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:15,  1.33it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.33it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.33it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.33it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.33it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:12,  1.33it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:12,  1.33it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:12,  1.33it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:12,  1.33it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.33it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.33it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.33it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.33it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.33it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.33it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.34it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.34it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.34it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.34it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.34it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.34it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.34it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.34it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.34it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.34it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.34it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.34it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.34it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.34it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.34it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.34it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.34it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.34it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.34it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.34it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.34it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.38it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.38it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.87it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:06,  0.81it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:05,  0.77it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.81it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:06<00:02,  0.83it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:07<00:01,  0.85it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:08<00:00,  0.86it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:36<00:00,  0.09it/s, pixel_AUPRO=0.923]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:36<00:00,  0.09it/s, pixel_AUPRO=0.923]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [06:04<00:00,  0.08it/s, pixel_AUPRO=0.923]INFO:anomalib.callbacks.timer:Training took 365.59 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:38,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:31,  0.16it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:18<00:24,  0.16it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:24<00:18,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:37<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 314.6107246875763 seconds
Throughput (batch_size=32) : 0.6357062372829461 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [05:13<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9185999631881714     â”‚
â”‚        pixel_AUPRO        â”‚    0.9227983355522156     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:04<01:59,  0.23it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:04<01:59,  0.23it/s]Epoch 0:   7%|â–‹         | 2/29 [00:05<01:12,  0.37it/s]Epoch 0:   7%|â–‹         | 2/29 [00:05<01:12,  0.37it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:07<01:06,  0.39it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:07<01:06,  0.39it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:08<00:55,  0.45it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:08<00:55,  0.45it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:10<00:49,  0.49it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:10<00:49,  0.49it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:12<00:48,  0.48it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:12<00:48,  0.48it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:13<00:43,  0.50it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:13<00:43,  0.50it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:15<00:39,  0.53it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:15<00:39,  0.53it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:17<00:39,  0.51it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:17<00:39,  0.51it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:18<00:36,  0.53it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:18<00:36,  0.53it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:20<00:33,  0.54it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:20<00:33,  0.54it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:22<00:31,  0.53it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:22<00:31,  0.53it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:23<00:29,  0.55it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:23<00:29,  0.55it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:24<00:26,  0.56it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:24<00:26,  0.56it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:26<00:24,  0.57it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:26<00:24,  0.57it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:28<00:23,  0.56it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:28<00:23,  0.56it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:29<00:21,  0.57it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:29<00:21,  0.57it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:31<00:18,  0.58it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:31<00:18,  0.58it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:33<00:17,  0.57it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:33<00:17,  0.57it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:34<00:15,  0.58it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:34<00:15,  0.58it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:36<00:13,  0.58it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:36<00:13,  0.58it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:38<00:12,  0.57it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:38<00:12,  0.57it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:39<00:10,  0.58it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:39<00:10,  0.58it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:40<00:08,  0.59it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:40<00:08,  0.59it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:43<00:06,  0.58it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:43<00:06,  0.58it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:44<00:05,  0.58it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:44<00:05,  0.58it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:45<00:03,  0.59it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:45<00:03,  0.59it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:47<00:01,  0.59it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:47<00:01,  0.59it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:48<00:00,  0.60it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:48<00:00,  0.60it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.15it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:03<00:09,  0.52it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:05<00:07,  0.51it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:08<00:06,  0.48it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:09<00:03,  0.52it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:10<00:01,  0.55it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:11<00:00,  0.60it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [03:34<00:00,  0.14it/s, pixel_AUPRO=nan.0]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [03:34<00:00,  0.14it/s, pixel_AUPRO=nan.0]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:02<00:00,  0.12it/s, pixel_AUPRO=nan.0]INFO:anomalib.callbacks.timer:Training took 243.63 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:36,  0.17it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:31,  0.16it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:18<00:24,  0.16it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:24<00:18,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:30<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:36<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 201.61342906951904 seconds
Throughput (batch_size=32) : 0.9919974126874123 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [03:20<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7731499671936035     â”‚
â”‚        pixel_AUPRO        â”‚    0.7501596808433533     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.01it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.01it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.29it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.29it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.30it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.30it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.31it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.31it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.28it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.28it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.27it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.27it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.29it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.29it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.29it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.29it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:15,  1.30it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:15,  1.30it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.30it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.30it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.30it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:12,  1.31it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:12,  1.31it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:12,  1.31it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:12,  1.31it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.32it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.32it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.32it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.32it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:09,  1.32it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:09,  1.32it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:09,  1.32it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:09,  1.32it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.33it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.33it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.33it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.33it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.33it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.33it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:06,  1.33it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:06,  1.33it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.33it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.33it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.33it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.33it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.33it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.33it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.34it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.34it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.34it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.34it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.34it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.38it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.38it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.13it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.90it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.80it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.87it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.93it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.98it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.02it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:13<00:00,  0.11it/s, pixel_AUPRO=0.713]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:13<00:00,  0.11it/s, pixel_AUPRO=0.713]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:41<00:00,  0.10it/s, pixel_AUPRO=0.713]INFO:anomalib.callbacks.timer:Training took 282.37 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:05<00:34,  0.17it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:28,  0.18it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:16<00:22,  0.18it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:21<00:16,  0.18it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:27<00:10,  0.18it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:32<00:05,  0.18it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:34<00:00,  0.20it/s]INFO:anomalib.callbacks.timer:Testing took 212.79350662231445 seconds
Throughput (batch_size=32) : 0.9398783035000144 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [03:31<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7780500054359436     â”‚
â”‚        pixel_AUPRO        â”‚    0.7119712829589844     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.99it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.99it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.28it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.28it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.30it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.30it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.31it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.31it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.32it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.32it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.32it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.32it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.33it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.33it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:15,  1.32it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:15,  1.32it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:15,  1.32it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:15,  1.32it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.32it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.32it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.33it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.33it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:12,  1.33it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:12,  1.33it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:12,  1.33it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:12,  1.33it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.33it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.33it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.33it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.33it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:09,  1.33it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:09,  1.33it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:09,  1.33it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:09,  1.33it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.33it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.33it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.33it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.33it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.33it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.33it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.33it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.33it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.33it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.33it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.33it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.33it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.33it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.33it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.33it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.33it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.34it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.34it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.34it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.38it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.38it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.12it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.85it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.82it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.89it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.94it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.99it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.02it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:19<00:00,  0.11it/s, pixel_AUPRO=0.691]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:19<00:00,  0.11it/s, pixel_AUPRO=0.691]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:48<00:00,  0.10it/s, pixel_AUPRO=0.691]INFO:anomalib.callbacks.timer:Training took 289.17 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:36,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:31,  0.16it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:18<00:25,  0.16it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:24<00:18,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:37<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 290.95643067359924 seconds
Throughput (batch_size=32) : 0.6873881410250183 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [04:49<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7724000215530396     â”‚
â”‚        pixel_AUPRO        â”‚    0.6899771690368652     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.01it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.01it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.31it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.31it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.33it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.33it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.35it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.35it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.35it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.35it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.35it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.35it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.35it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.35it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.35it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.35it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.35it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.35it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.35it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.35it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.36it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.36it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.36it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.36it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.36it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.36it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.36it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.36it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.36it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.36it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.36it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.36it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.36it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.36it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.36it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.36it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.36it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.36it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.36it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.36it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.36it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.36it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.36it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.36it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.36it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.36it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.36it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.36it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.36it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.36it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.36it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.36it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.36it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.36it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.36it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.41it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.41it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.17it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.91it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.84it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.91it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.98it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:05<00:00,  1.03it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.07it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:20<00:00,  0.11it/s, pixel_AUPRO=0.709]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:20<00:00,  0.11it/s, pixel_AUPRO=0.709]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:49<00:00,  0.10it/s, pixel_AUPRO=0.709]INFO:anomalib.callbacks.timer:Training took 290.30 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:05<00:34,  0.17it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:28,  0.18it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:16<00:22,  0.18it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:22<00:16,  0.18it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:27<00:11,  0.18it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:32<00:05,  0.18it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:34<00:00,  0.20it/s]INFO:anomalib.callbacks.timer:Testing took 271.60404539108276 seconds
Throughput (batch_size=32) : 0.7363660571108944 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [04:30<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.696399986743927     â”‚
â”‚        pixel_AUPRO        â”‚     0.70745849609375      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.32it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.32it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.34it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.34it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.35it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.35it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.36it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.36it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:16,  1.36it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:16,  1.36it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.36it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.36it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.36it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.36it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.36it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.36it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:13,  1.36it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:13,  1.36it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.37it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.37it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.37it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.37it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:10,  1.37it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:10,  1.37it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:10<00:10,  1.37it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:10<00:10,  1.37it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.37it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.37it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.37it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.37it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.37it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.37it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.37it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.37it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.38it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.38it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.38it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.38it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:15<00:05,  1.38it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:15<00:05,  1.38it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.38it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.38it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.38it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.38it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.38it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.38it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:18<00:02,  1.38it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:18<00:02,  1.38it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.38it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.38it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.38it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.38it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.43it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.43it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:04,  1.33it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:04,  1.01it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.90it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.96it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:04<00:01,  1.02it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:05<00:00,  1.07it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.11it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:21<00:00,  0.11it/s, pixel_AUPRO=0.728]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:21<00:00,  0.11it/s, pixel_AUPRO=0.728]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:51<00:00,  0.10it/s, pixel_AUPRO=0.728]INFO:anomalib.callbacks.timer:Training took 292.20 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:05<00:34,  0.17it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:28,  0.18it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:23,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:23<00:17,  0.17it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:29<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:35<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 252.5008647441864 seconds
Throughput (batch_size=32) : 0.7920764952730912 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [04:11<00:00,  0.03it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.6553499698638916     â”‚
â”‚        pixel_AUPRO        â”‚    0.7281581163406372     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa macaroni2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.31it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.31it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.32it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.32it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:18,  1.33it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:18,  1.33it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.34it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.34it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.34it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.34it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.34it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.34it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.34it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.34it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.34it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.34it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.35it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.35it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.35it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.35it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.35it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.35it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.35it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.35it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.35it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.35it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.35it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.35it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.35it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.35it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.35it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.35it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.35it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.35it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.35it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.35it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.35it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.35it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.35it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.35it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.35it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.35it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.36it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.36it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.36it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.36it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.36it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.36it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.36it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.36it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.36it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.36it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.36it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.40it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.40it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.87it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:06,  0.81it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:05,  0.77it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.80it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:06<00:02,  0.83it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:07<00:01,  0.84it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:08<00:00,  0.86it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:36<00:00,  0.10it/s, pixel_AUPRO=0.825]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:36<00:00,  0.10it/s, pixel_AUPRO=0.825]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:05<00:00,  0.10it/s, pixel_AUPRO=0.825]INFO:anomalib.callbacks.timer:Training took 305.83 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:39,  0.15it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:32,  0.15it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:18<00:25,  0.16it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:25<00:18,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:37<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 173.02186965942383 seconds
Throughput (batch_size=32) : 1.1559232390314584 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [02:51<00:00,  0.04it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7583000063896179     â”‚
â”‚        pixel_AUPRO        â”‚    0.8003318905830383     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:05<02:43,  0.17it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:05<02:43,  0.17it/s]Epoch 0:   7%|â–‹         | 2/29 [00:08<01:49,  0.25it/s]Epoch 0:   7%|â–‹         | 2/29 [00:08<01:49,  0.25it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:09<01:23,  0.31it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:09<01:23,  0.31it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:11<01:11,  0.35it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:11<01:11,  0.35it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:14<01:07,  0.36it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:14<01:07,  0.36it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:15<00:59,  0.39it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:15<00:59,  0.39it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:18<00:57,  0.38it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:18<00:57,  0.38it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:19<00:52,  0.40it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:19<00:52,  0.40it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:21<00:47,  0.42it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:21<00:47,  0.42it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:24<00:46,  0.41it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:24<00:46,  0.41it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:26<00:42,  0.42it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:26<00:42,  0.42it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:28<00:40,  0.41it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:28<00:40,  0.41it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:30<00:37,  0.43it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:30<00:37,  0.43it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:32<00:35,  0.42it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:32<00:35,  0.42it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:34<00:32,  0.43it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:34<00:32,  0.43it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:36<00:29,  0.44it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:36<00:29,  0.44it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:38<00:27,  0.44it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:38<00:27,  0.44it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:40<00:24,  0.45it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:40<00:24,  0.45it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:41<00:22,  0.45it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:41<00:22,  0.45it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:44<00:20,  0.45it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:44<00:20,  0.45it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:46<00:17,  0.46it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:46<00:17,  0.46it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:48<00:15,  0.45it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:48<00:15,  0.45it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:50<00:13,  0.46it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:50<00:13,  0.46it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:51<00:10,  0.46it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:51<00:10,  0.46it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:54<00:08,  0.46it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:54<00:08,  0.46it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:55<00:06,  0.46it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:55<00:06,  0.46it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:58<00:04,  0.46it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:58<00:04,  0.46it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [01:00<00:02,  0.46it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [01:00<00:02,  0.46it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:00<00:00,  0.48it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [01:00<00:00,  0.48it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.02it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:03<00:08,  0.60it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:06<00:09,  0.43it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:09<00:07,  0.40it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:11<00:04,  0.44it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:12<00:02,  0.47it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:13<00:00,  0.52it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [03:42<00:00,  0.13it/s, pixel_AUPRO=0.624]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [03:42<00:00,  0.13it/s, pixel_AUPRO=0.624]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:14<00:00,  0.11it/s, pixel_AUPRO=0.624]INFO:anomalib.callbacks.timer:Training took 255.02 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:36,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:29,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:23,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:23<00:17,  0.17it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:30<00:12,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:35<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 346.82980132102966 seconds
Throughput (batch_size=32) : 0.5766517157355739 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [05:45<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.819350004196167     â”‚
â”‚        pixel_AUPRO        â”‚    0.8739853501319885     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:29,  0.96it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:29,  0.96it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.20it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.20it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:21,  1.21it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:21,  1.21it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:20,  1.20it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:20,  1.20it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:04<00:20,  1.19it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:04<00:20,  1.19it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:05<00:19,  1.19it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:05<00:19,  1.19it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:18,  1.19it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:18,  1.19it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:17,  1.18it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:17,  1.18it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:16,  1.18it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:16,  1.18it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:08<00:16,  1.18it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:08<00:16,  1.18it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:09<00:15,  1.19it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:09<00:15,  1.19it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:10<00:14,  1.19it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:10<00:14,  1.19it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:13,  1.19it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:13,  1.19it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:12,  1.19it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:12,  1.19it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:12<00:11,  1.19it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:12<00:11,  1.19it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:13<00:10,  1.20it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:13<00:10,  1.20it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:14<00:09,  1.20it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:14<00:09,  1.20it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:09,  1.20it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:09,  1.20it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:15<00:08,  1.21it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:15<00:08,  1.21it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:16<00:07,  1.21it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:16<00:07,  1.21it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:17<00:06,  1.21it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:17<00:06,  1.21it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:18<00:05,  1.21it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:18<00:05,  1.21it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:18<00:04,  1.22it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:18<00:04,  1.22it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:19<00:04,  1.22it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:19<00:04,  1.22it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:20<00:03,  1.22it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:20<00:03,  1.22it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:21<00:02,  1.22it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:21<00:02,  1.22it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:22<00:01,  1.22it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:22<00:01,  1.22it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:22<00:00,  1.22it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:22<00:00,  1.22it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:23<00:00,  1.26it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:23<00:00,  1.26it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.03it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.89it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.81it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.88it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.92it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.96it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  0.99it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:26<00:00,  0.11it/s, pixel_AUPRO=0.858]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:26<00:00,  0.11it/s, pixel_AUPRO=0.858]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:59<00:00,  0.10it/s, pixel_AUPRO=0.858]INFO:anomalib.callbacks.timer:Training took 300.00 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:36,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:29,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:18<00:24,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:24<00:18,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:37<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 354.18423438072205 seconds
Throughput (batch_size=32) : 0.5646778726605168 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [05:52<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8240500092506409     â”‚
â”‚        pixel_AUPRO        â”‚    0.8577393293380737     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.98it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.98it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.23it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.23it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.25it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.25it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.27it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.27it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.28it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.28it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.29it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.29it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.30it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.30it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.30it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.30it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:15,  1.31it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:15,  1.31it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.30it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.30it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.30it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.30it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.30it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:12,  1.31it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:12,  1.31it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.31it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.31it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.31it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.31it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:09,  1.31it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:09,  1.31it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:09,  1.31it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:09,  1.31it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.31it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.31it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.31it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.31it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.32it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.32it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:06,  1.32it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:06,  1.32it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.32it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.32it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.32it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.32it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.32it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.32it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:03,  1.32it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:03,  1.32it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.32it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.32it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.32it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.32it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.33it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.36it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.04it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.90it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.83it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.90it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.94it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.98it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.00it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:24<00:00,  0.11it/s, pixel_AUPRO=0.866]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:24<00:00,  0.11it/s, pixel_AUPRO=0.866]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:56<00:00,  0.10it/s, pixel_AUPRO=0.866]INFO:anomalib.callbacks.timer:Training took 297.75 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:37,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:31,  0.16it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:18<00:25,  0.16it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:25<00:18,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:37<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 352.4707136154175 seconds
Throughput (batch_size=32) : 0.5674230291320628 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [05:50<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.813800036907196     â”‚
â”‚        pixel_AUPRO        â”‚    0.8660359978675842     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.98it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.98it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.23it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.23it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.26it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.26it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.27it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.27it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.28it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.28it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.28it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.28it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.28it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.28it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.28it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.28it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.28it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.28it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.28it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.28it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.28it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.28it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.28it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.28it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.28it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.28it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.28it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.28it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.29it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.29it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.29it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.29it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.29it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.29it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.29it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.29it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.29it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.29it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.29it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.29it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.29it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.29it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.29it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.29it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.29it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.29it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.29it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.29it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.29it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.29it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.30it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.30it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.30it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.30it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.30it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.30it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.33it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.05it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.91it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.84it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.91it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.95it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.99it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.01it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:28<00:00,  0.11it/s, pixel_AUPRO=0.847]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:28<00:00,  0.11it/s, pixel_AUPRO=0.847]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:01<00:00,  0.10it/s, pixel_AUPRO=0.847]INFO:anomalib.callbacks.timer:Training took 302.18 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:37,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:30,  0.16it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:23,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:23<00:17,  0.17it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:29<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:34<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 348.05731773376465 seconds
Throughput (batch_size=32) : 0.5746180005701924 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [05:46<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9143000245094299     â”‚
â”‚        pixel_AUPRO        â”‚    0.8469929099082947     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.97it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.97it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.24it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.24it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.26it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.26it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.26it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.26it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.27it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.27it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.28it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.28it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.28it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.28it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.28it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.28it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.28it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.28it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.28it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.28it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.28it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.28it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.28it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.28it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.28it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.28it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.28it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.28it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.28it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.28it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.28it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.28it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.28it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.28it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.28it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.28it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.28it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.28it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:07,  1.29it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:07,  1.29it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.29it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.29it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.29it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.29it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.29it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.29it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.29it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.29it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.29it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.29it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.29it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.29it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.29it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.29it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.29it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.29it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.32it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.32it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:04,  1.21it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.94it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.81it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.85it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.88it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.91it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  0.96it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:26<00:00,  0.11it/s, pixel_AUPRO=0.851]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:26<00:00,  0.11it/s, pixel_AUPRO=0.851]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:58<00:00,  0.10it/s, pixel_AUPRO=0.851]INFO:anomalib.callbacks.timer:Training took 299.75 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:36,  0.17it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:30,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:23,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:24<00:18,  0.17it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:30<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:36<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 348.8945984840393 seconds
Throughput (batch_size=32) : 0.5732390265398428 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [05:47<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9171000123023987     â”‚
â”‚        pixel_AUPRO        â”‚    0.8510738611221313     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb1 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.99it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.99it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.25it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.25it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.27it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.27it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:20,  1.22it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:20,  1.22it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:04<00:20,  1.19it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:04<00:20,  1.19it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:05<00:19,  1.17it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:05<00:19,  1.17it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:06<00:19,  1.15it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:06<00:19,  1.15it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:18,  1.16it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:18,  1.16it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:17,  1.17it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:17,  1.17it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:08<00:16,  1.18it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:08<00:16,  1.18it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:09<00:15,  1.19it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:09<00:15,  1.19it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:10<00:14,  1.19it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:10<00:14,  1.19it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:13,  1.20it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:13,  1.20it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:12,  1.20it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:12,  1.20it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:12<00:11,  1.20it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:12<00:11,  1.20it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:13<00:10,  1.21it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:13<00:10,  1.21it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:14<00:09,  1.21it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:14<00:09,  1.21it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:09,  1.21it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:09,  1.21it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:15<00:08,  1.21it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:15<00:08,  1.21it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:16<00:07,  1.22it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:16<00:07,  1.22it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:17<00:06,  1.22it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:17<00:06,  1.22it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:18<00:05,  1.22it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:18<00:05,  1.22it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:18<00:04,  1.22it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:18<00:04,  1.22it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:19<00:04,  1.22it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:19<00:04,  1.22it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:20<00:03,  1.23it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:20<00:03,  1.23it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:21<00:02,  1.23it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:21<00:02,  1.23it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:21<00:01,  1.23it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:21<00:01,  1.23it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:22<00:00,  1.23it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:22<00:00,  1.23it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:22<00:00,  1.26it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:22<00:00,  1.26it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:07,  0.76it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:06,  0.77it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:05,  0.75it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:05<00:03,  0.77it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:06<00:02,  0.78it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:07<00:01,  0.78it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:08<00:00,  0.79it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:30<00:00,  0.11it/s, pixel_AUPRO=0.891]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:30<00:00,  0.11it/s, pixel_AUPRO=0.891]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:03<00:00,  0.10it/s, pixel_AUPRO=0.891]INFO:anomalib.callbacks.timer:Training took 303.98 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:39,  0.15it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:32,  0.16it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:18<00:25,  0.16it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:24<00:18,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:37<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:40<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 350.694296836853 seconds
Throughput (batch_size=32) : 0.5702972697415786 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [05:49<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.947100043296814     â”‚
â”‚        pixel_AUPRO        â”‚    0.8905647993087769     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:03<01:41,  0.28it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:03<01:41,  0.28it/s]Epoch 0:   7%|â–‹         | 2/29 [00:06<01:23,  0.32it/s]Epoch 0:   7%|â–‹         | 2/29 [00:06<01:23,  0.32it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:07<01:06,  0.39it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:07<01:06,  0.39it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:10<01:05,  0.38it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:10<01:05,  0.38it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:12<00:57,  0.42it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:12<00:57,  0.42it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:13<00:52,  0.44it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:13<00:52,  0.44it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:16<00:50,  0.44it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:16<00:50,  0.44it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:17<00:46,  0.46it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:17<00:46,  0.46it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:18<00:42,  0.47it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:18<00:42,  0.47it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:21<00:41,  0.46it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:21<00:41,  0.46it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:23<00:37,  0.48it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:23<00:37,  0.48it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:25<00:36,  0.47it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:25<00:36,  0.47it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:27<00:33,  0.48it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:27<00:33,  0.48it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:28<00:30,  0.49it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:28<00:30,  0.49it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:31<00:29,  0.48it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:31<00:29,  0.48it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:32<00:26,  0.49it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:32<00:26,  0.49it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:34<00:24,  0.50it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:34<00:24,  0.50it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:36<00:22,  0.49it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:36<00:22,  0.49it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:38<00:20,  0.50it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:38<00:20,  0.50it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:40<00:18,  0.49it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:40<00:18,  0.49it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:42<00:16,  0.50it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:42<00:16,  0.50it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:43<00:13,  0.50it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:43<00:13,  0.50it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:46<00:12,  0.50it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:46<00:12,  0.50it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:47<00:09,  0.50it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:47<00:09,  0.50it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:49<00:07,  0.51it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:49<00:07,  0.51it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:51<00:05,  0.50it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:51<00:05,  0.50it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:53<00:03,  0.51it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:53<00:03,  0.51it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:55<00:01,  0.51it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:55<00:01,  0.51it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:56<00:00,  0.52it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:56<00:00,  0.52it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.01it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:04<00:10,  0.48it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:06<00:08,  0.46it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:09<00:06,  0.43it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:10<00:04,  0.47it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:11<00:01,  0.50it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:13<00:00,  0.53it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [03:54<00:00,  0.12it/s, pixel_AUPRO=0.671]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [03:54<00:00,  0.12it/s, pixel_AUPRO=0.671]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:27<00:00,  0.11it/s, pixel_AUPRO=0.671]INFO:anomalib.callbacks.timer:Training took 268.01 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:37,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:30,  0.16it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:18<00:24,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:23<00:17,  0.17it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:29<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:36<00:06,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 389.652095079422 seconds
Throughput (batch_size=32) : 0.513278389942275 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [06:28<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.733199954032898     â”‚
â”‚        pixel_AUPRO        â”‚     0.883690595626831     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:29,  0.95it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:29,  0.95it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.21it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.21it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:21,  1.23it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:21,  1.23it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:20,  1.24it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:20,  1.24it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:19,  1.25it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:19,  1.25it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.25it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.25it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.25it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.25it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.25it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.25it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.25it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.25it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:15,  1.25it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:15,  1.25it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.25it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.25it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.25it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.25it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.26it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.26it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:11,  1.26it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:11,  1.26it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:11,  1.26it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:11,  1.26it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.26it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.26it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.26it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.26it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.26it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.26it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:15<00:07,  1.26it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:15<00:07,  1.26it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:07,  1.26it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:07,  1.26it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.26it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.26it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.26it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.26it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:18<00:04,  1.26it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:18<00:04,  1.26it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:19<00:03,  1.26it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:19<00:03,  1.26it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.26it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.26it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.26it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.26it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:21<00:01,  1.26it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:21<00:01,  1.26it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:22<00:00,  1.26it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:22<00:00,  1.26it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:22<00:00,  1.30it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:22<00:00,  1.30it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.03it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.86it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:05,  0.79it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.84it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.90it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.93it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  0.96it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:53<00:00,  0.10it/s, pixel_AUPRO=0.870]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:53<00:00,  0.10it/s, pixel_AUPRO=0.870]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:26<00:00,  0.09it/s, pixel_AUPRO=0.870]INFO:anomalib.callbacks.timer:Training took 327.89 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:36,  0.17it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:29,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:23,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:23<00:17,  0.17it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:29<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:34<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:36<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 383.0699577331543 seconds
Throughput (batch_size=32) : 0.5220978465226437 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [06:21<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7976000308990479     â”‚
â”‚        pixel_AUPRO        â”‚    0.8699078559875488     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:29,  0.96it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:29,  0.96it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.21it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.21it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:21,  1.23it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:21,  1.23it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:20,  1.20it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:20,  1.20it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:04<00:20,  1.20it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:04<00:20,  1.20it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:05<00:19,  1.19it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:05<00:19,  1.19it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:18,  1.19it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:18,  1.19it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:17,  1.20it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:17,  1.20it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:16,  1.20it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:16,  1.20it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:08<00:15,  1.21it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:08<00:15,  1.21it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:09<00:14,  1.21it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:09<00:14,  1.21it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:14,  1.21it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:14,  1.21it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:13,  1.22it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:13,  1.22it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:12,  1.22it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:12,  1.22it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:12<00:11,  1.22it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:12<00:11,  1.22it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:13<00:10,  1.23it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:13<00:10,  1.23it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.23it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.23it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.24it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.24it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:15<00:08,  1.24it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:15<00:08,  1.24it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:16<00:07,  1.24it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:16<00:07,  1.24it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.24it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.24it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.25it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.25it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:18<00:04,  1.25it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:18<00:04,  1.25it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:19<00:03,  1.25it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:19<00:03,  1.25it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.25it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.25it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.25it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.25it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:21<00:01,  1.25it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:21<00:01,  1.25it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:22<00:00,  1.25it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:22<00:00,  1.25it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:22<00:00,  1.29it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:22<00:00,  1.29it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.03it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:06,  0.82it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:05,  0.78it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.85it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.90it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.93it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  0.96it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:58<00:00,  0.10it/s, pixel_AUPRO=0.874]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:58<00:00,  0.10it/s, pixel_AUPRO=0.874]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:30<00:00,  0.09it/s, pixel_AUPRO=0.874]INFO:anomalib.callbacks.timer:Training took 331.35 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:36,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:30,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:23,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:23<00:17,  0.17it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:29<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:35<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 390.75928235054016 seconds
Throughput (batch_size=32) : 0.5118240539212198 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [06:29<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8048999905586243     â”‚
â”‚        pixel_AUPRO        â”‚    0.8740521669387817     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.98it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.98it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.25it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.25it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.27it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.27it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.28it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.28it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.27it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.27it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.27it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.27it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.27it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.27it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.27it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.27it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.27it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.27it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.28it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.28it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.28it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.28it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.28it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.28it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.28it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.28it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.28it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.28it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.29it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.29it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.29it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.29it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.29it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.29it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.29it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.29it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.29it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.29it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.29it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.29it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.29it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.29it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.29it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.29it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.30it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.30it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.30it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.30it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.30it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.30it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.30it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.30it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.30it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.30it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.30it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.30it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.34it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.04it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.90it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.84it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.90it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.95it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.98it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.01it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:55<00:00,  0.10it/s, pixel_AUPRO=0.874]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:55<00:00,  0.10it/s, pixel_AUPRO=0.874]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:28<00:00,  0.09it/s, pixel_AUPRO=0.874]INFO:anomalib.callbacks.timer:Training took 329.45 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:37,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:30,  0.16it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:23,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:24<00:18,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:37<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 387.09007930755615 seconds
Throughput (batch_size=32) : 0.5166756026343244 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [06:25<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9451999664306641     â”‚
â”‚        pixel_AUPRO        â”‚    0.8743008971214294     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.97it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.97it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.26it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.26it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.27it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.27it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.28it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.28it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.28it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.28it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.25it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.25it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.26it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.26it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.25it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.25it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.26it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.26it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:15,  1.26it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:15,  1.26it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.26it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.26it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.27it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.27it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.27it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:11,  1.27it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:11,  1.27it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.27it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.27it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.28it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.28it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.28it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.28it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.28it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.28it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.28it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.28it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:07,  1.28it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:07,  1.28it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.28it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.28it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.28it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.28it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.28it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.28it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.28it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.28it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.29it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.29it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.29it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.29it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.29it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.29it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.29it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.29it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.33it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:04,  1.22it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.95it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.86it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.92it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.98it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:05<00:00,  1.03it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.07it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:54<00:00,  0.10it/s, pixel_AUPRO=0.873]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:54<00:00,  0.10it/s, pixel_AUPRO=0.873]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:26<00:00,  0.09it/s, pixel_AUPRO=0.873]INFO:anomalib.callbacks.timer:Training took 327.11 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:36,  0.17it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:29,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:23,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:23<00:17,  0.17it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:28<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:34<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:36<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 382.9816589355469 seconds
Throughput (batch_size=32) : 0.5222182194204203 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [06:21<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9314000606536865     â”‚
â”‚        pixel_AUPRO        â”‚    0.8732349872589111     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb2 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:29,  0.94it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:29,  0.94it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.22it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.22it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.24it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.24it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.25it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.25it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.27it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.27it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.27it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.27it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.27it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.27it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.27it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.27it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.28it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.28it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.28it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.28it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.29it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.29it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.29it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.29it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.29it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.29it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.29it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.29it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.29it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.29it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.29it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.29it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.29it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.29it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.29it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.29it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.29it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.29it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.29it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.29it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.29it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.29it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.30it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.30it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.30it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.30it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.30it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.30it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.30it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.30it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.30it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.30it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.29it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.29it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.29it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.29it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.33it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:07,  0.76it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:06,  0.73it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:04<00:05,  0.73it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:05<00:04,  0.75it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:06<00:02,  0.76it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:07<00:01,  0.77it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:08<00:00,  0.78it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:01<00:00,  0.10it/s, pixel_AUPRO=0.887]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:01<00:00,  0.10it/s, pixel_AUPRO=0.887]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:33<00:00,  0.09it/s, pixel_AUPRO=0.887]INFO:anomalib.callbacks.timer:Training took 334.80 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:39,  0.15it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:32,  0.15it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:19<00:26,  0.15it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:25<00:19,  0.15it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:32<00:13,  0.15it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:38<00:06,  0.15it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:41<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 393.3114061355591 seconds
Throughput (batch_size=32) : 0.5085029238411352 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [06:31<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9469000101089478     â”‚
â”‚        pixel_AUPRO        â”‚    0.8872100710868835     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb3 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:04<02:18,  0.20it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:04<02:18,  0.20it/s]Epoch 0:   7%|â–‹         | 2/29 [00:06<01:27,  0.31it/s]Epoch 0:   7%|â–‹         | 2/29 [00:06<01:27,  0.31it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:08<01:16,  0.34it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:08<01:16,  0.34it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:10<01:04,  0.39it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:10<01:04,  0.39it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:12<01:02,  0.39it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:12<01:02,  0.39it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:14<00:55,  0.42it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:14<00:55,  0.42it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:15<00:49,  0.44it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:15<00:49,  0.44it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:18<00:48,  0.43it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:18<00:48,  0.43it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:19<00:44,  0.45it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:19<00:44,  0.45it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:21<00:40,  0.47it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:21<00:40,  0.47it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:23<00:38,  0.46it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:23<00:38,  0.46it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:25<00:35,  0.48it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:25<00:35,  0.48it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:26<00:32,  0.49it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:26<00:32,  0.49it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:29<00:31,  0.48it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:29<00:31,  0.48it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:30<00:28,  0.49it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:30<00:28,  0.49it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:33<00:26,  0.48it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:33<00:26,  0.48it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:34<00:24,  0.49it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:34<00:24,  0.49it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:36<00:22,  0.50it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:36<00:22,  0.50it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:38<00:20,  0.49it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:38<00:20,  0.49it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:39<00:17,  0.50it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:39<00:17,  0.50it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:41<00:15,  0.51it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:41<00:15,  0.51it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:43<00:13,  0.50it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:43<00:13,  0.50it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:45<00:11,  0.51it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:45<00:11,  0.51it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:46<00:09,  0.51it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:46<00:09,  0.51it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:49<00:07,  0.51it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:49<00:07,  0.51it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:50<00:05,  0.52it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:50<00:05,  0.52it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:51<00:03,  0.52it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:51<00:03,  0.52it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:54<00:01,  0.51it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:54<00:01,  0.51it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:54<00:00,  0.53it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:54<00:00,  0.53it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:04,  1.22it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:03<00:08,  0.56it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:05<00:07,  0.54it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:08<00:06,  0.50it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:09<00:03,  0.55it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:10<00:01,  0.58it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:11<00:00,  0.63it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [03:45<00:00,  0.13it/s, pixel_AUPRO=0.748]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [03:45<00:00,  0.13it/s, pixel_AUPRO=0.748]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:11<00:00,  0.12it/s, pixel_AUPRO=0.748]INFO:anomalib.callbacks.timer:Training took 252.35 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:36,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:29,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:22,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:22<00:16,  0.18it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:29<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:35<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 301.199903011322 seconds
Throughput (batch_size=32) : 0.6673308921764309 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [04:59<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7416831254959106     â”‚
â”‚        pixel_AUPRO        â”‚    0.8520910143852234     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb3 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.32it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.32it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.33it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.33it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.34it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.34it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.33it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.33it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.31it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.31it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.30it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.30it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.29it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.29it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.27it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.27it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.27it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.27it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.27it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.28it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.28it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.28it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.28it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.28it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.28it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.28it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.28it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.29it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.29it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.29it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.29it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.29it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.29it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.29it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.29it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.29it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.29it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.30it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.30it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.30it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.30it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.30it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.30it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.30it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.30it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.30it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.30it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.30it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.30it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.30it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.30it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.31it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.31it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.34it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:04,  1.22it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.99it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.89it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.95it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:04<00:01,  1.02it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:05<00:00,  1.06it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.10it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:02<00:00,  0.12it/s, pixel_AUPRO=0.843]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:02<00:00,  0.12it/s, pixel_AUPRO=0.843]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:28<00:00,  0.11it/s, pixel_AUPRO=0.843]INFO:anomalib.callbacks.timer:Training took 269.34 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:05<00:35,  0.17it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:28,  0.18it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:16<00:22,  0.18it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:21<00:16,  0.18it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:27<00:10,  0.18it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:32<00:05,  0.18it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:34<00:00,  0.20it/s]INFO:anomalib.callbacks.timer:Testing took 296.90439200401306 seconds
Throughput (batch_size=32) : 0.6769856068592047 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [04:55<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7940593957901001     â”‚
â”‚        pixel_AUPRO        â”‚    0.8428129553794861     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb3 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.99it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.99it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.28it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.28it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.26it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.26it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.26it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.26it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:19,  1.25it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:19,  1.25it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.26it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.26it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.27it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.27it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.28it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.28it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.28it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.28it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.28it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.28it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.29it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.29it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.29it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.29it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.29it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.29it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.30it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.30it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.30it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.30it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:09,  1.30it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:09,  1.30it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:09,  1.31it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:09,  1.31it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.31it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.31it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.31it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.31it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.32it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.32it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:06,  1.32it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:06,  1.32it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.32it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.32it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.32it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.32it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.32it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.32it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:03,  1.32it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:03,  1.32it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.32it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.32it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.33it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.33it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.33it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.36it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.16it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.96it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.87it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.94it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.99it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:05<00:00,  1.04it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.09it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:04<00:00,  0.12it/s, pixel_AUPRO=0.840]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:04<00:00,  0.12it/s, pixel_AUPRO=0.840]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:31<00:00,  0.11it/s, pixel_AUPRO=0.840]INFO:anomalib.callbacks.timer:Training took 272.08 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:05<00:35,  0.17it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:29,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:22,  0.18it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:23<00:17,  0.17it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:29<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:35<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 297.0523669719696 seconds
Throughput (batch_size=32) : 0.6766483702820207 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [04:55<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7935643792152405     â”‚
â”‚        pixel_AUPRO        â”‚    0.8400952816009521     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb3 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.23it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.23it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.27it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.27it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.29it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.29it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.29it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.29it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.30it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:17,  1.30it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.31it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.31it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:15,  1.32it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:15,  1.32it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:15,  1.32it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:15,  1.32it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.33it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.33it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.33it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.33it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.33it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.33it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.34it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.34it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.34it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.34it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.34it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.34it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.34it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.34it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.35it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.35it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.35it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.35it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.35it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.35it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.35it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.35it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.35it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.35it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.36it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.36it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.36it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.36it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.36it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.36it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.36it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.36it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.36it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.36it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.36it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.36it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.36it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.36it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.40it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.40it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:04,  1.22it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.99it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.89it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.97it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:04<00:01,  1.03it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:05<00:00,  1.08it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.12it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:05<00:00,  0.12it/s, pixel_AUPRO=0.839]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:05<00:00,  0.12it/s, pixel_AUPRO=0.839]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:32<00:00,  0.11it/s, pixel_AUPRO=0.839]INFO:anomalib.callbacks.timer:Training took 273.68 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:05<00:35,  0.17it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:28,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:16<00:22,  0.18it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:22<00:16,  0.18it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:28<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:34<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 299.3364851474762 seconds
Throughput (batch_size=32) : 0.6714851345333728 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [04:57<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9125742316246033     â”‚
â”‚        pixel_AUPRO        â”‚    0.8389060497283936     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb3 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.02it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.32it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.32it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.34it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.34it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.36it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:02<00:18,  1.36it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.36it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:17,  1.36it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:16,  1.36it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:16,  1.36it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.36it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:16,  1.36it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.36it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:05<00:15,  1.36it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.36it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:14,  1.36it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:13,  1.36it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:13,  1.36it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.36it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.36it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.37it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:08<00:12,  1.37it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.37it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:11,  1.37it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:10,  1.37it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:10,  1.37it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:10<00:10,  1.37it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:10<00:10,  1.37it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.37it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:11<00:09,  1.37it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.37it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:08,  1.37it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.37it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.37it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.37it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:13<00:07,  1.37it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.37it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:14<00:06,  1.37it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.37it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:05,  1.37it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.37it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.37it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.37it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:16<00:04,  1.37it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.37it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:17<00:03,  1.37it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.37it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.37it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:18<00:02,  1.37it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:18<00:02,  1.37it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.37it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:19<00:01,  1.37it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.37it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.37it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.41it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:20<00:00,  1.41it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:04,  1.43it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:01<00:04,  1.04it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.92it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.99it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:04<00:01,  1.05it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:05<00:00,  1.10it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.15it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:08<00:00,  0.12it/s, pixel_AUPRO=0.852]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:08<00:00,  0.12it/s, pixel_AUPRO=0.852]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:35<00:00,  0.11it/s, pixel_AUPRO=0.852]INFO:anomalib.callbacks.timer:Training took 276.17 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:36,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:11<00:29,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:22,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:22<00:16,  0.18it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:29<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:34<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:36<00:00,  0.19it/s]INFO:anomalib.callbacks.timer:Testing took 299.3302478790283 seconds
Throughput (batch_size=32) : 0.6714991265474526 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [04:57<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9205940365791321     â”‚
â”‚        pixel_AUPRO        â”‚    0.8520534038543701     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb3 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.00it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.00it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:20,  1.29it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.29it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.31it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.31it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.31it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.31it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.27it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:18,  1.27it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.28it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.28it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.29it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.29it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.29it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.29it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:15,  1.30it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:06<00:15,  1.30it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.30it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.30it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.31it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:13,  1.31it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:12,  1.31it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:12,  1.31it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:12,  1.31it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:09<00:12,  1.31it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.32it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.32it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.32it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.32it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:09,  1.32it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:09,  1.32it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:09,  1.32it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:12<00:09,  1.32it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.33it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.33it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.33it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.33it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.33it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.33it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:06,  1.33it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:15<00:06,  1.33it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.33it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:16<00:05,  1.33it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.33it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.33it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.33it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.33it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.33it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:18<00:02,  1.33it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.34it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:19<00:02,  1.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.34it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.34it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:20<00:00,  1.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.37it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.37it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.91it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:06,  0.81it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:05,  0.78it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.82it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.85it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.88it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  0.89it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:09<00:00,  0.12it/s, pixel_AUPRO=0.875]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:09<00:00,  0.12it/s, pixel_AUPRO=0.875]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [04:35<00:00,  0.11it/s, pixel_AUPRO=0.875]INFO:anomalib.callbacks.timer:Training took 276.31 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:37,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:30,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:23,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:23<00:17,  0.17it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:29<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:35<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 299.16731452941895 seconds
Throughput (batch_size=32) : 0.6718648403023801 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [04:57<00:00,  0.02it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9619802236557007     â”‚
â”‚        pixel_AUPRO        â”‚    0.8754422068595886     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb4 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:02<01:11,  0.39it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:02<01:11,  0.39it/s]Epoch 0:   7%|â–‹         | 2/29 [00:04<01:04,  0.42it/s]Epoch 0:   7%|â–‹         | 2/29 [00:04<01:04,  0.42it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:06<00:55,  0.47it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:06<00:55,  0.47it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:07<00:49,  0.51it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:07<00:49,  0.51it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:10<00:49,  0.48it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:10<00:49,  0.48it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:11<00:45,  0.50it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:11<00:45,  0.50it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:13<00:41,  0.53it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:13<00:41,  0.53it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:15<00:41,  0.51it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:15<00:41,  0.51it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:17<00:38,  0.52it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:17<00:38,  0.52it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:18<00:35,  0.53it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:18<00:35,  0.53it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:21<00:34,  0.52it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:21<00:34,  0.52it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:22<00:31,  0.53it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:22<00:31,  0.53it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:25<00:31,  0.51it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:25<00:31,  0.51it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:27<00:29,  0.52it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:27<00:29,  0.52it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:28<00:26,  0.52it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:28<00:26,  0.52it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:31<00:25,  0.51it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:31<00:25,  0.51it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:32<00:22,  0.52it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:32<00:22,  0.52it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:34<00:20,  0.53it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:34<00:20,  0.53it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:36<00:19,  0.52it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:36<00:19,  0.52it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:38<00:17,  0.52it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:38<00:17,  0.52it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:40<00:15,  0.52it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:40<00:15,  0.52it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:42<00:13,  0.52it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:42<00:13,  0.52it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:43<00:11,  0.53it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:43<00:11,  0.53it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:45<00:09,  0.52it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:45<00:09,  0.52it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:47<00:07,  0.53it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:47<00:07,  0.53it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:48<00:05,  0.53it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:48<00:05,  0.53it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:51<00:03,  0.53it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:51<00:03,  0.53it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:52<00:01,  0.53it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:52<00:01,  0.53it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:53<00:00,  0.55it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:53<00:00,  0.55it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.96it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:03<00:07,  0.67it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:06<00:08,  0.49it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:07<00:05,  0.53it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:08<00:03,  0.57it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:10<00:01,  0.55it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:11<00:00,  0.59it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:42<00:00,  0.08it/s, pixel_AUPRO=0.691]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [05:42<00:00,  0.08it/s, pixel_AUPRO=0.691]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [06:16<00:00,  0.08it/s, pixel_AUPRO=0.691]INFO:anomalib.callbacks.timer:Training took 377.43 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:38,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:31,  0.16it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:18<00:25,  0.16it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:25<00:18,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:38<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:40<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 583.1242134571075 seconds
Throughput (batch_size=32) : 0.34469499870079534 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [09:41<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8994058966636658     â”‚
â”‚        pixel_AUPRO        â”‚     0.827556312084198     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb4 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:30,  0.93it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:30,  0.93it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.21it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.21it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.24it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.24it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.25it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.25it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:04<00:19,  1.23it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:04<00:19,  1.23it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.23it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.23it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.24it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.24it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.25it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.25it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.25it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.25it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:15,  1.26it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:15,  1.26it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.26it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.26it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.27it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.27it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.27it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:11,  1.27it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:11,  1.27it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.27it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.27it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.28it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.28it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.28it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.28it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.28it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.28it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.28it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.28it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:07,  1.28it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:07,  1.28it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.28it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.28it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.28it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.28it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.29it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.29it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.29it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.29it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.29it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.29it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.29it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.29it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.29it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.29it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.29it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.29it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.33it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.93it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.84it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:05,  0.77it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.84it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.89it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.92it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  0.94it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:33<00:00,  0.06it/s, pixel_AUPRO=0.844]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:33<00:00,  0.06it/s, pixel_AUPRO=0.844]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [08:07<00:00,  0.06it/s, pixel_AUPRO=0.844]INFO:anomalib.callbacks.timer:Training took 488.27 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:37,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:13<00:32,  0.15it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:19<00:25,  0.15it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:26<00:19,  0.15it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:33<00:13,  0.15it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:39<00:06,  0.15it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:42<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 571.0579450130463 seconds
Throughput (batch_size=32) : 0.35197829179210177 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [09:29<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9443564414978027     â”‚
â”‚        pixel_AUPRO        â”‚    0.8441746830940247     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb4 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:00<00:27,  1.00it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  1.00it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.19it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.19it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:22,  1.15it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:22,  1.15it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:22,  1.13it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:22,  1.13it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:04<00:21,  1.14it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:04<00:21,  1.14it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:05<00:19,  1.16it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:05<00:19,  1.16it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:18,  1.18it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:18,  1.18it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:17,  1.20it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:17,  1.20it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:16,  1.21it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:16,  1.21it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:08<00:15,  1.22it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:08<00:15,  1.22it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.22it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.22it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.23it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.23it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.24it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.24it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:12,  1.24it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:12,  1.24it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:12<00:11,  1.25it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:12<00:11,  1.25it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.25it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.25it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.25it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.25it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.26it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.26it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:15<00:07,  1.26it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:15<00:07,  1.26it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:07,  1.27it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:07,  1.27it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.27it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.27it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.27it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.27it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:18<00:04,  1.27it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:18<00:04,  1.27it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.27it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.27it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.28it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.28it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.28it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.28it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:21<00:01,  1.28it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:21<00:01,  1.28it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.28it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.28it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:22<00:00,  1.32it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:22<00:00,  1.32it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.99it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.88it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.82it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.88it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.92it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.95it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  0.97it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:42<00:00,  0.06it/s, pixel_AUPRO=0.841]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:42<00:00,  0.06it/s, pixel_AUPRO=0.841]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [08:16<00:00,  0.06it/s, pixel_AUPRO=0.841]INFO:anomalib.callbacks.timer:Training took 497.32 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:37,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:30,  0.16it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:18<00:24,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:24<00:18,  0.17it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:30<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:36<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 565.3226962089539 seconds
Throughput (batch_size=32) : 0.35554914272486 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [09:23<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9465345740318298     â”‚
â”‚        pixel_AUPRO        â”‚    0.8415138721466064     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb4 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:29,  0.95it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:29,  0.95it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.22it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.22it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.25it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.25it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.25it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.25it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:19,  1.26it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:19,  1.26it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.26it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.26it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.27it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.27it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.26it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.26it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.27it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.27it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.27it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.27it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.27it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.27it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.27it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.27it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.27it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.27it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.27it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.27it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.28it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.28it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.28it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.28it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.28it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.28it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.28it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.28it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:07,  1.28it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:07,  1.28it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.28it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.28it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.28it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.28it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.28it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.28it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.28it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.28it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.28it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.28it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.28it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.28it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:21<00:01,  1.28it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:21<00:01,  1.28it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.29it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.29it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.32it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.32it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:06,  0.98it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:06,  0.83it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:05,  0.78it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.84it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.88it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:06<00:01,  0.91it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  0.94it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:36<00:00,  0.06it/s, pixel_AUPRO=0.837]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:36<00:00,  0.06it/s, pixel_AUPRO=0.837]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [08:11<00:00,  0.06it/s, pixel_AUPRO=0.837]INFO:anomalib.callbacks.timer:Training took 492.22 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:38,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:32,  0.15it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:19<00:25,  0.16it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:24<00:18,  0.16it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:31<00:12,  0.16it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:36<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 583.5867626667023 seconds
Throughput (batch_size=32) : 0.3444217944244136 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [09:42<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9968316555023193     â”‚
â”‚        pixel_AUPRO        â”‚    0.8365511894226074     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb4 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.98it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:28,  0.98it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.29it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:21,  1.29it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.30it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:19,  1.30it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.26it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.26it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:04<00:19,  1.23it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:04<00:19,  1.23it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.24it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.24it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.25it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.25it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.25it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.25it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.25it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.25it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:15,  1.26it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:15,  1.26it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.26it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.26it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.27it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.27it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.27it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.27it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:11,  1.27it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:11<00:11,  1.27it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.27it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.27it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.28it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.28it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.28it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.28it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.28it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:14<00:08,  1.28it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.28it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.28it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.29it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.29it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.29it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.29it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.29it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.29it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.29it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.29it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.29it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.29it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.29it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.29it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.29it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.29it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.29it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.29it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.29it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.29it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.33it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:00<00:05,  1.15it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:05,  0.94it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:03<00:04,  0.85it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:04<00:03,  0.91it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:05<00:02,  0.97it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:05<00:00,  1.02it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.04it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:39<00:00,  0.06it/s, pixel_AUPRO=0.847]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:39<00:00,  0.06it/s, pixel_AUPRO=0.847]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [08:12<00:00,  0.06it/s, pixel_AUPRO=0.847]INFO:anomalib.callbacks.timer:Training took 493.26 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:06<00:36,  0.16it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:12<00:30,  0.17it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:17<00:23,  0.17it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:23<00:17,  0.17it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:29<00:11,  0.17it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:35<00:05,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 581.2780375480652 seconds
Throughput (batch_size=32) : 0.3457897718755279 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [09:39<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9965345859527588     â”‚
â”‚        pixel_AUPRO        â”‚    0.8467438817024231     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pcb4 with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/29 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:29,  0.95it/s]Epoch 0:   3%|â–Ž         | 1/29 [00:01<00:29,  0.95it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.21it/s]Epoch 0:   7%|â–‹         | 2/29 [00:01<00:22,  1.21it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.24it/s]Epoch 0:  10%|â–ˆ         | 3/29 [00:02<00:20,  1.24it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.25it/s]Epoch 0:  14%|â–ˆâ–        | 4/29 [00:03<00:19,  1.25it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:19,  1.25it/s]Epoch 0:  17%|â–ˆâ–‹        | 5/29 [00:03<00:19,  1.25it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.26it/s]Epoch 0:  21%|â–ˆâ–ˆ        | 6/29 [00:04<00:18,  1.26it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.27it/s]Epoch 0:  24%|â–ˆâ–ˆâ–       | 7/29 [00:05<00:17,  1.27it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.27it/s]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 8/29 [00:06<00:16,  1.27it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.27it/s]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 9/29 [00:07<00:15,  1.27it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.27it/s]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 10/29 [00:07<00:14,  1.27it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.28it/s]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 11/29 [00:08<00:14,  1.28it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.28it/s]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 12/29 [00:09<00:13,  1.28it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.28it/s]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/29 [00:10<00:12,  1.28it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.29it/s]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 14/29 [00:10<00:11,  1.29it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.29it/s]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 15/29 [00:11<00:10,  1.29it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.29it/s]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 16/29 [00:12<00:10,  1.29it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.29it/s]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 17/29 [00:13<00:09,  1.29it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.29it/s]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 18/29 [00:13<00:08,  1.29it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.29it/s]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 19/29 [00:14<00:07,  1.29it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.29it/s]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 20/29 [00:15<00:06,  1.29it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.29it/s]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 21/29 [00:16<00:06,  1.29it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.29it/s]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 22/29 [00:17<00:05,  1.29it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.29it/s]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 23/29 [00:17<00:04,  1.29it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.29it/s]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 24/29 [00:18<00:03,  1.29it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.30it/s]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 25/29 [00:19<00:03,  1.30it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.30it/s]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 26/29 [00:20<00:02,  1.30it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.30it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 27/29 [00:20<00:01,  1.30it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.30it/s]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 28/29 [00:21<00:00,  1.30it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.33it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:21<00:00,  1.33it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s][A
Validation DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:01<00:08,  0.70it/s][A
Validation DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:06,  0.73it/s][A
Validation DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:04<00:05,  0.73it/s][A
Validation DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:05<00:04,  0.74it/s][A
Validation DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:06<00:02,  0.74it/s][A
Validation DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:08<00:01,  0.74it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:09<00:00,  0.75it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:41<00:00,  0.06it/s, pixel_AUPRO=0.884]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [07:41<00:00,  0.06it/s, pixel_AUPRO=0.884]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [08:15<00:00,  0.06it/s, pixel_AUPRO=0.884]INFO:anomalib.callbacks.timer:Training took 496.44 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]Testing DataLoader 0:  14%|â–ˆâ–        | 1/7 [00:07<00:42,  0.14it/s]Testing DataLoader 0:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:13<00:34,  0.14it/s]Testing DataLoader 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:20<00:27,  0.15it/s]Testing DataLoader 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:27<00:20,  0.15it/s]Testing DataLoader 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:34<00:13,  0.15it/s]Testing DataLoader 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:40<00:06,  0.15it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:43<00:00,  0.16it/s]INFO:anomalib.callbacks.timer:Testing took 587.3927974700928 seconds
Throughput (batch_size=32) : 0.34219009982027226 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [09:45<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9972277283668518     â”‚
â”‚        pixel_AUPRO        â”‚    0.8841233849525452     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
14        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pipe_fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:04<01:03,  0.22it/s]Epoch 0:   7%|â–‹         | 1/15 [00:04<01:03,  0.22it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:06<00:44,  0.29it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:06<00:44,  0.29it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:08<00:33,  0.36it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:08<00:33,  0.36it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:09<00:26,  0.41it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:09<00:26,  0.41it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:11<00:23,  0.43it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:11<00:23,  0.43it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:12<00:19,  0.46it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:12<00:19,  0.46it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:14<00:16,  0.49it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:14<00:16,  0.49it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:15<00:13,  0.52it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:15<00:13,  0.52it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:17<00:11,  0.51it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:17<00:11,  0.51it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:18<00:09,  0.53it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:18<00:09,  0.53it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:19<00:07,  0.55it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:19<00:07,  0.55it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:22<00:05,  0.54it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:22<00:05,  0.54it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:23<00:03,  0.55it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:23<00:03,  0.55it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:24<00:01,  0.57it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:24<00:01,  0.57it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:24<00:00,  0.61it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:24<00:00,  0.61it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.89it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:03<00:05,  0.59it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:06<00:04,  0.46it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:07<00:01,  0.51it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:08<00:00,  0.57it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:04<00:00,  0.05it/s, pixel_AUPRO=0.917]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:04<00:00,  0.05it/s, pixel_AUPRO=0.917]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:41<00:00,  0.04it/s, pixel_AUPRO=0.917]INFO:anomalib.callbacks.timer:Training took 342.76 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:25,  0.16it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:18,  0.16it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:18<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:24<00:06,  0.17it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:28<00:00,  0.18it/s]INFO:anomalib.callbacks.timer:Testing took 422.2114369869232 seconds
Throughput (batch_size=32) : 0.35527223296096033 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:00<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚     0.770799994468689     â”‚
â”‚        pixel_AUPRO        â”‚    0.9344801902770996     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pipe_fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.98it/s]Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.98it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.26it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.26it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.28it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.28it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.29it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.29it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.29it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.29it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.30it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.30it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.30it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.30it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.30it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.30it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.30it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.30it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.31it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.31it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.31it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.31it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.31it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.31it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.31it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.31it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.31it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.31it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.40it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.40it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.90it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.84it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.80it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.85it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.87it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:55<00:00,  0.04it/s, pixel_AUPRO=0.933]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:55<00:00,  0.04it/s, pixel_AUPRO=0.933]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:32<00:00,  0.04it/s, pixel_AUPRO=0.933]INFO:anomalib.callbacks.timer:Training took 393.60 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:25,  0.16it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:18,  0.16it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:18<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:24<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:29<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 421.3299877643585 seconds
Throughput (batch_size=32) : 0.3560154851448457 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [06:59<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.8040000796318054     â”‚
â”‚        pixel_AUPRO        â”‚    0.9327687621116638     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pipe_fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.02it/s]Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.02it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.29it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.29it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.30it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.30it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.32it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.32it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.33it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.33it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.34it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.34it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:05,  1.34it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:05,  1.34it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:05<00:05,  1.35it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:05<00:05,  1.35it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.35it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.35it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.35it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.35it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:02,  1.35it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:02,  1.35it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:08<00:02,  1.35it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:08<00:02,  1.35it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.35it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.35it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.35it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.35it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.44it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.44it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.91it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.83it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.79it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.83it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.86it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:00<00:00,  0.04it/s, pixel_AUPRO=0.931]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:00<00:00,  0.04it/s, pixel_AUPRO=0.931]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:37<00:00,  0.04it/s, pixel_AUPRO=0.931]INFO:anomalib.callbacks.timer:Training took 398.66 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:25,  0.15it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:18,  0.16it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:18<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:24<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:28<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 426.969135761261 seconds
Throughput (batch_size=32) : 0.3513134497006646 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:05<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.7977999448776245     â”‚
â”‚        pixel_AUPRO        â”‚    0.9307581186294556     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pipe_fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.01it/s]Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.01it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.29it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.29it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.29it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.29it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.23it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.23it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:08,  1.21it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:04<00:08,  1.21it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.22it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.22it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.24it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.24it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.25it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.25it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.26it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.26it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.27it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.27it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.28it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.28it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.29it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.29it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.30it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.30it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.30it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.30it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.39it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.39it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  0.91it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.86it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.81it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.86it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.89it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:03<00:00,  0.04it/s, pixel_AUPRO=0.930]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:03<00:00,  0.04it/s, pixel_AUPRO=0.930]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:40<00:00,  0.04it/s, pixel_AUPRO=0.930]INFO:anomalib.callbacks.timer:Training took 401.85 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:25,  0.16it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:12<00:18,  0.16it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:18<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:25<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:30<00:00,  0.16it/s]INFO:anomalib.callbacks.timer:Testing took 431.15574383735657 seconds
Throughput (batch_size=32) : 0.3479021261898902 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:09<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9793999791145325     â”‚
â”‚        pixel_AUPRO        â”‚    0.9299197793006897     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pipe_fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.01it/s]Epoch 0:   7%|â–‹         | 1/15 [00:00<00:13,  1.01it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:09,  1.31it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:09,  1.31it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.33it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.33it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:02<00:08,  1.34it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:02<00:08,  1.34it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.35it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.35it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.34it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:06,  1.34it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:05,  1.35it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:05,  1.35it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:05<00:05,  1.34it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:05<00:05,  1.34it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.34it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:06<00:04,  1.34it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.34it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.34it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:02,  1.34it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:02,  1.34it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:08<00:02,  1.34it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:08<00:02,  1.34it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.34it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:09<00:01,  1.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.34it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.34it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.43it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.43it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.00it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  0.90it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  0.83it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:04<00:01,  0.89it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  0.92it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:06<00:00,  0.04it/s, pixel_AUPRO=0.928]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:06<00:00,  0.04it/s, pixel_AUPRO=0.928]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:43<00:00,  0.04it/s, pixel_AUPRO=0.928]INFO:anomalib.callbacks.timer:Training took 404.53 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:25,  0.16it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:13<00:19,  0.15it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:19<00:12,  0.16it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:25<00:06,  0.16it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:29<00:00,  0.17it/s]INFO:anomalib.callbacks.timer:Testing took 427.5529742240906 seconds
Throughput (batch_size=32) : 0.35083371896129406 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:06<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9811999797821045     â”‚
â”‚        pixel_AUPRO        â”‚    0.9282838106155396     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
INFO:anomalib.models.components.base.anomaly_module:Initializing Padim model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing LWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing Patchcore model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPADE model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:anomalib.models.components.base.anomaly_module:Initializing SPALWinNN model.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)
INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:182: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer

  | Name                  | Type                     | Params | Mode 
---------------------------------------------------------------------------
0 | model                 | SPALWinNNModel           | 2.8 M  | train
1 | _transform            | Compose                  | 0      | train
2 | normalization_metrics | MetricCollection         | 0      | train
3 | image_threshold       | F1AdaptiveThreshold      | 0      | train
4 | pixel_threshold       | F1AdaptiveThreshold      | 0      | train
5 | image_metrics         | AnomalibMetricCollection | 0      | train
6 | pixel_metrics         | AnomalibMetricCollection | 0      | train
---------------------------------------------------------------------------
2.8 M     Trainable params
0         Non-trainable params
2.8 M     Total params
11.131    Total estimated model params size (MB)
15        Modules in train mode
69        Modules in eval mode
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Running dataset visa pipe_fryum with model spalwinnn
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] /cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...
Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.97it/s]Epoch 0:   7%|â–‹         | 1/15 [00:01<00:14,  0.97it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.24it/s]Epoch 0:  13%|â–ˆâ–Ž        | 2/15 [00:01<00:10,  1.24it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.27it/s]Epoch 0:  20%|â–ˆâ–ˆ        | 3/15 [00:02<00:09,  1.27it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.28it/s]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 4/15 [00:03<00:08,  1.28it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.25it/s]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [00:03<00:07,  1.25it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.26it/s]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [00:04<00:07,  1.26it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.27it/s]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [00:05<00:06,  1.27it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.27it/s]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [00:06<00:05,  1.27it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.28it/s]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [00:07<00:04,  1.28it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.28it/s]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [00:07<00:03,  1.28it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.28it/s]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [00:08<00:03,  1.28it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.28it/s]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [00:09<00:02,  1.28it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.28it/s]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [00:10<00:01,  1.28it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.29it/s]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [00:10<00:00,  1.29it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.37it/s]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.37it/s]
Validation: |          | 0/? [00:00<?, ?it/s][AINFO:src.anomalib.models.image.spalwinnn.lightning_model:Aggregating the embedding extracted from the training set.
INFO:src.anomalib.models.image.spalwinnn.lightning_model:Generating memory bank

Validation:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s][A
Validation DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:06,  0.65it/s][A
Validation DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:04,  0.67it/s][A
Validation DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:04<00:02,  0.68it/s][A
Validation DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:05<00:01,  0.68it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:07<00:00,  0.69it/s][A
                                                                      [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:08<00:00,  0.04it/s, pixel_AUPRO=0.938]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:08<00:00,  0.04it/s, pixel_AUPRO=0.938]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:45<00:00,  0.04it/s, pixel_AUPRO=0.938]INFO:anomalib.callbacks.timer:Training took 406.46 seconds
INFO:src.anomalib.data.image.visa:Found the dataset and train/test split.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/cw/dtaijupiter/NoCsBack/dtai/mariette/miniconda3/envs/new_env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.

Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]Testing DataLoader 0:  20%|â–ˆâ–ˆ        | 1/5 [00:06<00:27,  0.14it/s]Testing DataLoader 0:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:13<00:20,  0.15it/s]Testing DataLoader 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:19<00:13,  0.15it/s]Testing DataLoader 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:26<00:06,  0.15it/s]Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:31<00:00,  0.16it/s]INFO:anomalib.callbacks.timer:Testing took 425.0561144351959 seconds
Throughput (batch_size=32) : 0.3528945823995881 FPS
Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [07:03<00:00,  0.01it/s]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚        image_AUROC        â”‚    0.9909999370574951     â”‚
â”‚        pixel_AUPRO        â”‚     0.938185453414917     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
